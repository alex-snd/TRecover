---
title: "️✔️Summary"
---

# ✔️Summary

<h3 align="justify">
    Nowadays, Big Tech corporations are training state of the art multi-billion parameter models on thousands of GPUs, 
    spending millions of dollars on it, which the ordinary researcher cannot afford. 😢
    <br><br>
    What I want to show with this project is that even for a non-profit research task, you can find like-minded 
volunteers and train a fairly large machine learning model.
</h3>

## Work accomplished
- <div align="justify"><font size="4">Designed and trained 50M parameters Transformer model to “read” meaningful text in columns that can be compiled for a
  <a href="https://en.wikipedia.org/wiki/Running_key_cipher">Running Key Cipher</a>, widely known in field of Cryptography</font></div>
- <div align="justify"><font size="4">Implemented the code base and prepared the infrastructure for conveniently conducting an experiment on 
  distributed model training across the Internet with the help of volunteers</font></div>
- <div align="justify"><font size="4">Implemented beam search for inference and CLI interface, dashboard and API with a 
  celery worker under the hood for convenient interaction, and packed it all into docker to make it more reproducible</font></div>
- <div align="justify"><font size="4">Configured CI/CD pipelines to publish python package to PyPi, build docker images 
  and push them to Docker Hub and GitHub Packages, sync repository with HuggingFace Sphere and update project website</font></div>