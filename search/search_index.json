{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\ud83d\udc4b Welcome to Text Recovery Project A Python library for distributed LLM training over the Internet to solve the Running Key Cipher , a well-known Cryptography problem Nowadays, Big Tech corporations are training state of the art multi-billion parameter models on thousands of GPUs, spending millions of dollars on it, which the ordinary researcher cannot afford. What I want to show with this project is that even for a non-profit research task, you can find like-minded volunteers and train a fairly large machine learning model. The main goal of the project is to study the possibility of using LLM to \u201cread\u201d meaningful text in columns that can be compiled for a Running Key Cipher. The second goal is to train a fairly large model in a distributed manner with the help of volunteers from around the globe. \ud83d\udca1 Task Definition Notations For a clearer explanation of the problem, let me introduce the following concepts. Let \\(A\\) be the alphabet of the English language, the letters of which are arranged in natural order. The \\(\\alpha + \\gamma = \\beta \\,\\) notation will be understood as the modular addition of letter numbers from \\(\\{ 0, \\, 1, \\, 2, \\, ... \\, , \\, |A|- 1 \\}\\) modulo \\(|A|\\), where \\(\\alpha, \\, \\gamma, \\, \\beta \\in A\\). That is, when encrypting plaintext , we will identify letters with their numbers in the alphabet \\(A\\). Simple case Let's consider a simple example in which only two letters \\(\\gamma', \\, \\gamma'' \\in A\\) can be used as a keystream \\(\\bar{\\gamma}=(\\gamma_1, \\, \\gamma_2, \\, \u2026 \\, , \\, \\gamma_n)\\) values, that is, the ciphertext \\(\\bar{\\beta}=(\\beta_1, \\, \\beta_2, \\, \u2026 \\, , \\, \\beta_n)\\) formation equation has the form \\(\\alpha_i + \\gamma_i = \\beta_i, \\; i \\in \\{1, \\, 2, \\, \u2026 \\, , \\, n\\}\\), where \\(n\\) is the length of the message. In this case, recovering the plaintext \\(\\bar{\\alpha}=(\\alpha_1, \\, \\alpha_2, \\, ... \\, , \\, \\alpha_n)\\) by ciphertext \\(\\bar{\\beta}\\) isn't difficult. Indeed, we will make up the columns \\(\\bar{\\nabla}=(\\bar{\\nabla_1}, \\, \\bar{\\nabla_2}, \\, ... \\, , \\, \\bar{\\nabla_n})\\) according to the known ciphertext \\(\\bar{\\beta}\\), where \\(\\bar{\\nabla_i}=(\\beta_i-\\gamma', \\, \\beta_i-\\gamma''), \\; i \\in \\{1, \\, 2, \\, ... \\, , \\, n\\}\\): \\(\\beta_1-\\gamma'\\) \\(\\beta_2-\\gamma'\\) \\(\\beta_3-\\gamma'\\) \\(...\\) \\(\\beta_n-\\gamma'\\) \\(\\beta_1-\\gamma''\\) \\(\\beta_2-\\gamma''\\) \\(\\beta_3-\\gamma''\\) \\(...\\) \\(\\beta_n-\\gamma''\\) Obviously, each column \\(\\bar{\\nabla_i} \\) consists of unique values and contains one letter \\(\\alpha_i \\) of the plaintext \\(\\bar{\\alpha} \\). You can try to recover this text using its redundancy . Without going into details, here is one example for \u201creading\u201d in columns: \\(c\\) \\(d\\) \\(l\\) \\(p\\) \\(q\\) \\(o\\) \\(k\\) \\(u\\) \\(a\\) \\(x\\) \\(h\\) \\(g\\) \\(y\\) \\(r\\) \\(y\\) \\(w\\) \\(t\\) \\(j\\) \\(g\\) \\(r\\) \\(b\\) \\(p\\) \\(m\\) \\(y\\) Have you read the word ... cryptography \ud83d\ude09? If four letters were used for encryption, then the column depth would be equal to four. And if all the letters were used, then the depth of the columns would be equal to \\(|A|\\) and in this case you can read any text in them. General case Let's take a closer look at an example in which all letters from the alphabet \\(A\\) can be used as keystream \\(\\bar{\\gamma}\\) values. In this case, there are also certain approaches for making up the columns \\(\\bar{\\nabla}\\). One of them is that in each column \\(\\bar{\\nabla_i}=(\\alpha_i^{(1)}, \\, \\alpha_i^{(2)}, \\, ... \\, , \\, \\alpha_i^{(|A|)})\\) the order of possible plaintext letters \\(\\alpha_i^{(j)} \\in A\\) is determined by decreasing (more precisely, not increasing) their probabilities: \\[P(\\alpha_i^{(j)} \\, | \\, \\beta_i) = \\frac{P(\\alpha_i^{(j)}, \\, \\beta_i)}{P(\\beta_i)}=\\frac{\\phi(\\alpha_i^{(j)}) \\cdot \\varphi(\\beta_i-\\alpha_i^{(j)})}{\\sum_{\\alpha\u2019 \\in A} \\phi(\\alpha\u2019) \\cdot \\varphi(\\beta_i-\\alpha\u2019)},\\] \\[i \\in \\{1, \\, 2, \\, ... \\, , \\, n\\}, \\; j \\in \\{1, \\, 2, \\, ... \\, , \\, |A|\\},\\] with a known fixed letter \\(\\beta_i \\in A\\) of the ciphertext \\(\\bar{\\beta}\\). Here \\(\\phi(\\alpha), \\; \\alpha \\in A\\) is probability distribution of letters of meaningful texts for the alphabet \\(A, \\; \\varphi (\\gamma), \\, \\gamma \\in A\\) is probability distribution of the \\(\\bar{\\gamma}\\) keystream values. Also, for a more accurate ordering of letters in columns, their probabilities are calculated based on n-grams. Depth limitation The depth \\(\\bar{h}=(h_1, \\, h_2, \\, ... \\, , \\, h_n)\\) of the columns \\(\\bar{\\nabla}=(\\bar{\\nabla_1}, \\, \\bar{\\nabla_2}, \\, ... \\, , \\, \\bar{\\nabla_n})\\) can be limited using a pre-selected value of the \\(\\epsilon \\in (0, 1]\\) parameter: \\[\\begin{aligned} h_i=max\\{\\ell \\in \\{ 1, \\, 2, \\, ... \\, , \\, |A|\\} : \\sum_{j=1}^{\\ell} P(\\alpha_i^{(j)} \\, | \\, \\beta_i) \\le \\epsilon \\}. \\end{aligned}\\] The critical depth of the columns \\(\\hat{h}\\), at which it is possible to unambiguously determine the original plaintext, is calculated by the formula: \\[\\begin{aligned} \\hat{h}=|A|^{1 - H(A)}, \\end{aligned}\\] where \\(H(A)\\) is entropy of a language with the \\(A\\) alphabet. For English, \\(\\hat{h} \\approx 13\\) . \ud83d\udc40 Demo \ud83e\udd17 Hugging Face \ud83d\udc33 Docker Compose \ud83d\udcbb Local Torch Hub You can play with a pre-trained model hosted on HuggingFace Sphere . Use the command below to run the service via Docker Compose. Pull from Docker Hub Build from source standalone scallable docker-compose -f docker/compose/all-in-one-service.yml up docker-compose -f docker/compose/scalable-service.yml up standalone scallable trecover download artifacts docker-compose -f docker/compose/all-in-one-service-duild.yml up --build trecover download artifacts docker-compose -f docker/compose/scalable-service-build.yml up --build You can also try the Play with Docker service mentioned in the official docker documentation . To run the service locally, docker must be installed. // Install the package $ pip install trecover [ demo ] <b>Successfully installed trecover</b> <br> // Initialize project's environment $ trecover init <b>Project's environment is initialized.</b> <br> // Download pretrained model $ trecover download artifacts <b>Downloaded \"model.pt\" to ../inference/model.pt</b> <b>Downloaded \"config.json\" to ../inference/config.json</b> <br> // Start the service $ trecover up <b>\ud83d\ude80 The service is started</b> For more information use trecover --help or read the reference . You can load the pre-trained model directly from the python script. import torch device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) model = torch . hub . load ( 'alex-snd/TRecover' , model = 'trecover' , device = device , version = 'latest' ) To print a help message and available pre-trained models run this: import torch print ( torch . hub . help ( 'alex-snd/TRecover' , model = 'trecover' , force_reload = True )) Do you want to challenge yourself ? \ud83d\ude09 Try to read meaningful text in these columns: a d f o n p p u w f o u d d y k d d a u n t r y x g n k w n t n t t a t t u r e t g t x r u e w r t h n x o d v t i t i o t p f m o b k j z t g d s c y w w w t d x h k n d p d a r d x d n g t h p u r w u n d d n z s p f g g e a r n g r h e h o w l z w c g l i f o p c e w w r e y g c b l y w d n h k k w s r o e u s i n e g i s h n p h n v u v b o b u z a u x p p i i b i w k k s z e k h q l x r t o u y z e p a e e q q p g f a u o h e p s h i c u n i i q y r o i e l p p r e f m e s r c k u i e m h g e w i h l q r s a s h e b r t r q h s t q e c e r q o o q e q e c o r y l a p e a m q e u l r p i h q b j m c s c y c o s s a l t s s r m j j o s a c j o q o o o m o m q m b c t m i m s And see what the pre-trained model will recover: 1. Copy these columns a ds fpziq ofe ngkhbo p pghl ue waq frlqjo o u dnxrm dgr yrtsco kho deuasm dhysc ao u nwzhy tle r yzpe xwabc gce nger klqto wiq nfprso t no tpgq tcfh ae twas tw ur re e t gyutsm t xgo rc ubhq e wle r ty h nwpeaq xdsc o dnhelm v thir ikcq tkuo i o twn ps frio mo oe b kuiqtb jsq zi tnye ge dgrqs s cioe ys whic wne wp thlo dnprsc xvpyrt hurlm kveaj nbfp dome pbeaj dusmo a r dzrqsm xace du nxkuai gpulcm tpi h pie uim r wbhrj ui n dwgp dkeio nkwhqs zs 2. Open the dashboard hosted here 3. In the sidebar, select \"Noisy columns\" as an input type 4. Paste the copied columns into the input field 5. Click the \"Recover\" button \ud83c\udf89 \ud83d\udcaa Join Collaborative Training \ud83d\ude0e Easy way You can help to train the model by running a pre-prepared Jupyter Notebook on Kaggle or Google Colab. To join the collaborative training, all you have to do is to keep the Notebook running for at least 15 minutes (but more is better) and you're free to close it after that and join again in another time. Join as a client peer Kaggle gives you around 40 hrs per week of GPU time, so it's preferred over Colab, unless you have Colab Pro or Colab Pro+. Kaggle (recommended) Google Colab Please make sure to select GPU accelerator and switch the \"Internet\" ON, in kernel settings If you are a new Kaggle member, you will need to verify your phone number in the settings pane to turn on the Internet. Warning: please don't use multiple Kaggle accounts at once Open in Kaggle Please make sure to select GPU accelerator Warning: please don't use multiple Google Colab accounts at once Open in Colab \ud83d\ude0a Preferred way You can join the collaborative training as an auxiliary peer if you have a Linux computer with 15+ GB RAM, at least 100 Mbit/s download&upload speed and one port opened to incoming connections. Also, if in addition to all this there is also a GPU, then you can join as a trainer peer. Why is this way more preferable? There are two broad types of peers: normal (full) peers and client mode peers. Client peers rely on others to average their gradients, but otherwise behave the same as full peers. This way of participation is preferable as the auxiliary and trainer peers not only don\u2019t rely on others, but also can serve as relays and help others with all-reduce. Installation This step is common to the trainer and auxiliary peer, and requires Python 3.8 or higher. Open a tmux (or screen) session that will stay up after you logout and follow the instructions below. // Clone the repository $ git clone https://github.com/alex-snd/TRecover.git Cloning into 'TRecover'... remote: Enumerating objects: 4716, done. remote: Counting objects: 100% (368/368), done. remote: Compressing objects: 100% (133/133), done. remote: Total 4716 (delta 196), reused 348 (delta 181) ---> 100% Receiving objects: 100% (4716/4716), 18.69 MiB | 4.36 MiB/s, done. Resolving deltas: 100% (2908/2908), done. <br> // Change the working dir $ cd TRecover <br> // Create a virtual environment $ python3 -m venv venv <br> // Activate the virtual environment $ source venv/bin/activate <br> // Install the package $ pip install -e . [ collab ] <b>Successfully installed trecover</b> <br> // Initialize project's environment $ trecover init <b>Project's environment is initialized.</b> <br> <br> // For more information use <font color=\"#36464E\">trecover --help</font> or read the <a href=\"https://alex-snd.github.io/TRecover/src/trecover/app/cli\">reference</a>. Join as a trainer peer Trainers are peers with GPUs (or other compute accelerators) that compute gradients, average them via all-reduce and perform optimizer steps. // Download the train dataset $ trecover download data ---> 100% Downloaded \"data.zip\" to ../data/data.zip Archive extracted to ../data // Run the trainer peer $ trecover collab train --sync-args --batch-size 1 --n-workers 2 --backup-every-step 1 // Use <font color=\"#36464E\">--help</font> flag for more details Join as an auxiliary peer Auxiliary peers are low-end servers without GPU that will keep track of the latest model checkpoint and assist in gradient averaging. // Run the auxiliary peer $ trecover collab aux --sync-args --verbose --as-active-peer --backup-every-step 1 // Use <font color=\"#36464E\">--help</font> flag for more details","title":"Home"},{"location":"#task-definition","text":"","title":"\ud83d\udca1 Task Definition"},{"location":"#notations","text":"For a clearer explanation of the problem, let me introduce the following concepts. Let \\(A\\) be the alphabet of the English language, the letters of which are arranged in natural order. The \\(\\alpha + \\gamma = \\beta \\,\\) notation will be understood as the modular addition of letter numbers from \\(\\{ 0, \\, 1, \\, 2, \\, ... \\, , \\, |A|- 1 \\}\\) modulo \\(|A|\\), where \\(\\alpha, \\, \\gamma, \\, \\beta \\in A\\). That is, when encrypting plaintext , we will identify letters with their numbers in the alphabet \\(A\\).","title":"Notations"},{"location":"#simple-case","text":"Let's consider a simple example in which only two letters \\(\\gamma', \\, \\gamma'' \\in A\\) can be used as a keystream \\(\\bar{\\gamma}=(\\gamma_1, \\, \\gamma_2, \\, \u2026 \\, , \\, \\gamma_n)\\) values, that is, the ciphertext \\(\\bar{\\beta}=(\\beta_1, \\, \\beta_2, \\, \u2026 \\, , \\, \\beta_n)\\) formation equation has the form \\(\\alpha_i + \\gamma_i = \\beta_i, \\; i \\in \\{1, \\, 2, \\, \u2026 \\, , \\, n\\}\\), where \\(n\\) is the length of the message. In this case, recovering the plaintext \\(\\bar{\\alpha}=(\\alpha_1, \\, \\alpha_2, \\, ... \\, , \\, \\alpha_n)\\) by ciphertext \\(\\bar{\\beta}\\) isn't difficult. Indeed, we will make up the columns \\(\\bar{\\nabla}=(\\bar{\\nabla_1}, \\, \\bar{\\nabla_2}, \\, ... \\, , \\, \\bar{\\nabla_n})\\) according to the known ciphertext \\(\\bar{\\beta}\\), where \\(\\bar{\\nabla_i}=(\\beta_i-\\gamma', \\, \\beta_i-\\gamma''), \\; i \\in \\{1, \\, 2, \\, ... \\, , \\, n\\}\\): \\(\\beta_1-\\gamma'\\) \\(\\beta_2-\\gamma'\\) \\(\\beta_3-\\gamma'\\) \\(...\\) \\(\\beta_n-\\gamma'\\) \\(\\beta_1-\\gamma''\\) \\(\\beta_2-\\gamma''\\) \\(\\beta_3-\\gamma''\\) \\(...\\) \\(\\beta_n-\\gamma''\\) Obviously, each column \\(\\bar{\\nabla_i} \\) consists of unique values and contains one letter \\(\\alpha_i \\) of the plaintext \\(\\bar{\\alpha} \\). You can try to recover this text using its redundancy . Without going into details, here is one example for \u201creading\u201d in columns: \\(c\\) \\(d\\) \\(l\\) \\(p\\) \\(q\\) \\(o\\) \\(k\\) \\(u\\) \\(a\\) \\(x\\) \\(h\\) \\(g\\) \\(y\\) \\(r\\) \\(y\\) \\(w\\) \\(t\\) \\(j\\) \\(g\\) \\(r\\) \\(b\\) \\(p\\) \\(m\\) \\(y\\) Have you read the word ...","title":"Simple case"},{"location":"#general-case","text":"Let's take a closer look at an example in which all letters from the alphabet \\(A\\) can be used as keystream \\(\\bar{\\gamma}\\) values. In this case, there are also certain approaches for making up the columns \\(\\bar{\\nabla}\\). One of them is that in each column \\(\\bar{\\nabla_i}=(\\alpha_i^{(1)}, \\, \\alpha_i^{(2)}, \\, ... \\, , \\, \\alpha_i^{(|A|)})\\) the order of possible plaintext letters \\(\\alpha_i^{(j)} \\in A\\) is determined by decreasing (more precisely, not increasing) their probabilities: \\[P(\\alpha_i^{(j)} \\, | \\, \\beta_i) = \\frac{P(\\alpha_i^{(j)}, \\, \\beta_i)}{P(\\beta_i)}=\\frac{\\phi(\\alpha_i^{(j)}) \\cdot \\varphi(\\beta_i-\\alpha_i^{(j)})}{\\sum_{\\alpha\u2019 \\in A} \\phi(\\alpha\u2019) \\cdot \\varphi(\\beta_i-\\alpha\u2019)},\\] \\[i \\in \\{1, \\, 2, \\, ... \\, , \\, n\\}, \\; j \\in \\{1, \\, 2, \\, ... \\, , \\, |A|\\},\\] with a known fixed letter \\(\\beta_i \\in A\\) of the ciphertext \\(\\bar{\\beta}\\). Here \\(\\phi(\\alpha), \\; \\alpha \\in A\\) is probability distribution of letters of meaningful texts for the alphabet \\(A, \\; \\varphi (\\gamma), \\, \\gamma \\in A\\) is probability distribution of the \\(\\bar{\\gamma}\\) keystream values. Also, for a more accurate ordering of letters in columns, their probabilities are calculated based on n-grams.","title":"General case"},{"location":"#depth-limitation","text":"The depth \\(\\bar{h}=(h_1, \\, h_2, \\, ... \\, , \\, h_n)\\) of the columns \\(\\bar{\\nabla}=(\\bar{\\nabla_1}, \\, \\bar{\\nabla_2}, \\, ... \\, , \\, \\bar{\\nabla_n})\\) can be limited using a pre-selected value of the \\(\\epsilon \\in (0, 1]\\) parameter: \\[\\begin{aligned} h_i=max\\{\\ell \\in \\{ 1, \\, 2, \\, ... \\, , \\, |A|\\} : \\sum_{j=1}^{\\ell} P(\\alpha_i^{(j)} \\, | \\, \\beta_i) \\le \\epsilon \\}. \\end{aligned}\\] The critical depth of the columns \\(\\hat{h}\\), at which it is possible to unambiguously determine the original plaintext, is calculated by the formula: \\[\\begin{aligned} \\hat{h}=|A|^{1 - H(A)}, \\end{aligned}\\] where \\(H(A)\\) is entropy of a language with the \\(A\\) alphabet. For English, \\(\\hat{h} \\approx 13\\) .","title":"Depth limitation"},{"location":"#demo","text":"\ud83e\udd17 Hugging Face \ud83d\udc33 Docker Compose \ud83d\udcbb Local Torch Hub You can play with a pre-trained model hosted on HuggingFace Sphere . Use the command below to run the service via Docker Compose. Pull from Docker Hub Build from source standalone scallable docker-compose -f docker/compose/all-in-one-service.yml up docker-compose -f docker/compose/scalable-service.yml up standalone scallable trecover download artifacts docker-compose -f docker/compose/all-in-one-service-duild.yml up --build trecover download artifacts docker-compose -f docker/compose/scalable-service-build.yml up --build You can also try the Play with Docker service mentioned in the official docker documentation . To run the service locally, docker must be installed. // Install the package $ pip install trecover [ demo ] <b>Successfully installed trecover</b> <br> // Initialize project's environment $ trecover init <b>Project's environment is initialized.</b> <br> // Download pretrained model $ trecover download artifacts <b>Downloaded \"model.pt\" to ../inference/model.pt</b> <b>Downloaded \"config.json\" to ../inference/config.json</b> <br> // Start the service $ trecover up <b>\ud83d\ude80 The service is started</b> For more information use trecover --help or read the reference . You can load the pre-trained model directly from the python script. import torch device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) model = torch . hub . load ( 'alex-snd/TRecover' , model = 'trecover' , device = device , version = 'latest' ) To print a help message and available pre-trained models run this: import torch print ( torch . hub . help ( 'alex-snd/TRecover' , model = 'trecover' , force_reload = True )) Do you want to challenge yourself ? \ud83d\ude09 Try to read meaningful text in these columns: a d f o n p p u w f o u d d y k d d a u n t r y x g n k w n t n t t a t t u r e t g t x r u e w r t h n x o d v t i t i o t p f m o b k j z t g d s c y w w w t d x h k n d p d a r d x d n g t h p u r w u n d d n z s p f g g e a r n g r h e h o w l z w c g l i f o p c e w w r e y g c b l y w d n h k k w s r o e u s i n e g i s h n p h n v u v b o b u z a u x p p i i b i w k k s z e k h q l x r t o u y z e p a e e q q p g f a u o h e p s h i c u n i i q y r o i e l p p r e f m e s r c k u i e m h g e w i h l q r s a s h e b r t r q h s t q e c e r q o o q e q e c o r y l a p e a m q e u l r p i h q b j m c s c y c o s s a l t s s r m j j o s a c j o q o o o m o m q m b c t m i m s And see what the pre-trained model will recover: 1. Copy these columns a ds fpziq ofe ngkhbo p pghl ue waq frlqjo o u dnxrm dgr yrtsco kho deuasm dhysc ao u nwzhy tle r yzpe xwabc gce nger klqto wiq nfprso t no tpgq tcfh ae twas tw ur re e t gyutsm t xgo rc ubhq e wle r ty h nwpeaq xdsc o dnhelm v thir ikcq tkuo i o twn ps frio mo oe b kuiqtb jsq zi tnye ge dgrqs s cioe ys whic wne wp thlo dnprsc xvpyrt hurlm kveaj nbfp dome pbeaj dusmo a r dzrqsm xace du nxkuai gpulcm tpi h pie uim r wbhrj ui n dwgp dkeio nkwhqs zs 2. Open the dashboard hosted here 3. In the sidebar, select \"Noisy columns\" as an input type 4. Paste the copied columns into the input field 5. Click the \"Recover\" button \ud83c\udf89","title":"\ud83d\udc40 Demo"},{"location":"#join-collaborative-training","text":"","title":"\ud83d\udcaa Join Collaborative Training"},{"location":"#easy-way","text":"You can help to train the model by running a pre-prepared Jupyter Notebook on Kaggle or Google Colab. To join the collaborative training, all you have to do is to keep the Notebook running for at least 15 minutes (but more is better) and you're free to close it after that and join again in another time.","title":"\ud83d\ude0e Easy way"},{"location":"#join-as-a-client-peer","text":"Kaggle gives you around 40 hrs per week of GPU time, so it's preferred over Colab, unless you have Colab Pro or Colab Pro+. Kaggle (recommended) Google Colab Please make sure to select GPU accelerator and switch the \"Internet\" ON, in kernel settings If you are a new Kaggle member, you will need to verify your phone number in the settings pane to turn on the Internet. Warning: please don't use multiple Kaggle accounts at once Open in Kaggle Please make sure to select GPU accelerator Warning: please don't use multiple Google Colab accounts at once Open in Colab","title":"Join as a client peer"},{"location":"#preferred-way","text":"You can join the collaborative training as an auxiliary peer if you have a Linux computer with 15+ GB RAM, at least 100 Mbit/s download&upload speed and one port opened to incoming connections. Also, if in addition to all this there is also a GPU, then you can join as a trainer peer. Why is this way more preferable? There are two broad types of peers: normal (full) peers and client mode peers. Client peers rely on others to average their gradients, but otherwise behave the same as full peers. This way of participation is preferable as the auxiliary and trainer peers not only don\u2019t rely on others, but also can serve as relays and help others with all-reduce.","title":"\ud83d\ude0a Preferred way"},{"location":"#installation","text":"This step is common to the trainer and auxiliary peer, and requires Python 3.8 or higher. Open a tmux (or screen) session that will stay up after you logout and follow the instructions below. // Clone the repository $ git clone https://github.com/alex-snd/TRecover.git Cloning into 'TRecover'... remote: Enumerating objects: 4716, done. remote: Counting objects: 100% (368/368), done. remote: Compressing objects: 100% (133/133), done. remote: Total 4716 (delta 196), reused 348 (delta 181) ---> 100% Receiving objects: 100% (4716/4716), 18.69 MiB | 4.36 MiB/s, done. Resolving deltas: 100% (2908/2908), done. <br> // Change the working dir $ cd TRecover <br> // Create a virtual environment $ python3 -m venv venv <br> // Activate the virtual environment $ source venv/bin/activate <br> // Install the package $ pip install -e . [ collab ] <b>Successfully installed trecover</b> <br> // Initialize project's environment $ trecover init <b>Project's environment is initialized.</b> <br> <br> // For more information use <font color=\"#36464E\">trecover --help</font> or read the <a href=\"https://alex-snd.github.io/TRecover/src/trecover/app/cli\">reference</a>.","title":"Installation"},{"location":"#join-as-a-trainer-peer","text":"Trainers are peers with GPUs (or other compute accelerators) that compute gradients, average them via all-reduce and perform optimizer steps. // Download the train dataset $ trecover download data ---> 100% Downloaded \"data.zip\" to ../data/data.zip Archive extracted to ../data // Run the trainer peer $ trecover collab train --sync-args --batch-size 1 --n-workers 2 --backup-every-step 1 // Use <font color=\"#36464E\">--help</font> flag for more details","title":"Join as a trainer peer"},{"location":"#join-as-an-auxiliary-peer","text":"Auxiliary peers are low-end servers without GPU that will keep track of the latest model checkpoint and assist in gradient averaging. // Run the auxiliary peer $ trecover collab aux --sync-args --verbose --as-active-peer --backup-every-step 1 // Use <font color=\"#36464E\">--help</font> flag for more details","title":"Join as an auxiliary peer"},{"location":"src/trecover/app/api/","text":"startup startup () API startup handler. Source code in src/trecover/app/api/trecoverapi.py 17 18 19 20 21 @api . on_event ( 'startup' ) def startup () -> None : \"\"\" API startup handler. \"\"\" log . project_logger . info ( 'FatAPI launched' ) construct_response construct_response ( handler ) A decorator that wraps a request handler. Parameters: Name Type Description Default handler Callable [..., Dict ] Request processing function. required Returns: Name Type Description wrap Callable [..., Dict ] Decorated handler. Source code in src/trecover/app/api/trecoverapi.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def construct_response ( handler : Callable [ ... , Dict ]) -> Callable [ ... , Dict ]: \"\"\" A decorator that wraps a request handler. Parameters ---------- handler : Callable[..., Dict] Request processing function. Returns ------- wrap : Callable[..., Dict] Decorated handler. \"\"\" @wraps ( handler ) def wrap ( request : Request , * args , ** kwargs ) -> Dict : \"\"\" A wrapper that constructs a JSON response for an endpoint's results. Parameters ---------- request : Request Client request information. Returns ------- response : Dict The result of the handler function. \"\"\" response = handler ( request , * args , ** kwargs ) response [ 'method' ] = request . method response [ 'timestamp' ] = datetime . now () . isoformat () response [ 'url' ] = request . url . _url return response return wrap index index ( request ) Healthcheck handler. Parameters: Name Type Description Default request Request Client request information. required Returns: Name Type Description Dict Dict OK phrase as a Dict response. Source code in src/trecover/app/api/trecoverapi.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 @api . get ( '/' , tags = [ 'General' ]) @construct_response def index ( request : Request ) -> Dict : \"\"\" Healthcheck handler. Parameters ---------- request : Request Client request information. Returns ------- Dict: OK phrase as a Dict response. \"\"\" return { 'message' : HTTPStatus . OK . phrase , 'status_code' : HTTPStatus . OK } config config ( request ) Get the configuration of the model used for inference. Parameters: Name Type Description Default request Request Client request information. required Returns: Name Type Description response Dict Response containing the values of the model configuration in the 'config' field. Source code in src/trecover/app/api/trecoverapi.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 @api . get ( '/config' , tags = [ 'Configuration' ]) @construct_response def config ( request : Request ) -> Dict : \"\"\" Get the configuration of the model used for inference. Parameters ---------- request : Request Client request information. Returns ------- response : Dict Response containing the values of the model configuration in the 'config' field. \"\"\" task = get_model_config . delay () model_config = task . get () response = { 'message' : HTTPStatus . OK . phrase , 'status_code' : HTTPStatus . OK , 'config' : model_config } return response config_param config_param ( request , param ) Get the specific configuration parameter of the model used for inference. Parameters: Name Type Description Default request Request Client request information. required param str Parameter name. required Returns: Name Type Description response Dict Response containing the value of the specific configuration parameter in the ' ' field if it exists, otherwise 'Not found' value. Source code in src/trecover/app/api/trecoverapi.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 @api . get ( '/config/ {param} ' , tags = [ 'Configuration' ]) @construct_response def config_param ( request : Request , param : str ) -> Dict : \"\"\" Get the specific configuration parameter of the model used for inference. Parameters ---------- request : Request Client request information. param : str Parameter name. Returns ------- response : Dict Response containing the value of the specific configuration parameter in the '<param>' field if it exists, otherwise 'Not found' value. \"\"\" task = get_model_config . delay () model_config = task . get () response = { 'message' : HTTPStatus . OK . phrase , 'status_code' : HTTPStatus . OK , param : model_config . get ( param , 'Not found' ) } return response recover recover ( request , payload ) Perform keyless reading. Parameters: Name Type Description Default request Request Client request information. required payload PredictPayload Data for keyless reading. required Returns: Name Type Description response TaskResponse Response containing the id of the celery task in the 'task_id' field. Source code in src/trecover/app/api/trecoverapi.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 @api . post ( '/recover' , tags = [ 'Prediction' ], response_model = TaskResponse ) @construct_response def recover ( request : Request , payload : PredictPayload ) -> Dict : \"\"\" Perform keyless reading. Parameters ---------- request : Request Client request information. payload : PredictPayload Data for keyless reading. Returns ------- response : TaskResponse Response containing the id of the celery task in the 'task_id' field. \"\"\" task = predict . delay ( payload . columns , payload . beam_width , payload . delimiter ) response = { 'message' : HTTPStatus . ACCEPTED . phrase , 'status_code' : HTTPStatus . ACCEPTED , 'task_id' : task . id } return response status status ( request , task_id = Path ( Ellipsis , title = \"The ID of the task to get status\" , regex = \"[a-z0-9] {8} -[a-z0-9] {4} -[a-z0-9] {4} -[a-z0-9] {4} -[a-z0-9] {12} \" , ), ) Get a celery task status. Parameters: Name Type Description Default request Request Client request information. required task_id str Celery task id. Path(Ellipsis, title='The ID of the task to get status', regex='[a-z0-9]{8}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{12}') Returns: Name Type Description response PredictResponse Response containing the status of the celery task in the 'state' and 'progress' fields if it's still in process, error information in 'message' and 'status_code' fields if it's failed, otherwise the result of keyless reading in the 'chains' field. Source code in src/trecover/app/api/trecoverapi.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 @api . get ( '/status/ {task_id} ' , tags = [ 'Prediction' ], response_model = PredictResponse ) @construct_response def status ( request : Request , task_id : str = Path ( ... , title = 'The ID of the task to get status' , regex = r '[a-z0-9] {8} -[a-z0-9] {4} -[a-z0-9] {4} -[a-z0-9] {4} -[a-z0-9] {12} ' ) ) -> Dict : \"\"\" Get a celery task status. Parameters ---------- request : Request Client request information. task_id : str Celery task id. Returns ------- response : PredictResponse Response containing the status of the celery task in the 'state' and 'progress' fields if it's still in process, error information in 'message' and 'status_code' fields if it's failed, otherwise the result of keyless reading in the 'chains' field. \"\"\" task = AsyncResult ( task_id , app = celery_app ) if task . failed (): response = { 'message' : str ( task . info ), 'status_code' : HTTPStatus . CONFLICT } elif task . ready (): columns , chains = task . get () response = { 'message' : HTTPStatus . OK . phrase , 'status_code' : HTTPStatus . OK , 'columns' : columns , 'chains' : chains , 'progress' : len ( columns ) } else : info = task . info response = { 'message' : HTTPStatus . PROCESSING . phrase , 'status_code' : HTTPStatus . PROCESSING , 'state' : task . status , 'progress' : info . get ( 'progress' ) if isinstance ( info , dict ) else None } return response delete_prediction delete_prediction ( request , task_id = Path ( Ellipsis , title = \"The ID of the task to forget\" , regex = \"[a-z0-9] {8} -[a-z0-9] {4} -[a-z0-9] {4} -[a-z0-9] {4} -[a-z0-9] {12} \" , ), ) Delete task result in the celery backend database. Parameters: Name Type Description Default request Request Client request information. required task_id str Task ID to delete its result from celery backend database. Path(Ellipsis, title='The ID of the task to forget', regex='[a-z0-9]{8}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{12}') Returns: Name Type Description response Dict OK phrase. Source code in src/trecover/app/api/trecoverapi.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 @api . delete ( '/ {task_id} ' , tags = [ 'Prediction' ]) @construct_response def delete_prediction ( request : Request , task_id : str = Path ( ... , title = 'The ID of the task to forget' , regex = r '[a-z0-9] {8} -[a-z0-9] {4} -[a-z0-9] {4} -[a-z0-9] {4} -[a-z0-9] {12} ' ) ) -> Dict : \"\"\" Delete task result in the celery backend database. Parameters ---------- request : Request Client request information. task_id : str Task ID to delete its result from celery backend database. Returns ------- response : Dict OK phrase. \"\"\" task = AsyncResult ( task_id , app = celery_app ) if task . ready (): task . forget () else : task . revoke ( terminate = True ) response = { 'message' : HTTPStatus . OK . phrase , 'status_code' : HTTPStatus . OK } return response","title":"API"},{"location":"src/trecover/app/api/#src.trecover.app.api.trecoverapi.startup","text":"startup () API startup handler. Source code in src/trecover/app/api/trecoverapi.py 17 18 19 20 21 @api . on_event ( 'startup' ) def startup () -> None : \"\"\" API startup handler. \"\"\" log . project_logger . info ( 'FatAPI launched' )","title":"startup()"},{"location":"src/trecover/app/api/#src.trecover.app.api.trecoverapi.construct_response","text":"construct_response ( handler ) A decorator that wraps a request handler. Parameters: Name Type Description Default handler Callable [..., Dict ] Request processing function. required Returns: Name Type Description wrap Callable [..., Dict ] Decorated handler. Source code in src/trecover/app/api/trecoverapi.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def construct_response ( handler : Callable [ ... , Dict ]) -> Callable [ ... , Dict ]: \"\"\" A decorator that wraps a request handler. Parameters ---------- handler : Callable[..., Dict] Request processing function. Returns ------- wrap : Callable[..., Dict] Decorated handler. \"\"\" @wraps ( handler ) def wrap ( request : Request , * args , ** kwargs ) -> Dict : \"\"\" A wrapper that constructs a JSON response for an endpoint's results. Parameters ---------- request : Request Client request information. Returns ------- response : Dict The result of the handler function. \"\"\" response = handler ( request , * args , ** kwargs ) response [ 'method' ] = request . method response [ 'timestamp' ] = datetime . now () . isoformat () response [ 'url' ] = request . url . _url return response return wrap","title":"construct_response()"},{"location":"src/trecover/app/api/#src.trecover.app.api.trecoverapi.index","text":"index ( request ) Healthcheck handler. Parameters: Name Type Description Default request Request Client request information. required Returns: Name Type Description Dict Dict OK phrase as a Dict response. Source code in src/trecover/app/api/trecoverapi.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 @api . get ( '/' , tags = [ 'General' ]) @construct_response def index ( request : Request ) -> Dict : \"\"\" Healthcheck handler. Parameters ---------- request : Request Client request information. Returns ------- Dict: OK phrase as a Dict response. \"\"\" return { 'message' : HTTPStatus . OK . phrase , 'status_code' : HTTPStatus . OK }","title":"index()"},{"location":"src/trecover/app/api/#src.trecover.app.api.trecoverapi.config","text":"config ( request ) Get the configuration of the model used for inference. Parameters: Name Type Description Default request Request Client request information. required Returns: Name Type Description response Dict Response containing the values of the model configuration in the 'config' field. Source code in src/trecover/app/api/trecoverapi.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 @api . get ( '/config' , tags = [ 'Configuration' ]) @construct_response def config ( request : Request ) -> Dict : \"\"\" Get the configuration of the model used for inference. Parameters ---------- request : Request Client request information. Returns ------- response : Dict Response containing the values of the model configuration in the 'config' field. \"\"\" task = get_model_config . delay () model_config = task . get () response = { 'message' : HTTPStatus . OK . phrase , 'status_code' : HTTPStatus . OK , 'config' : model_config } return response","title":"config()"},{"location":"src/trecover/app/api/#src.trecover.app.api.trecoverapi.config_param","text":"config_param ( request , param ) Get the specific configuration parameter of the model used for inference. Parameters: Name Type Description Default request Request Client request information. required param str Parameter name. required Returns: Name Type Description response Dict Response containing the value of the specific configuration parameter in the ' ' field if it exists, otherwise 'Not found' value. Source code in src/trecover/app/api/trecoverapi.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 @api . get ( '/config/ {param} ' , tags = [ 'Configuration' ]) @construct_response def config_param ( request : Request , param : str ) -> Dict : \"\"\" Get the specific configuration parameter of the model used for inference. Parameters ---------- request : Request Client request information. param : str Parameter name. Returns ------- response : Dict Response containing the value of the specific configuration parameter in the '<param>' field if it exists, otherwise 'Not found' value. \"\"\" task = get_model_config . delay () model_config = task . get () response = { 'message' : HTTPStatus . OK . phrase , 'status_code' : HTTPStatus . OK , param : model_config . get ( param , 'Not found' ) } return response","title":"config_param()"},{"location":"src/trecover/app/api/#src.trecover.app.api.trecoverapi.recover","text":"recover ( request , payload ) Perform keyless reading. Parameters: Name Type Description Default request Request Client request information. required payload PredictPayload Data for keyless reading. required Returns: Name Type Description response TaskResponse Response containing the id of the celery task in the 'task_id' field. Source code in src/trecover/app/api/trecoverapi.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 @api . post ( '/recover' , tags = [ 'Prediction' ], response_model = TaskResponse ) @construct_response def recover ( request : Request , payload : PredictPayload ) -> Dict : \"\"\" Perform keyless reading. Parameters ---------- request : Request Client request information. payload : PredictPayload Data for keyless reading. Returns ------- response : TaskResponse Response containing the id of the celery task in the 'task_id' field. \"\"\" task = predict . delay ( payload . columns , payload . beam_width , payload . delimiter ) response = { 'message' : HTTPStatus . ACCEPTED . phrase , 'status_code' : HTTPStatus . ACCEPTED , 'task_id' : task . id } return response","title":"recover()"},{"location":"src/trecover/app/api/#src.trecover.app.api.trecoverapi.status","text":"status ( request , task_id = Path ( Ellipsis , title = \"The ID of the task to get status\" , regex = \"[a-z0-9] {8} -[a-z0-9] {4} -[a-z0-9] {4} -[a-z0-9] {4} -[a-z0-9] {12} \" , ), ) Get a celery task status. Parameters: Name Type Description Default request Request Client request information. required task_id str Celery task id. Path(Ellipsis, title='The ID of the task to get status', regex='[a-z0-9]{8}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{12}') Returns: Name Type Description response PredictResponse Response containing the status of the celery task in the 'state' and 'progress' fields if it's still in process, error information in 'message' and 'status_code' fields if it's failed, otherwise the result of keyless reading in the 'chains' field. Source code in src/trecover/app/api/trecoverapi.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 @api . get ( '/status/ {task_id} ' , tags = [ 'Prediction' ], response_model = PredictResponse ) @construct_response def status ( request : Request , task_id : str = Path ( ... , title = 'The ID of the task to get status' , regex = r '[a-z0-9] {8} -[a-z0-9] {4} -[a-z0-9] {4} -[a-z0-9] {4} -[a-z0-9] {12} ' ) ) -> Dict : \"\"\" Get a celery task status. Parameters ---------- request : Request Client request information. task_id : str Celery task id. Returns ------- response : PredictResponse Response containing the status of the celery task in the 'state' and 'progress' fields if it's still in process, error information in 'message' and 'status_code' fields if it's failed, otherwise the result of keyless reading in the 'chains' field. \"\"\" task = AsyncResult ( task_id , app = celery_app ) if task . failed (): response = { 'message' : str ( task . info ), 'status_code' : HTTPStatus . CONFLICT } elif task . ready (): columns , chains = task . get () response = { 'message' : HTTPStatus . OK . phrase , 'status_code' : HTTPStatus . OK , 'columns' : columns , 'chains' : chains , 'progress' : len ( columns ) } else : info = task . info response = { 'message' : HTTPStatus . PROCESSING . phrase , 'status_code' : HTTPStatus . PROCESSING , 'state' : task . status , 'progress' : info . get ( 'progress' ) if isinstance ( info , dict ) else None } return response","title":"status()"},{"location":"src/trecover/app/api/#src.trecover.app.api.trecoverapi.delete_prediction","text":"delete_prediction ( request , task_id = Path ( Ellipsis , title = \"The ID of the task to forget\" , regex = \"[a-z0-9] {8} -[a-z0-9] {4} -[a-z0-9] {4} -[a-z0-9] {4} -[a-z0-9] {12} \" , ), ) Delete task result in the celery backend database. Parameters: Name Type Description Default request Request Client request information. required task_id str Task ID to delete its result from celery backend database. Path(Ellipsis, title='The ID of the task to forget', regex='[a-z0-9]{8}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{12}') Returns: Name Type Description response Dict OK phrase. Source code in src/trecover/app/api/trecoverapi.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 @api . delete ( '/ {task_id} ' , tags = [ 'Prediction' ]) @construct_response def delete_prediction ( request : Request , task_id : str = Path ( ... , title = 'The ID of the task to forget' , regex = r '[a-z0-9] {8} -[a-z0-9] {4} -[a-z0-9] {4} -[a-z0-9] {4} -[a-z0-9] {12} ' ) ) -> Dict : \"\"\" Delete task result in the celery backend database. Parameters ---------- request : Request Client request information. task_id : str Task ID to delete its result from celery backend database. Returns ------- response : Dict OK phrase. \"\"\" task = AsyncResult ( task_id , app = celery_app ) if task . ready (): task . forget () else : task . revoke ( terminate = True ) response = { 'message' : HTTPStatus . OK . phrase , 'status_code' : HTTPStatus . OK } return response","title":"delete_prediction()"},{"location":"src/trecover/app/api/schemas/","text":"PredictPayload Bases: BaseModel Content declaration of the request body to keyless read. Source code in src/trecover/app/api/schemas.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class PredictPayload ( BaseModel ): \"\"\" Content declaration of the request body to keyless read. \"\"\" columns : List [ str ] beam_width : int delimiter : Optional [ str ] = '' @validator ( 'columns' ) def columns_validator ( cls , columns : List [ str ]) -> List [ str ]: \"\"\" Check columns to keyless read. Parameters ---------- columns: List[str] Columns to keyless read Returns ------- data : List[str] Columns to keyless read. Raises ------ ValueError: If at least one column is empty. \"\"\" if any ([ len ( column ) == 0 for column in columns ]): raise ValueError ( f 'Columns to keyless read must contain at least one character' ) return columns @validator ( 'beam_width' ) def beam_width_validator ( cls , beam_width : int ) -> int : \"\"\" Check columns to keyless read. Parameters ---------- beam_width: int Columns to keyless read Returns ------- beam_width : int Width of beam search algorithm. Raises ------ ValueError: If the beam width is out of range. \"\"\" if not 1 <= beam_width <= len ( var . ALPHABET ): raise ValueError ( f 'Beam width must be in between 1 and { len ( var . ALPHABET ) } ' ) return beam_width class Config : \"\"\" PredictPayload example for API documentation\"\"\" schema_extra = { 'example' : { 'columns' : [ 'adin' , 'scjz' , 'pzxz' , 'evft' , 'odev' , 'pkzc' , 'lipss' , 'eayh' , 'aepqc' , 'rvuqh' , 'ozldx' , 'ulgq' , 'nabhr' , 'dvvf' , 'tmil' , 'huiow' , 'euzq' , 'cdmh' , 'ompzv' , 'uguq' , 'ntvqi' , 'tctm' , 'rwgi' , 'yuonh' , 'wkvr' , 'efau' , 'nrele' , 'tbhxq' , 'iiqd' , 'nhwfy' , 'tymio' , 'onzpj' , 'turho' , 'hhgnd' , 'ehmof' , 'scpoi' , 'tyqbn' , 'roul' , 'exoe' , 'edwyc' , 'thid' , 'snqrp' , 'tmmh' , 'oejg' , 'cjbaw' , 'hgzmb' , 'ezlwj' , 'esxb' , 'rria' , 'taeo' , 'hpnln' , 'eeomf' , 'cvxr' , 'ofwy' , 'nuon' , 'vvex' , 'ikbd' , 'cmusb' , 'tdnkb' , 'iccd' , 'ocay' , 'nnxds' , 'slbgd' , 'oupap' , 'mhip' , 'ebtv' , 'bipta' , 'unazv' , 'sccol' , 'iovq' , 'nxzev' , 'ezaz' , 'sakm' , 'skls' , 'eipvd' , 'segz' , 'inrlo' , 'nzkug' , 'puau' , 'oolt' , 'recs' , 'tjot' , 'lwds' , 'assb' , 'ntyzf' , 'dpad' , 'byep' , 'owgol' , 'aiwbb' , 'ralk' , 'dffos' , 'ejfp' , 'dwyug' , 'uxucm' , 'ppch' , 'teznz' , 'hpfbv' , 'eaocf' , 'iist' , 'rdlyx' , 'wseu' , 'ivgg' , 'nabi' , 'dtuiw' , 'oxzje' , 'wderj' , 'sawm' , 'odpw' , 'noqs' , 'cimnf' , 'eukjx' , 'aszy' , 'ghmiq' , 'ajcsd' , 'ipwgf' , 'nhth' , 'tpvfp' , 'hnede' , 'aclb' , 'tteuq' , 'nbvi' , 'isgc' , 'gejbz' , 'hufnl' , 'tggvn' , 'aahfn' , 'skkxl' , 'mmwq' , 'anqgw' , 'lrvji' , 'lker' , 'gvqt' , 'rdcd' , 'obvt' , 'ussht' , 'piwb' , 'ozbn' , 'fyig' , 'aczx' , 'clak' , 'tgnxi' , 'iieyi' , 'vqyqo' , 'ijgp' , 'shfth' , 'tnbeg' , 'sbuzx' , 'wwnja' , 'eguhn' , 'aneai' , 'rpsl' , 'izut' , 'njixa' , 'grgh' , 'bkypw' , 'lliv' , 'azeob' , 'cojl' , 'kmqnd' , 'ahpq' , 'pckw' , 'phvpa' , 'rgqv' , 'opqby' , 'aiqsp' , 'clirf' , 'hlrp' , 'eztn' , 'dgek' , 'azubn' , 'gnskd' , 'rajo' , 'okgv' , 'ugcp' , 'pilf' , 'oxbx' , 'fwqq' , 'jsmf' , 'okgw' , 'uvjyg' , 'rfio' , 'nbtcy' , 'aryp' , 'ledgj' , 'inex' , 'sdqmm' , 'tcmtw' , 'soqh' , 'tquu' , 'hrfi' , 'rsddp' , 'ewfye' , 'aumxj' , 'tssui' , 'ecgd' , 'nunfp' , 'isfka' , 'ncvx' , 'gqxi' , 'typc' , 'okcdt' , 'sgcaq' , 'mbcwd' , 'angb' , 'svyl' , 'hjmk' , 'tlcgd' , 'hpka' , 'efule' , 'cbjou' , 'agjjr' , 'mziur' , 'emoqh' , 'rajte' , 'ahoz' , 'shaxe' , 'okeyj' , 'fgwnq' , 'twojw' , 'hcyfc' , 'ojehd' , 'soij' , 'ekhpc' , 'wbyq' , 'hdfi' , 'opyas' , 'rzba' , 'ediml' , 'mvwys' , 'avtmz' , 'ilvj' , 'nefy' , 'eycng' , 'dqdk' , 'oihcj' , 'nzuw' , 'srrye' , 'ccsi' , 'emhnz' , 'neosn' , 'ewzki' ], 'beam_width' : 5 , 'delimiter' : '' } } Config PredictPayload example for API documentation Source code in src/trecover/app/api/schemas.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class Config : \"\"\" PredictPayload example for API documentation\"\"\" schema_extra = { 'example' : { 'columns' : [ 'adin' , 'scjz' , 'pzxz' , 'evft' , 'odev' , 'pkzc' , 'lipss' , 'eayh' , 'aepqc' , 'rvuqh' , 'ozldx' , 'ulgq' , 'nabhr' , 'dvvf' , 'tmil' , 'huiow' , 'euzq' , 'cdmh' , 'ompzv' , 'uguq' , 'ntvqi' , 'tctm' , 'rwgi' , 'yuonh' , 'wkvr' , 'efau' , 'nrele' , 'tbhxq' , 'iiqd' , 'nhwfy' , 'tymio' , 'onzpj' , 'turho' , 'hhgnd' , 'ehmof' , 'scpoi' , 'tyqbn' , 'roul' , 'exoe' , 'edwyc' , 'thid' , 'snqrp' , 'tmmh' , 'oejg' , 'cjbaw' , 'hgzmb' , 'ezlwj' , 'esxb' , 'rria' , 'taeo' , 'hpnln' , 'eeomf' , 'cvxr' , 'ofwy' , 'nuon' , 'vvex' , 'ikbd' , 'cmusb' , 'tdnkb' , 'iccd' , 'ocay' , 'nnxds' , 'slbgd' , 'oupap' , 'mhip' , 'ebtv' , 'bipta' , 'unazv' , 'sccol' , 'iovq' , 'nxzev' , 'ezaz' , 'sakm' , 'skls' , 'eipvd' , 'segz' , 'inrlo' , 'nzkug' , 'puau' , 'oolt' , 'recs' , 'tjot' , 'lwds' , 'assb' , 'ntyzf' , 'dpad' , 'byep' , 'owgol' , 'aiwbb' , 'ralk' , 'dffos' , 'ejfp' , 'dwyug' , 'uxucm' , 'ppch' , 'teznz' , 'hpfbv' , 'eaocf' , 'iist' , 'rdlyx' , 'wseu' , 'ivgg' , 'nabi' , 'dtuiw' , 'oxzje' , 'wderj' , 'sawm' , 'odpw' , 'noqs' , 'cimnf' , 'eukjx' , 'aszy' , 'ghmiq' , 'ajcsd' , 'ipwgf' , 'nhth' , 'tpvfp' , 'hnede' , 'aclb' , 'tteuq' , 'nbvi' , 'isgc' , 'gejbz' , 'hufnl' , 'tggvn' , 'aahfn' , 'skkxl' , 'mmwq' , 'anqgw' , 'lrvji' , 'lker' , 'gvqt' , 'rdcd' , 'obvt' , 'ussht' , 'piwb' , 'ozbn' , 'fyig' , 'aczx' , 'clak' , 'tgnxi' , 'iieyi' , 'vqyqo' , 'ijgp' , 'shfth' , 'tnbeg' , 'sbuzx' , 'wwnja' , 'eguhn' , 'aneai' , 'rpsl' , 'izut' , 'njixa' , 'grgh' , 'bkypw' , 'lliv' , 'azeob' , 'cojl' , 'kmqnd' , 'ahpq' , 'pckw' , 'phvpa' , 'rgqv' , 'opqby' , 'aiqsp' , 'clirf' , 'hlrp' , 'eztn' , 'dgek' , 'azubn' , 'gnskd' , 'rajo' , 'okgv' , 'ugcp' , 'pilf' , 'oxbx' , 'fwqq' , 'jsmf' , 'okgw' , 'uvjyg' , 'rfio' , 'nbtcy' , 'aryp' , 'ledgj' , 'inex' , 'sdqmm' , 'tcmtw' , 'soqh' , 'tquu' , 'hrfi' , 'rsddp' , 'ewfye' , 'aumxj' , 'tssui' , 'ecgd' , 'nunfp' , 'isfka' , 'ncvx' , 'gqxi' , 'typc' , 'okcdt' , 'sgcaq' , 'mbcwd' , 'angb' , 'svyl' , 'hjmk' , 'tlcgd' , 'hpka' , 'efule' , 'cbjou' , 'agjjr' , 'mziur' , 'emoqh' , 'rajte' , 'ahoz' , 'shaxe' , 'okeyj' , 'fgwnq' , 'twojw' , 'hcyfc' , 'ojehd' , 'soij' , 'ekhpc' , 'wbyq' , 'hdfi' , 'opyas' , 'rzba' , 'ediml' , 'mvwys' , 'avtmz' , 'ilvj' , 'nefy' , 'eycng' , 'dqdk' , 'oihcj' , 'nzuw' , 'srrye' , 'ccsi' , 'emhnz' , 'neosn' , 'ewzki' ], 'beam_width' : 5 , 'delimiter' : '' } } columns_validator columns_validator ( columns ) Check columns to keyless read. Parameters: Name Type Description Default columns List [ str ] Columns to keyless read required Returns: Name Type Description data List [ str ] Columns to keyless read. Raises: Type Description ValueError: If at least one column is empty. Source code in src/trecover/app/api/schemas.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 @validator ( 'columns' ) def columns_validator ( cls , columns : List [ str ]) -> List [ str ]: \"\"\" Check columns to keyless read. Parameters ---------- columns: List[str] Columns to keyless read Returns ------- data : List[str] Columns to keyless read. Raises ------ ValueError: If at least one column is empty. \"\"\" if any ([ len ( column ) == 0 for column in columns ]): raise ValueError ( f 'Columns to keyless read must contain at least one character' ) return columns beam_width_validator beam_width_validator ( beam_width ) Check columns to keyless read. Parameters: Name Type Description Default beam_width int Columns to keyless read required Returns: Name Type Description beam_width int Width of beam search algorithm. Raises: Type Description ValueError: If the beam width is out of range. Source code in src/trecover/app/api/schemas.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 @validator ( 'beam_width' ) def beam_width_validator ( cls , beam_width : int ) -> int : \"\"\" Check columns to keyless read. Parameters ---------- beam_width: int Columns to keyless read Returns ------- beam_width : int Width of beam search algorithm. Raises ------ ValueError: If the beam width is out of range. \"\"\" if not 1 <= beam_width <= len ( var . ALPHABET ): raise ValueError ( f 'Beam width must be in between 1 and { len ( var . ALPHABET ) } ' ) return beam_width BaseResponse Bases: BaseModel Basic contents declaration of the response body for all API handlers. Source code in src/trecover/app/api/schemas.py 104 105 106 107 108 109 110 111 class BaseResponse ( BaseModel ): \"\"\" Basic contents declaration of the response body for all API handlers. \"\"\" message : str method : str status_code : int timestamp : str url : str TaskResponse Bases: BaseResponse Contents declaration of the 'recover' handler response body. Source code in src/trecover/app/api/schemas.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 class TaskResponse ( BaseResponse ): \"\"\" Contents declaration of the 'recover' handler response body. \"\"\" task_id : Optional [ str ] class Config : \"\"\" TaskResponse example for API documentation\"\"\" schema_extra = { 'example' : { \"message\" : \"Accepted\" , \"method\" : \"POST\" , \"status_code\" : 202 , \"timestamp\" : \"2021-09-04T12:53:35.512412\" , \"url\" : \"http://localhost:8001/recover\" , \"task_id\" : \"909e4817-cca3-4dbf-a598-f7f83c5d60c9\" } } Config TaskResponse example for API documentation Source code in src/trecover/app/api/schemas.py 119 120 121 122 123 124 125 126 127 128 129 130 131 class Config : \"\"\" TaskResponse example for API documentation\"\"\" schema_extra = { 'example' : { \"message\" : \"Accepted\" , \"method\" : \"POST\" , \"status_code\" : 202 , \"timestamp\" : \"2021-09-04T12:53:35.512412\" , \"url\" : \"http://localhost:8001/recover\" , \"task_id\" : \"909e4817-cca3-4dbf-a598-f7f83c5d60c9\" } } PredictResponse Bases: BaseResponse Contents declaration of the 'status' handler response body. Source code in src/trecover/app/api/schemas.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 class PredictResponse ( BaseResponse ): \"\"\" Contents declaration of the 'status' handler response body. \"\"\" columns : Optional [ List [ str ]] chains : Optional [ List [ Tuple ]] state : Optional [ str ] progress : Optional [ int ] class Config : \"\"\" PredictResponse example for API documentation\"\"\" schema_extra = { 'example' : { \"message\" : \"OK\" , \"method\" : \"GET\" , \"status_code\" : 200 , \"timestamp\" : \"2021-09-04T12:54:49.801614\" , \"url\" : \"http://localhost:8001/status/909e4817-cca3-4dbf-a598-f7f83c5d60c9\" , \"columns\" : [ \"adin\" , \"scjz\" , \"pzxz\" , \"evft\" , \"odev\" , \"pkzc\" , \"lipss\" , \"eayh\" , \"aepqc\" , \"rvuqh\" , \"ozldx\" , \"ulgq\" , \"nabhr\" , \"dvvf\" , \"tmil\" , \"huiow\" , \"euzq\" , \"cdmh\" , \"ompzv\" , \"uguq\" , \"ntvqi\" , \"tctm\" , \"rwgi\" , \"yuonh\" , \"wkvr\" , \"efau\" , \"nrele\" , \"tbhxq\" , \"iiqd\" , \"nhwfy\" , \"tymio\" , \"onzpj\" , \"turho\" , \"hhgnd\" , \"ehmof\" , \"scpoi\" , \"tyqbn\" , \"roul\" , \"exoe\" , \"edwyc\" , \"thid\" , \"snqrp\" , \"tmmh\" , \"oejg\" , \"cjbaw\" , \"hgzmb\" , \"ezlwj\" , \"esxb\" , \"rria\" , \"taeo\" , \"hpnln\" , \"eeomf\" , \"cvxr\" , \"ofwy\" , \"nuon\" , \"vvex\" , \"ikbd\" , \"cmusb\" , \"tdnkb\" , \"iccd\" , \"ocay\" , \"nnxds\" , \"slbgd\" , \"oupap\" , \"mhip\" , \"ebtv\" , \"bipta\" , \"unazv\" , \"sccol\" , \"iovq\" , \"nxzev\" , \"ezaz\" , \"sakm\" , \"skls\" , \"eipvd\" , \"segz\" , \"inrlo\" , \"nzkug\" , \"puau\" , \"oolt\" , \"recs\" , \"tjot\" , \"lwds\" , \"assb\" , \"ntyzf\" , \"dpad\" , \"byep\" , \"owgol\" , \"aiwbb\" , \"ralk\" , \"dffos\" , \"ejfp\" , \"dwyug\" , \"uxucm\" , \"ppch\" , \"teznz\" , \"hpfbv\" , \"eaocf\" , \"iist\" , \"rdlyx\" , \"wseu\" , \"ivgg\" , \"nabi\" , \"dtuiw\" , \"oxzje\" , \"wderj\" , \"sawm\" , \"odpw\" , \"noqs\" , \"cimnf\" , \"eukjx\" , \"aszy\" , \"ghmiq\" , \"ajcsd\" , \"ipwgf\" , \"nhth\" , \"tpvfp\" , \"hnede\" , \"aclb\" , \"tteuq\" , \"nbvi\" , \"isgc\" , \"gejbz\" , \"hufnl\" , \"tggvn\" , \"aahfn\" , \"skkxl\" , \"mmwq\" , \"anqgw\" , \"lrvji\" , \"lker\" , \"gvqt\" , \"rdcd\" , \"obvt\" , \"ussht\" , \"piwb\" , \"ozbn\" , \"fyig\" , \"aczx\" , \"clak\" , \"tgnxi\" , \"iieyi\" , \"vqyqo\" , \"ijgp\" , \"shfth\" , \"tnbeg\" , \"sbuzx\" , \"wwnja\" , \"eguhn\" , \"aneai\" , \"rpsl\" , \"izut\" , \"njixa\" , \"grgh\" , \"bkypw\" , \"lliv\" , \"azeob\" , \"cojl\" , \"kmqnd\" , \"ahpq\" , \"pckw\" , \"phvpa\" , \"rgqv\" , \"opqby\" , \"aiqsp\" , \"clirf\" , \"hlrp\" , \"eztn\" , \"dgek\" , \"azubn\" , \"gnskd\" , \"rajo\" , \"okgv\" , \"ugcp\" , \"pilf\" , \"oxbx\" , \"fwqq\" , \"jsmf\" , \"okgw\" , \"uvjyg\" , \"rfio\" , \"nbtcy\" , \"aryp\" , \"ledgj\" , \"inex\" , \"sdqmm\" , \"tcmtw\" , \"soqh\" , \"tquu\" , \"hrfi\" , \"rsddp\" , \"ewfye\" , \"aumxj\" , \"tssui\" , \"ecgd\" , \"nunfp\" , \"isfka\" , \"ncvx\" , \"gqxi\" , \"typc\" , \"okcdt\" , \"sgcaq\" , \"mbcwd\" , \"angb\" , \"svyl\" , \"hjmk\" , \"tlcgd\" , \"hpka\" , \"efule\" , \"cbjou\" , \"agjjr\" , \"mziur\" , \"emoqh\" , \"rajte\" , \"ahoz\" , \"shaxe\" , \"okeyj\" , \"fgwnq\" , \"twojw\" , \"hcyfc\" , \"ojehd\" , \"soij\" , \"ekhpc\" , \"wbyq\" , \"hdfi\" , \"opyas\" , \"rzba\" , \"ediml\" , \"mvwys\" , \"avtmz\" , \"ilvj\" , \"nefy\" , \"eycng\" , \"dqdk\" , \"oihcj\" , \"nzuw\" , \"srrye\" , \"ccsi\" , \"emhnz\" , \"neosn\" , \"ewzki\" ], \"chains\" : [ [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsonceagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"courseateningtoawalkthecametheonthesewisremainedourcesw\" , - 33.01948561671179 ], [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsdoneagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"courseateningtoawalkthecametheonthesewisremainedourcesw\" , - 33.05044434355386 ], [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsdoneagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"courseateningtoawalkthecametheonthesewisremainedourchew\" , - 33.686079347001396 ], [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsdoneagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"coursexteningtoawalkthecametheonthesewisremainedourchew\" , - 33.83420559252136 ], [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsonceagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"courseateningtoawalkthecametheonthesewisremainedourchew\" , - 33.948646204201395 ] ], \"state\" : None , \"progress\" : None } } Config PredictResponse example for API documentation Source code in src/trecover/app/api/schemas.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 class Config : \"\"\" PredictResponse example for API documentation\"\"\" schema_extra = { 'example' : { \"message\" : \"OK\" , \"method\" : \"GET\" , \"status_code\" : 200 , \"timestamp\" : \"2021-09-04T12:54:49.801614\" , \"url\" : \"http://localhost:8001/status/909e4817-cca3-4dbf-a598-f7f83c5d60c9\" , \"columns\" : [ \"adin\" , \"scjz\" , \"pzxz\" , \"evft\" , \"odev\" , \"pkzc\" , \"lipss\" , \"eayh\" , \"aepqc\" , \"rvuqh\" , \"ozldx\" , \"ulgq\" , \"nabhr\" , \"dvvf\" , \"tmil\" , \"huiow\" , \"euzq\" , \"cdmh\" , \"ompzv\" , \"uguq\" , \"ntvqi\" , \"tctm\" , \"rwgi\" , \"yuonh\" , \"wkvr\" , \"efau\" , \"nrele\" , \"tbhxq\" , \"iiqd\" , \"nhwfy\" , \"tymio\" , \"onzpj\" , \"turho\" , \"hhgnd\" , \"ehmof\" , \"scpoi\" , \"tyqbn\" , \"roul\" , \"exoe\" , \"edwyc\" , \"thid\" , \"snqrp\" , \"tmmh\" , \"oejg\" , \"cjbaw\" , \"hgzmb\" , \"ezlwj\" , \"esxb\" , \"rria\" , \"taeo\" , \"hpnln\" , \"eeomf\" , \"cvxr\" , \"ofwy\" , \"nuon\" , \"vvex\" , \"ikbd\" , \"cmusb\" , \"tdnkb\" , \"iccd\" , \"ocay\" , \"nnxds\" , \"slbgd\" , \"oupap\" , \"mhip\" , \"ebtv\" , \"bipta\" , \"unazv\" , \"sccol\" , \"iovq\" , \"nxzev\" , \"ezaz\" , \"sakm\" , \"skls\" , \"eipvd\" , \"segz\" , \"inrlo\" , \"nzkug\" , \"puau\" , \"oolt\" , \"recs\" , \"tjot\" , \"lwds\" , \"assb\" , \"ntyzf\" , \"dpad\" , \"byep\" , \"owgol\" , \"aiwbb\" , \"ralk\" , \"dffos\" , \"ejfp\" , \"dwyug\" , \"uxucm\" , \"ppch\" , \"teznz\" , \"hpfbv\" , \"eaocf\" , \"iist\" , \"rdlyx\" , \"wseu\" , \"ivgg\" , \"nabi\" , \"dtuiw\" , \"oxzje\" , \"wderj\" , \"sawm\" , \"odpw\" , \"noqs\" , \"cimnf\" , \"eukjx\" , \"aszy\" , \"ghmiq\" , \"ajcsd\" , \"ipwgf\" , \"nhth\" , \"tpvfp\" , \"hnede\" , \"aclb\" , \"tteuq\" , \"nbvi\" , \"isgc\" , \"gejbz\" , \"hufnl\" , \"tggvn\" , \"aahfn\" , \"skkxl\" , \"mmwq\" , \"anqgw\" , \"lrvji\" , \"lker\" , \"gvqt\" , \"rdcd\" , \"obvt\" , \"ussht\" , \"piwb\" , \"ozbn\" , \"fyig\" , \"aczx\" , \"clak\" , \"tgnxi\" , \"iieyi\" , \"vqyqo\" , \"ijgp\" , \"shfth\" , \"tnbeg\" , \"sbuzx\" , \"wwnja\" , \"eguhn\" , \"aneai\" , \"rpsl\" , \"izut\" , \"njixa\" , \"grgh\" , \"bkypw\" , \"lliv\" , \"azeob\" , \"cojl\" , \"kmqnd\" , \"ahpq\" , \"pckw\" , \"phvpa\" , \"rgqv\" , \"opqby\" , \"aiqsp\" , \"clirf\" , \"hlrp\" , \"eztn\" , \"dgek\" , \"azubn\" , \"gnskd\" , \"rajo\" , \"okgv\" , \"ugcp\" , \"pilf\" , \"oxbx\" , \"fwqq\" , \"jsmf\" , \"okgw\" , \"uvjyg\" , \"rfio\" , \"nbtcy\" , \"aryp\" , \"ledgj\" , \"inex\" , \"sdqmm\" , \"tcmtw\" , \"soqh\" , \"tquu\" , \"hrfi\" , \"rsddp\" , \"ewfye\" , \"aumxj\" , \"tssui\" , \"ecgd\" , \"nunfp\" , \"isfka\" , \"ncvx\" , \"gqxi\" , \"typc\" , \"okcdt\" , \"sgcaq\" , \"mbcwd\" , \"angb\" , \"svyl\" , \"hjmk\" , \"tlcgd\" , \"hpka\" , \"efule\" , \"cbjou\" , \"agjjr\" , \"mziur\" , \"emoqh\" , \"rajte\" , \"ahoz\" , \"shaxe\" , \"okeyj\" , \"fgwnq\" , \"twojw\" , \"hcyfc\" , \"ojehd\" , \"soij\" , \"ekhpc\" , \"wbyq\" , \"hdfi\" , \"opyas\" , \"rzba\" , \"ediml\" , \"mvwys\" , \"avtmz\" , \"ilvj\" , \"nefy\" , \"eycng\" , \"dqdk\" , \"oihcj\" , \"nzuw\" , \"srrye\" , \"ccsi\" , \"emhnz\" , \"neosn\" , \"ewzki\" ], \"chains\" : [ [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsonceagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"courseateningtoawalkthecametheonthesewisremainedourcesw\" , - 33.01948561671179 ], [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsdoneagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"courseateningtoawalkthecametheonthesewisremainedourcesw\" , - 33.05044434355386 ], [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsdoneagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"courseateningtoawalkthecametheonthesewisremainedourchew\" , - 33.686079347001396 ], [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsdoneagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"coursexteningtoawalkthecametheonthesewisremainedourchew\" , - 33.83420559252136 ], [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsonceagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"courseateningtoawalkthecametheonthesewisremainedourchew\" , - 33.948646204201395 ] ], \"state\" : None , \"progress\" : None } }","title":"Schemas"},{"location":"src/trecover/app/api/schemas/#src.trecover.app.api.schemas.PredictPayload","text":"Bases: BaseModel Content declaration of the request body to keyless read. Source code in src/trecover/app/api/schemas.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class PredictPayload ( BaseModel ): \"\"\" Content declaration of the request body to keyless read. \"\"\" columns : List [ str ] beam_width : int delimiter : Optional [ str ] = '' @validator ( 'columns' ) def columns_validator ( cls , columns : List [ str ]) -> List [ str ]: \"\"\" Check columns to keyless read. Parameters ---------- columns: List[str] Columns to keyless read Returns ------- data : List[str] Columns to keyless read. Raises ------ ValueError: If at least one column is empty. \"\"\" if any ([ len ( column ) == 0 for column in columns ]): raise ValueError ( f 'Columns to keyless read must contain at least one character' ) return columns @validator ( 'beam_width' ) def beam_width_validator ( cls , beam_width : int ) -> int : \"\"\" Check columns to keyless read. Parameters ---------- beam_width: int Columns to keyless read Returns ------- beam_width : int Width of beam search algorithm. Raises ------ ValueError: If the beam width is out of range. \"\"\" if not 1 <= beam_width <= len ( var . ALPHABET ): raise ValueError ( f 'Beam width must be in between 1 and { len ( var . ALPHABET ) } ' ) return beam_width class Config : \"\"\" PredictPayload example for API documentation\"\"\" schema_extra = { 'example' : { 'columns' : [ 'adin' , 'scjz' , 'pzxz' , 'evft' , 'odev' , 'pkzc' , 'lipss' , 'eayh' , 'aepqc' , 'rvuqh' , 'ozldx' , 'ulgq' , 'nabhr' , 'dvvf' , 'tmil' , 'huiow' , 'euzq' , 'cdmh' , 'ompzv' , 'uguq' , 'ntvqi' , 'tctm' , 'rwgi' , 'yuonh' , 'wkvr' , 'efau' , 'nrele' , 'tbhxq' , 'iiqd' , 'nhwfy' , 'tymio' , 'onzpj' , 'turho' , 'hhgnd' , 'ehmof' , 'scpoi' , 'tyqbn' , 'roul' , 'exoe' , 'edwyc' , 'thid' , 'snqrp' , 'tmmh' , 'oejg' , 'cjbaw' , 'hgzmb' , 'ezlwj' , 'esxb' , 'rria' , 'taeo' , 'hpnln' , 'eeomf' , 'cvxr' , 'ofwy' , 'nuon' , 'vvex' , 'ikbd' , 'cmusb' , 'tdnkb' , 'iccd' , 'ocay' , 'nnxds' , 'slbgd' , 'oupap' , 'mhip' , 'ebtv' , 'bipta' , 'unazv' , 'sccol' , 'iovq' , 'nxzev' , 'ezaz' , 'sakm' , 'skls' , 'eipvd' , 'segz' , 'inrlo' , 'nzkug' , 'puau' , 'oolt' , 'recs' , 'tjot' , 'lwds' , 'assb' , 'ntyzf' , 'dpad' , 'byep' , 'owgol' , 'aiwbb' , 'ralk' , 'dffos' , 'ejfp' , 'dwyug' , 'uxucm' , 'ppch' , 'teznz' , 'hpfbv' , 'eaocf' , 'iist' , 'rdlyx' , 'wseu' , 'ivgg' , 'nabi' , 'dtuiw' , 'oxzje' , 'wderj' , 'sawm' , 'odpw' , 'noqs' , 'cimnf' , 'eukjx' , 'aszy' , 'ghmiq' , 'ajcsd' , 'ipwgf' , 'nhth' , 'tpvfp' , 'hnede' , 'aclb' , 'tteuq' , 'nbvi' , 'isgc' , 'gejbz' , 'hufnl' , 'tggvn' , 'aahfn' , 'skkxl' , 'mmwq' , 'anqgw' , 'lrvji' , 'lker' , 'gvqt' , 'rdcd' , 'obvt' , 'ussht' , 'piwb' , 'ozbn' , 'fyig' , 'aczx' , 'clak' , 'tgnxi' , 'iieyi' , 'vqyqo' , 'ijgp' , 'shfth' , 'tnbeg' , 'sbuzx' , 'wwnja' , 'eguhn' , 'aneai' , 'rpsl' , 'izut' , 'njixa' , 'grgh' , 'bkypw' , 'lliv' , 'azeob' , 'cojl' , 'kmqnd' , 'ahpq' , 'pckw' , 'phvpa' , 'rgqv' , 'opqby' , 'aiqsp' , 'clirf' , 'hlrp' , 'eztn' , 'dgek' , 'azubn' , 'gnskd' , 'rajo' , 'okgv' , 'ugcp' , 'pilf' , 'oxbx' , 'fwqq' , 'jsmf' , 'okgw' , 'uvjyg' , 'rfio' , 'nbtcy' , 'aryp' , 'ledgj' , 'inex' , 'sdqmm' , 'tcmtw' , 'soqh' , 'tquu' , 'hrfi' , 'rsddp' , 'ewfye' , 'aumxj' , 'tssui' , 'ecgd' , 'nunfp' , 'isfka' , 'ncvx' , 'gqxi' , 'typc' , 'okcdt' , 'sgcaq' , 'mbcwd' , 'angb' , 'svyl' , 'hjmk' , 'tlcgd' , 'hpka' , 'efule' , 'cbjou' , 'agjjr' , 'mziur' , 'emoqh' , 'rajte' , 'ahoz' , 'shaxe' , 'okeyj' , 'fgwnq' , 'twojw' , 'hcyfc' , 'ojehd' , 'soij' , 'ekhpc' , 'wbyq' , 'hdfi' , 'opyas' , 'rzba' , 'ediml' , 'mvwys' , 'avtmz' , 'ilvj' , 'nefy' , 'eycng' , 'dqdk' , 'oihcj' , 'nzuw' , 'srrye' , 'ccsi' , 'emhnz' , 'neosn' , 'ewzki' ], 'beam_width' : 5 , 'delimiter' : '' } }","title":"PredictPayload"},{"location":"src/trecover/app/api/schemas/#src.trecover.app.api.schemas.PredictPayload.Config","text":"PredictPayload example for API documentation Source code in src/trecover/app/api/schemas.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class Config : \"\"\" PredictPayload example for API documentation\"\"\" schema_extra = { 'example' : { 'columns' : [ 'adin' , 'scjz' , 'pzxz' , 'evft' , 'odev' , 'pkzc' , 'lipss' , 'eayh' , 'aepqc' , 'rvuqh' , 'ozldx' , 'ulgq' , 'nabhr' , 'dvvf' , 'tmil' , 'huiow' , 'euzq' , 'cdmh' , 'ompzv' , 'uguq' , 'ntvqi' , 'tctm' , 'rwgi' , 'yuonh' , 'wkvr' , 'efau' , 'nrele' , 'tbhxq' , 'iiqd' , 'nhwfy' , 'tymio' , 'onzpj' , 'turho' , 'hhgnd' , 'ehmof' , 'scpoi' , 'tyqbn' , 'roul' , 'exoe' , 'edwyc' , 'thid' , 'snqrp' , 'tmmh' , 'oejg' , 'cjbaw' , 'hgzmb' , 'ezlwj' , 'esxb' , 'rria' , 'taeo' , 'hpnln' , 'eeomf' , 'cvxr' , 'ofwy' , 'nuon' , 'vvex' , 'ikbd' , 'cmusb' , 'tdnkb' , 'iccd' , 'ocay' , 'nnxds' , 'slbgd' , 'oupap' , 'mhip' , 'ebtv' , 'bipta' , 'unazv' , 'sccol' , 'iovq' , 'nxzev' , 'ezaz' , 'sakm' , 'skls' , 'eipvd' , 'segz' , 'inrlo' , 'nzkug' , 'puau' , 'oolt' , 'recs' , 'tjot' , 'lwds' , 'assb' , 'ntyzf' , 'dpad' , 'byep' , 'owgol' , 'aiwbb' , 'ralk' , 'dffos' , 'ejfp' , 'dwyug' , 'uxucm' , 'ppch' , 'teznz' , 'hpfbv' , 'eaocf' , 'iist' , 'rdlyx' , 'wseu' , 'ivgg' , 'nabi' , 'dtuiw' , 'oxzje' , 'wderj' , 'sawm' , 'odpw' , 'noqs' , 'cimnf' , 'eukjx' , 'aszy' , 'ghmiq' , 'ajcsd' , 'ipwgf' , 'nhth' , 'tpvfp' , 'hnede' , 'aclb' , 'tteuq' , 'nbvi' , 'isgc' , 'gejbz' , 'hufnl' , 'tggvn' , 'aahfn' , 'skkxl' , 'mmwq' , 'anqgw' , 'lrvji' , 'lker' , 'gvqt' , 'rdcd' , 'obvt' , 'ussht' , 'piwb' , 'ozbn' , 'fyig' , 'aczx' , 'clak' , 'tgnxi' , 'iieyi' , 'vqyqo' , 'ijgp' , 'shfth' , 'tnbeg' , 'sbuzx' , 'wwnja' , 'eguhn' , 'aneai' , 'rpsl' , 'izut' , 'njixa' , 'grgh' , 'bkypw' , 'lliv' , 'azeob' , 'cojl' , 'kmqnd' , 'ahpq' , 'pckw' , 'phvpa' , 'rgqv' , 'opqby' , 'aiqsp' , 'clirf' , 'hlrp' , 'eztn' , 'dgek' , 'azubn' , 'gnskd' , 'rajo' , 'okgv' , 'ugcp' , 'pilf' , 'oxbx' , 'fwqq' , 'jsmf' , 'okgw' , 'uvjyg' , 'rfio' , 'nbtcy' , 'aryp' , 'ledgj' , 'inex' , 'sdqmm' , 'tcmtw' , 'soqh' , 'tquu' , 'hrfi' , 'rsddp' , 'ewfye' , 'aumxj' , 'tssui' , 'ecgd' , 'nunfp' , 'isfka' , 'ncvx' , 'gqxi' , 'typc' , 'okcdt' , 'sgcaq' , 'mbcwd' , 'angb' , 'svyl' , 'hjmk' , 'tlcgd' , 'hpka' , 'efule' , 'cbjou' , 'agjjr' , 'mziur' , 'emoqh' , 'rajte' , 'ahoz' , 'shaxe' , 'okeyj' , 'fgwnq' , 'twojw' , 'hcyfc' , 'ojehd' , 'soij' , 'ekhpc' , 'wbyq' , 'hdfi' , 'opyas' , 'rzba' , 'ediml' , 'mvwys' , 'avtmz' , 'ilvj' , 'nefy' , 'eycng' , 'dqdk' , 'oihcj' , 'nzuw' , 'srrye' , 'ccsi' , 'emhnz' , 'neosn' , 'ewzki' ], 'beam_width' : 5 , 'delimiter' : '' } }","title":"Config"},{"location":"src/trecover/app/api/schemas/#src.trecover.app.api.schemas.PredictPayload.columns_validator","text":"columns_validator ( columns ) Check columns to keyless read. Parameters: Name Type Description Default columns List [ str ] Columns to keyless read required Returns: Name Type Description data List [ str ] Columns to keyless read. Raises: Type Description ValueError: If at least one column is empty. Source code in src/trecover/app/api/schemas.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 @validator ( 'columns' ) def columns_validator ( cls , columns : List [ str ]) -> List [ str ]: \"\"\" Check columns to keyless read. Parameters ---------- columns: List[str] Columns to keyless read Returns ------- data : List[str] Columns to keyless read. Raises ------ ValueError: If at least one column is empty. \"\"\" if any ([ len ( column ) == 0 for column in columns ]): raise ValueError ( f 'Columns to keyless read must contain at least one character' ) return columns","title":"columns_validator()"},{"location":"src/trecover/app/api/schemas/#src.trecover.app.api.schemas.PredictPayload.beam_width_validator","text":"beam_width_validator ( beam_width ) Check columns to keyless read. Parameters: Name Type Description Default beam_width int Columns to keyless read required Returns: Name Type Description beam_width int Width of beam search algorithm. Raises: Type Description ValueError: If the beam width is out of range. Source code in src/trecover/app/api/schemas.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 @validator ( 'beam_width' ) def beam_width_validator ( cls , beam_width : int ) -> int : \"\"\" Check columns to keyless read. Parameters ---------- beam_width: int Columns to keyless read Returns ------- beam_width : int Width of beam search algorithm. Raises ------ ValueError: If the beam width is out of range. \"\"\" if not 1 <= beam_width <= len ( var . ALPHABET ): raise ValueError ( f 'Beam width must be in between 1 and { len ( var . ALPHABET ) } ' ) return beam_width","title":"beam_width_validator()"},{"location":"src/trecover/app/api/schemas/#src.trecover.app.api.schemas.BaseResponse","text":"Bases: BaseModel Basic contents declaration of the response body for all API handlers. Source code in src/trecover/app/api/schemas.py 104 105 106 107 108 109 110 111 class BaseResponse ( BaseModel ): \"\"\" Basic contents declaration of the response body for all API handlers. \"\"\" message : str method : str status_code : int timestamp : str url : str","title":"BaseResponse"},{"location":"src/trecover/app/api/schemas/#src.trecover.app.api.schemas.TaskResponse","text":"Bases: BaseResponse Contents declaration of the 'recover' handler response body. Source code in src/trecover/app/api/schemas.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 class TaskResponse ( BaseResponse ): \"\"\" Contents declaration of the 'recover' handler response body. \"\"\" task_id : Optional [ str ] class Config : \"\"\" TaskResponse example for API documentation\"\"\" schema_extra = { 'example' : { \"message\" : \"Accepted\" , \"method\" : \"POST\" , \"status_code\" : 202 , \"timestamp\" : \"2021-09-04T12:53:35.512412\" , \"url\" : \"http://localhost:8001/recover\" , \"task_id\" : \"909e4817-cca3-4dbf-a598-f7f83c5d60c9\" } }","title":"TaskResponse"},{"location":"src/trecover/app/api/schemas/#src.trecover.app.api.schemas.TaskResponse.Config","text":"TaskResponse example for API documentation Source code in src/trecover/app/api/schemas.py 119 120 121 122 123 124 125 126 127 128 129 130 131 class Config : \"\"\" TaskResponse example for API documentation\"\"\" schema_extra = { 'example' : { \"message\" : \"Accepted\" , \"method\" : \"POST\" , \"status_code\" : 202 , \"timestamp\" : \"2021-09-04T12:53:35.512412\" , \"url\" : \"http://localhost:8001/recover\" , \"task_id\" : \"909e4817-cca3-4dbf-a598-f7f83c5d60c9\" } }","title":"Config"},{"location":"src/trecover/app/api/schemas/#src.trecover.app.api.schemas.PredictResponse","text":"Bases: BaseResponse Contents declaration of the 'status' handler response body. Source code in src/trecover/app/api/schemas.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 class PredictResponse ( BaseResponse ): \"\"\" Contents declaration of the 'status' handler response body. \"\"\" columns : Optional [ List [ str ]] chains : Optional [ List [ Tuple ]] state : Optional [ str ] progress : Optional [ int ] class Config : \"\"\" PredictResponse example for API documentation\"\"\" schema_extra = { 'example' : { \"message\" : \"OK\" , \"method\" : \"GET\" , \"status_code\" : 200 , \"timestamp\" : \"2021-09-04T12:54:49.801614\" , \"url\" : \"http://localhost:8001/status/909e4817-cca3-4dbf-a598-f7f83c5d60c9\" , \"columns\" : [ \"adin\" , \"scjz\" , \"pzxz\" , \"evft\" , \"odev\" , \"pkzc\" , \"lipss\" , \"eayh\" , \"aepqc\" , \"rvuqh\" , \"ozldx\" , \"ulgq\" , \"nabhr\" , \"dvvf\" , \"tmil\" , \"huiow\" , \"euzq\" , \"cdmh\" , \"ompzv\" , \"uguq\" , \"ntvqi\" , \"tctm\" , \"rwgi\" , \"yuonh\" , \"wkvr\" , \"efau\" , \"nrele\" , \"tbhxq\" , \"iiqd\" , \"nhwfy\" , \"tymio\" , \"onzpj\" , \"turho\" , \"hhgnd\" , \"ehmof\" , \"scpoi\" , \"tyqbn\" , \"roul\" , \"exoe\" , \"edwyc\" , \"thid\" , \"snqrp\" , \"tmmh\" , \"oejg\" , \"cjbaw\" , \"hgzmb\" , \"ezlwj\" , \"esxb\" , \"rria\" , \"taeo\" , \"hpnln\" , \"eeomf\" , \"cvxr\" , \"ofwy\" , \"nuon\" , \"vvex\" , \"ikbd\" , \"cmusb\" , \"tdnkb\" , \"iccd\" , \"ocay\" , \"nnxds\" , \"slbgd\" , \"oupap\" , \"mhip\" , \"ebtv\" , \"bipta\" , \"unazv\" , \"sccol\" , \"iovq\" , \"nxzev\" , \"ezaz\" , \"sakm\" , \"skls\" , \"eipvd\" , \"segz\" , \"inrlo\" , \"nzkug\" , \"puau\" , \"oolt\" , \"recs\" , \"tjot\" , \"lwds\" , \"assb\" , \"ntyzf\" , \"dpad\" , \"byep\" , \"owgol\" , \"aiwbb\" , \"ralk\" , \"dffos\" , \"ejfp\" , \"dwyug\" , \"uxucm\" , \"ppch\" , \"teznz\" , \"hpfbv\" , \"eaocf\" , \"iist\" , \"rdlyx\" , \"wseu\" , \"ivgg\" , \"nabi\" , \"dtuiw\" , \"oxzje\" , \"wderj\" , \"sawm\" , \"odpw\" , \"noqs\" , \"cimnf\" , \"eukjx\" , \"aszy\" , \"ghmiq\" , \"ajcsd\" , \"ipwgf\" , \"nhth\" , \"tpvfp\" , \"hnede\" , \"aclb\" , \"tteuq\" , \"nbvi\" , \"isgc\" , \"gejbz\" , \"hufnl\" , \"tggvn\" , \"aahfn\" , \"skkxl\" , \"mmwq\" , \"anqgw\" , \"lrvji\" , \"lker\" , \"gvqt\" , \"rdcd\" , \"obvt\" , \"ussht\" , \"piwb\" , \"ozbn\" , \"fyig\" , \"aczx\" , \"clak\" , \"tgnxi\" , \"iieyi\" , \"vqyqo\" , \"ijgp\" , \"shfth\" , \"tnbeg\" , \"sbuzx\" , \"wwnja\" , \"eguhn\" , \"aneai\" , \"rpsl\" , \"izut\" , \"njixa\" , \"grgh\" , \"bkypw\" , \"lliv\" , \"azeob\" , \"cojl\" , \"kmqnd\" , \"ahpq\" , \"pckw\" , \"phvpa\" , \"rgqv\" , \"opqby\" , \"aiqsp\" , \"clirf\" , \"hlrp\" , \"eztn\" , \"dgek\" , \"azubn\" , \"gnskd\" , \"rajo\" , \"okgv\" , \"ugcp\" , \"pilf\" , \"oxbx\" , \"fwqq\" , \"jsmf\" , \"okgw\" , \"uvjyg\" , \"rfio\" , \"nbtcy\" , \"aryp\" , \"ledgj\" , \"inex\" , \"sdqmm\" , \"tcmtw\" , \"soqh\" , \"tquu\" , \"hrfi\" , \"rsddp\" , \"ewfye\" , \"aumxj\" , \"tssui\" , \"ecgd\" , \"nunfp\" , \"isfka\" , \"ncvx\" , \"gqxi\" , \"typc\" , \"okcdt\" , \"sgcaq\" , \"mbcwd\" , \"angb\" , \"svyl\" , \"hjmk\" , \"tlcgd\" , \"hpka\" , \"efule\" , \"cbjou\" , \"agjjr\" , \"mziur\" , \"emoqh\" , \"rajte\" , \"ahoz\" , \"shaxe\" , \"okeyj\" , \"fgwnq\" , \"twojw\" , \"hcyfc\" , \"ojehd\" , \"soij\" , \"ekhpc\" , \"wbyq\" , \"hdfi\" , \"opyas\" , \"rzba\" , \"ediml\" , \"mvwys\" , \"avtmz\" , \"ilvj\" , \"nefy\" , \"eycng\" , \"dqdk\" , \"oihcj\" , \"nzuw\" , \"srrye\" , \"ccsi\" , \"emhnz\" , \"neosn\" , \"ewzki\" ], \"chains\" : [ [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsonceagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"courseateningtoawalkthecametheonthesewisremainedourcesw\" , - 33.01948561671179 ], [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsdoneagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"courseateningtoawalkthecametheonthesewisremainedourcesw\" , - 33.05044434355386 ], [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsdoneagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"courseateningtoawalkthecametheonthesewisremainedourchew\" , - 33.686079347001396 ], [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsdoneagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"coursexteningtoawalkthecametheonthesewisremainedourchew\" , - 33.83420559252136 ], [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsonceagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"courseateningtoawalkthecametheonthesewisremainedourchew\" , - 33.948646204201395 ] ], \"state\" : None , \"progress\" : None } }","title":"PredictResponse"},{"location":"src/trecover/app/api/schemas/#src.trecover.app.api.schemas.PredictResponse.Config","text":"PredictResponse example for API documentation Source code in src/trecover/app/api/schemas.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 class Config : \"\"\" PredictResponse example for API documentation\"\"\" schema_extra = { 'example' : { \"message\" : \"OK\" , \"method\" : \"GET\" , \"status_code\" : 200 , \"timestamp\" : \"2021-09-04T12:54:49.801614\" , \"url\" : \"http://localhost:8001/status/909e4817-cca3-4dbf-a598-f7f83c5d60c9\" , \"columns\" : [ \"adin\" , \"scjz\" , \"pzxz\" , \"evft\" , \"odev\" , \"pkzc\" , \"lipss\" , \"eayh\" , \"aepqc\" , \"rvuqh\" , \"ozldx\" , \"ulgq\" , \"nabhr\" , \"dvvf\" , \"tmil\" , \"huiow\" , \"euzq\" , \"cdmh\" , \"ompzv\" , \"uguq\" , \"ntvqi\" , \"tctm\" , \"rwgi\" , \"yuonh\" , \"wkvr\" , \"efau\" , \"nrele\" , \"tbhxq\" , \"iiqd\" , \"nhwfy\" , \"tymio\" , \"onzpj\" , \"turho\" , \"hhgnd\" , \"ehmof\" , \"scpoi\" , \"tyqbn\" , \"roul\" , \"exoe\" , \"edwyc\" , \"thid\" , \"snqrp\" , \"tmmh\" , \"oejg\" , \"cjbaw\" , \"hgzmb\" , \"ezlwj\" , \"esxb\" , \"rria\" , \"taeo\" , \"hpnln\" , \"eeomf\" , \"cvxr\" , \"ofwy\" , \"nuon\" , \"vvex\" , \"ikbd\" , \"cmusb\" , \"tdnkb\" , \"iccd\" , \"ocay\" , \"nnxds\" , \"slbgd\" , \"oupap\" , \"mhip\" , \"ebtv\" , \"bipta\" , \"unazv\" , \"sccol\" , \"iovq\" , \"nxzev\" , \"ezaz\" , \"sakm\" , \"skls\" , \"eipvd\" , \"segz\" , \"inrlo\" , \"nzkug\" , \"puau\" , \"oolt\" , \"recs\" , \"tjot\" , \"lwds\" , \"assb\" , \"ntyzf\" , \"dpad\" , \"byep\" , \"owgol\" , \"aiwbb\" , \"ralk\" , \"dffos\" , \"ejfp\" , \"dwyug\" , \"uxucm\" , \"ppch\" , \"teznz\" , \"hpfbv\" , \"eaocf\" , \"iist\" , \"rdlyx\" , \"wseu\" , \"ivgg\" , \"nabi\" , \"dtuiw\" , \"oxzje\" , \"wderj\" , \"sawm\" , \"odpw\" , \"noqs\" , \"cimnf\" , \"eukjx\" , \"aszy\" , \"ghmiq\" , \"ajcsd\" , \"ipwgf\" , \"nhth\" , \"tpvfp\" , \"hnede\" , \"aclb\" , \"tteuq\" , \"nbvi\" , \"isgc\" , \"gejbz\" , \"hufnl\" , \"tggvn\" , \"aahfn\" , \"skkxl\" , \"mmwq\" , \"anqgw\" , \"lrvji\" , \"lker\" , \"gvqt\" , \"rdcd\" , \"obvt\" , \"ussht\" , \"piwb\" , \"ozbn\" , \"fyig\" , \"aczx\" , \"clak\" , \"tgnxi\" , \"iieyi\" , \"vqyqo\" , \"ijgp\" , \"shfth\" , \"tnbeg\" , \"sbuzx\" , \"wwnja\" , \"eguhn\" , \"aneai\" , \"rpsl\" , \"izut\" , \"njixa\" , \"grgh\" , \"bkypw\" , \"lliv\" , \"azeob\" , \"cojl\" , \"kmqnd\" , \"ahpq\" , \"pckw\" , \"phvpa\" , \"rgqv\" , \"opqby\" , \"aiqsp\" , \"clirf\" , \"hlrp\" , \"eztn\" , \"dgek\" , \"azubn\" , \"gnskd\" , \"rajo\" , \"okgv\" , \"ugcp\" , \"pilf\" , \"oxbx\" , \"fwqq\" , \"jsmf\" , \"okgw\" , \"uvjyg\" , \"rfio\" , \"nbtcy\" , \"aryp\" , \"ledgj\" , \"inex\" , \"sdqmm\" , \"tcmtw\" , \"soqh\" , \"tquu\" , \"hrfi\" , \"rsddp\" , \"ewfye\" , \"aumxj\" , \"tssui\" , \"ecgd\" , \"nunfp\" , \"isfka\" , \"ncvx\" , \"gqxi\" , \"typc\" , \"okcdt\" , \"sgcaq\" , \"mbcwd\" , \"angb\" , \"svyl\" , \"hjmk\" , \"tlcgd\" , \"hpka\" , \"efule\" , \"cbjou\" , \"agjjr\" , \"mziur\" , \"emoqh\" , \"rajte\" , \"ahoz\" , \"shaxe\" , \"okeyj\" , \"fgwnq\" , \"twojw\" , \"hcyfc\" , \"ojehd\" , \"soij\" , \"ekhpc\" , \"wbyq\" , \"hdfi\" , \"opyas\" , \"rzba\" , \"ediml\" , \"mvwys\" , \"avtmz\" , \"ilvj\" , \"nefy\" , \"eycng\" , \"dqdk\" , \"oihcj\" , \"nzuw\" , \"srrye\" , \"ccsi\" , \"emhnz\" , \"neosn\" , \"ewzki\" ], \"chains\" : [ [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsonceagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"courseateningtoawalkthecametheonthesewisremainedourcesw\" , - 33.01948561671179 ], [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsdoneagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"courseateningtoawalkthecametheonthesewisremainedourcesw\" , - 33.05044434355386 ], [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsdoneagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"courseateningtoawalkthecametheonthesewisremainedourchew\" , - 33.686079347001396 ], [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsdoneagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"coursexteningtoawalkthecametheonthesewisremainedourchew\" , - 33.83420559252136 ], [ \"aspeoplearoundthecountrywentintothestreetstoabeertheconvictionsametalineaspeonportsandboarded\" \"uptheirwindowsonceagainthatnightasmallgroupofactivistswearingbloodapproachedagroupoffourtales\" \"courseateningtoawalkthecametheonthesewisremainedourchew\" , - 33.948646204201395 ] ], \"state\" : None , \"progress\" : None } }","title":"Config"},{"location":"src/trecover/app/api/backend/tasks/","text":"get_model_config get_model_config ( self ) Celery task implementation that returns values of the model configuration. Parameters: Name Type Description Default self ModelConfigTask Celery task base class. required Returns: Type Description self . config The values of the model configuration. Source code in src/trecover/app/api/backend/tasks.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 @celery_app . task ( bind = True , base = ModelConfigTask ) def get_model_config ( self : ModelConfigTask ) -> Dict : \"\"\" Celery task implementation that returns values of the model configuration. Parameters ---------- self : ModelConfigTask Celery task base class. Returns ------- self.config : Dict The values of the model configuration. \"\"\" return self . config predict predict ( self , columns , beam_width , delimiter ) Celery task implementation that performs keyless reading. Parameters: Name Type Description Default self ArtifactsTask Celery task base class. required columns List [ str ] Columns to keyless read. required beam_width int Width for beam search algorithm. Maximum value is alphabet size. required delimiter str Delimiter for columns visualization. required Returns: Type Description columns , chains The columns and read chains. Raises: Type Description AssertionError: If the number of columns is grater than self.model.pe_max_len. Source code in src/trecover/app/api/backend/tasks.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 @celery_app . task ( bind = True , base = PredictTask ) def predict ( self : PredictTask , columns : List [ str ], beam_width : int , delimiter : str ) -> Tuple [ List [ str ], List [ Tuple [ str , float ]]]: \"\"\" Celery task implementation that performs keyless reading. Parameters ---------- self : ArtifactsTask Celery task base class. columns : List[str] Columns to keyless read. beam_width : int Width for beam search algorithm. Maximum value is alphabet size. delimiter : str Delimiter for columns visualization. Returns ------- (columns, chains) : Tuple[List[str], List[Tuple[str, float]]] The columns and read chains. Raises ------ AssertionError: If the number of columns is grater than self.model.pe_max_len. \"\"\" from trecover.utils.beam_search import beam_search , celery_task_loop from trecover.utils.transform import columns_to_tensor , tensor_to_target from trecover.utils.visualization import visualize_target assert len ( columns ) <= self . model . pe_max_len , f 'Number of columns must be less than { self . model . pe_max_len } .' src = columns_to_tensor ( columns , self . device ) chains = beam_search ( src , self . model , beam_width , self . device , beam_loop = celery_task_loop ( self )) chains = [( visualize_target ( tensor_to_target ( chain ), delimiter = delimiter ), prob ) for ( chain , prob ) in chains ] return columns , chains","title":"Celery Tasks"},{"location":"src/trecover/app/api/backend/tasks/#src.trecover.app.api.backend.tasks.get_model_config","text":"get_model_config ( self ) Celery task implementation that returns values of the model configuration. Parameters: Name Type Description Default self ModelConfigTask Celery task base class. required Returns: Type Description self . config The values of the model configuration. Source code in src/trecover/app/api/backend/tasks.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 @celery_app . task ( bind = True , base = ModelConfigTask ) def get_model_config ( self : ModelConfigTask ) -> Dict : \"\"\" Celery task implementation that returns values of the model configuration. Parameters ---------- self : ModelConfigTask Celery task base class. Returns ------- self.config : Dict The values of the model configuration. \"\"\" return self . config","title":"get_model_config()"},{"location":"src/trecover/app/api/backend/tasks/#src.trecover.app.api.backend.tasks.predict","text":"predict ( self , columns , beam_width , delimiter ) Celery task implementation that performs keyless reading. Parameters: Name Type Description Default self ArtifactsTask Celery task base class. required columns List [ str ] Columns to keyless read. required beam_width int Width for beam search algorithm. Maximum value is alphabet size. required delimiter str Delimiter for columns visualization. required Returns: Type Description columns , chains The columns and read chains. Raises: Type Description AssertionError: If the number of columns is grater than self.model.pe_max_len. Source code in src/trecover/app/api/backend/tasks.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 @celery_app . task ( bind = True , base = PredictTask ) def predict ( self : PredictTask , columns : List [ str ], beam_width : int , delimiter : str ) -> Tuple [ List [ str ], List [ Tuple [ str , float ]]]: \"\"\" Celery task implementation that performs keyless reading. Parameters ---------- self : ArtifactsTask Celery task base class. columns : List[str] Columns to keyless read. beam_width : int Width for beam search algorithm. Maximum value is alphabet size. delimiter : str Delimiter for columns visualization. Returns ------- (columns, chains) : Tuple[List[str], List[Tuple[str, float]]] The columns and read chains. Raises ------ AssertionError: If the number of columns is grater than self.model.pe_max_len. \"\"\" from trecover.utils.beam_search import beam_search , celery_task_loop from trecover.utils.transform import columns_to_tensor , tensor_to_target from trecover.utils.visualization import visualize_target assert len ( columns ) <= self . model . pe_max_len , f 'Number of columns must be less than { self . model . pe_max_len } .' src = columns_to_tensor ( columns , self . device ) chains = beam_search ( src , self . model , beam_width , self . device , beam_loop = celery_task_loop ( self )) chains = [( visualize_target ( tensor_to_target ( chain ), delimiter = delimiter ), prob ) for ( chain , prob ) in chains ] return columns , chains","title":"predict()"},{"location":"src/trecover/app/cli/","text":"recover recover ( data_path = Argument ( Ellipsis , help = \"Path to file or dir for data\" , exists = True , ), model_params = Option ( var . INFERENCE_PARAMS_PATH , help = \"Path to model params json file\" , exists = True , ), weights_path = Option ( var . INFERENCE_WEIGHTS_PATH , help = \"Path to model weights\" , exists = True , ), cuda = Option ( var . CUDA , envvar = \"CUDA\" , help = \"CUDA enabled\" ), gpu_id = Option ( 0 , help = \"GPU id\" ), separator = Option ( \" \" , help = \"Columns separator in the input files\" ), noisy = Option ( False , help = \"Input files are noisy texts\" ), min_noise = Option ( 3 , help = \"Min noise parameter. Minimum value is zero\" ), max_noise = Option ( 5 , help = \"Max noise parameter. Maximum value is alphabet size\" , ), beam_width = Option ( 5 , help = \"Width for beam search algorithm. Maximum value is alphabet size\" , ), n_to_show = Option ( 0 , help = \"Number of columns to visualize. Zero value means for no restriction's\" , ), delimiter = Option ( \"\" , help = \"Delimiter for columns visualization\" ), ) Perform keyless reading locally. Parameters: Name Type Description Default data_path Path Path to file or dir for data. Argument(Ellipsis, help='Path to file or dir for data', exists=True) model_params Path Path to model params json file. Option(var.INFERENCE_PARAMS_PATH, help='Path to model params json file', exists=True) weights_path Path Path to model weights. Option(var.INFERENCE_WEIGHTS_PATH, help='Path to model weights', exists=True) cuda bool CUDA enabled. Option(var.CUDA, envvar='CUDA', help='CUDA enabled') gpu_id int , default GPU id on which perform computations. Option(0, help='GPU id') separator str , default Columns separator in the input files. Option(' ', help='Columns separator in the input files') noisy bool , default Indicates that input files are noisy texts. Option(False, help='Input files are noisy texts') min_noise int , default Min noise size per column. Minimum value is zero. Option(3, help='Min noise parameter. Minimum value is zero') max_noise int , default Max noise size per column. Maximum value is alphabet size. Option(5, help='Max noise parameter. Maximum value is alphabet size') beam_width int , default Width for beam search algorithm. Maximum value is alphabet size. Option(5, help='Width for beam search algorithm. Maximum value is alphabet size') n_to_show int , default Number of columns to visualize. Zero value means for no restriction's. Option(0, help=\"Number of columns to visualize. Zero value means for no restriction's\") delimiter str , default Delimiter for columns visualization. Option('', help='Delimiter for columns visualization') Examples: >>> trecover recover examples / example_1 . txt \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 example_1.txt \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Columns \u2502 \u2502 ajocmbfeafodadbddciafqnahdfeihhkieeaacacafkdchddakhecmmlibfinaehbcbdiicejkeahnfemaeaadbkagacbdmahbibacfddfbbbca\u2026 \u2502 \u2502 enpenkhgglrifflheioentrmjenkjnrmlhphdddeihliekeeeolflonpmctjolgkdeljjmljmmjiisjknjghgeelhkbddlpjjekrkdkilgiocii\u2026 \u2502 \u2502 gsxtoplqkrtknksinktipwvnlnqqrstotoqspoejtsnoiuoflpohvtovqeutunjojlmksonosskpvxporrltnfgoprdemstnshnssgnronjreqj\u2026 \u2502 \u2502 xvzwttqtxvxuoptowuxnxyzrwrrtwtyqwqvutrwrxvtxxwurrtqlwuqzvnwvxossmmpnutosuxlswyuvtttvqulrqzrrwuxtyqouwiuupwsxnrm\u2026 \u2502 \u2502 y y yz zy y w zy uz yys u tzs x u wx wy w tuvpuwu x yyowyz z wxyu xyy v yr t yvw\u2026 \u2502 \u2502 Predicted \u2502 \u2502 enpeoplearoundthecountrywereintothestreetstickedatheconvictionsspewditnessesinpentlandboardeddytheirwindowsbyra\u2026 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Elapsed: 4.716 s Notes A larger \"beam_width\" parameter value can improve keyless reading, but it will also take longer to compute. Source code in src/trecover/app/cli/trecovercli.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @cli . command ( help = 'Perform keyless reading' ) def recover ( data_path : Path = Argument ( ... , help = 'Path to file or dir for data' , exists = True ), model_params : Path = Option ( var . INFERENCE_PARAMS_PATH , help = 'Path to model params json file' , exists = True ), weights_path : Path = Option ( var . INFERENCE_WEIGHTS_PATH , help = 'Path to model weights' , exists = True ), cuda : bool = Option ( var . CUDA , envvar = 'CUDA' , help = 'CUDA enabled' ), gpu_id : int = Option ( 0 , help = 'GPU id' ), separator : str = Option ( ' ' , help = 'Columns separator in the input files' ), noisy : bool = Option ( False , help = 'Input files are noisy texts' ), min_noise : int = Option ( 3 , help = 'Min noise parameter. Minimum value is zero' ), max_noise : int = Option ( 5 , help = 'Max noise parameter. Maximum value is alphabet size' ), beam_width : int = Option ( 5 , help = 'Width for beam search algorithm. Maximum value is alphabet size' ), n_to_show : int = Option ( 0 , help = \"Number of columns to visualize. Zero value means for no restriction's\" ), delimiter : str = Option ( '' , help = 'Delimiter for columns visualization' ) ) -> None : \"\"\" Perform keyless reading locally. Parameters ---------- data_path : Path Path to file or dir for data. model_params : Path Path to model params json file. weights_path : Path Path to model weights. cuda : bool CUDA enabled. gpu_id : int, default=0 GPU id on which perform computations. separator : str, default=' ' Columns separator in the input files. noisy : bool, default=False Indicates that input files are noisy texts. min_noise : int, default=3 Min noise size per column. Minimum value is zero. max_noise : int, default=5 Max noise size per column. Maximum value is alphabet size. beam_width : int, default=5 Width for beam search algorithm. Maximum value is alphabet size. n_to_show : int, default=0 Number of columns to visualize. Zero value means for no restriction's. delimiter : str, default='' Delimiter for columns visualization. Examples -------- >>> trecover recover examples/example_1.txt \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 example_1.txt \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Columns \u2502 \u2502 ajocmbfeafodadbddciafqnahdfeihhkieeaacacafkdchddakhecmmlibfinaehbcbdiicejkeahnfemaeaadbkagacbdmahbibacfddfbbbca\u2026 \u2502 \u2502 enpenkhgglrifflheioentrmjenkjnrmlhphdddeihliekeeeolflonpmctjolgkdeljjmljmmjiisjknjghgeelhkbddlpjjekrkdkilgiocii\u2026 \u2502 \u2502 gsxtoplqkrtknksinktipwvnlnqqrstotoqspoejtsnoiuoflpohvtovqeutunjojlmksonosskpvxporrltnfgoprdemstnshnssgnronjreqj\u2026 \u2502 \u2502 xvzwttqtxvxuoptowuxnxyzrwrrtwtyqwqvutrwrxvtxxwurrtqlwuqzvnwvxossmmpnutosuxlswyuvtttvqulrqzrrwuxtyqouwiuupwsxnrm\u2026 \u2502 \u2502 y y yz zy y w zy uz yys u tzs x u wx wy w tuvpuwu x yyowyz z wxyu xyy v yr t yvw\u2026 \u2502 \u2502 Predicted \u2502 \u2502 enpeoplearoundthecountrywereintothestreetstickedatheconvictionsspewditnessesinpentlandboardeddytheirwindowsbyra\u2026 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Elapsed: 4.716 s Notes ----- A larger \"beam_width\" parameter value can improve keyless reading, but it will also take longer to compute. \"\"\" from time import time import torch from rich.console import Group from rich.panel import Panel from rich.progress import Progress , TextColumn from rich.text import Text from trecover.config import log from trecover.utils.beam_search import beam_search , cli_interactive_loop from trecover.utils.cli import get_files_columns from trecover.utils.transform import files_columns_to_tensors from trecover.utils.model import get_model from trecover.utils.train import load_params from trecover.utils.transform import tensor_to_columns , tensor_to_target from trecover.utils.visualization import visualize_columns , visualize_target data_path = Path ( data_path ) params = load_params ( Path ( model_params )) if not noisy and min_noise >= max_noise : log . project_logger . error ( '[red]Maximum noise range must be grater than minimum noise range' ) return if not any ([ data_path . is_file (), data_path . is_dir ()]): log . project_logger . error ( '[red]Files for inference needed to be specified' ) return if params . pe_max_len < n_to_show : log . project_logger . error ( f '[red]Parameter n_to_show= { n_to_show } must be less than { params . pe_max_len } ' ) return elif n_to_show == 0 : n_to_show = params . pe_max_len device = torch . device ( f 'cuda: { gpu_id } ' if cuda and torch . cuda . is_available () else 'cpu' ) with Progress ( TextColumn ( ' {task.description} ' , style = 'bright_blue' ), transient = True , console = log . project_console ) as progress : progress . add_task ( 'Model loading...' ) model = get_model ( params . token_size , params . pe_max_len , params . num_layers , params . d_model , params . n_heads , params . d_ff , params . dropout , device , weights = Path ( weights_path ), silently = True ) model . eval () files , files_columns = get_files_columns ( data_path , separator , noisy , min_noise , max_noise , n_to_show ) files_src = files_columns_to_tensors ( files_columns , device ) for file_id , ( file , src ) in enumerate ( zip ( files , files_src ), start = 1 ): start_time = time () loop_label = f ' { file_id } / { len ( files_src ) } Processing { file . name } ' chains = beam_search ( src , model , beam_width , device , beam_loop = cli_interactive_loop ( label = loop_label )) chains = [ Text ( visualize_target ( tensor_to_target ( chain ), delimiter = delimiter ), style = 'cyan' , justify = 'center' , overflow = 'ellipsis' , end = ' \\n\\n ' ) for ( chain , _ ) in chains ] columns = visualize_columns ( tensor_to_columns ( src ), delimiter = delimiter , as_rows = True ) columns = ( Text ( row , style = 'bright_blue' , overflow = 'ellipsis' , no_wrap = True , justify = 'left' ) for row in columns ) panel_group = Group ( Text ( 'Columns' , style = 'magenta' , justify = 'center' ), * columns , Text ( 'Predicted' , style = 'magenta' , justify = 'center' ), * chains ) log . project_console . print ( Panel ( panel_group , title = file . name , border_style = 'magenta' ), justify = 'center' ) log . project_console . print ( f ' \\n Elapsed: { time () - start_time : >7.3f } s \\n ' , style = 'bright_blue' ) train train ( ctx , show_help = Option ( False , \"--help\" , \"-h\" , is_flag = True , help = \"Show help message and exit.\" , ), ) Start local training. Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required show_help bool , default Show local train options. Option(False, '--help', '-h', is_flag=True, help='Show help message and exit.') Source code in src/trecover/app/cli/trecovercli.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 @cli . command ( add_help_option = False , help = 'Start local training' , context_settings = { 'allow_extra_args' : True , 'ignore_unknown_options' : True }) def train ( ctx : Context , show_help : bool = Option ( False , '--help' , '-h' , is_flag = True , help = 'Show help message and exit.' )) -> None : \"\"\" Start local training. Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. show_help : bool, default=False Show local train options. \"\"\" from trecover.train.local import train , get_local_parser if show_help : get_local_parser () . print_help () else : train ( cli_args = ctx . args ) cli_state_verification cli_state_verification ( ctx , config_file = Option ( var . BASE_DIR / \"trecover-compose.toml\" , \"--file\" , \"-f\" , file_okay = True , help = 'Path to TRecover configuration file for \"up\" command' , ), ) Perform cli commands verification (state checking) and config file parsing. Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required config_file Path , default Path to TRecover configuration file for \"up\" command. Option(var.BASE_DIR / 'trecover-compose.toml', '--file', '-f', file_okay=True, help='Path to TRecover configuration file for \"up\" command') Source code in src/trecover/app/cli/trecovercli.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 @cli . callback ( invoke_without_command = True , help = '' ) def cli_state_verification ( ctx : Context , config_file : Path = Option ( var . BASE_DIR / 'trecover-compose.toml' , '--file' , '-f' , file_okay = True , help = 'Path to TRecover configuration file for \"up\" command' ), ) -> None : \"\"\" Perform cli commands verification (state checking) and config file parsing. Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. config_file : Path, default=var.BASE_DIR / 'trecover-compose.toml' Path to TRecover configuration file for \"up\" command. \"\"\" if ctx . invoked_subcommand == 'init' : return elif not var . BASE_INIT . exists (): typer . echo ( typer . style ( 'You need to initialize the project environment. \\n ' 'For more information use: trecover init --help' , fg = typer . colors . RED )) ctx . exit ( 1 ) from trecover.config import log if ctx . invoked_subcommand is None : log . project_console . print ( ctx . get_help (), markup = False ) ctx . exit ( 0 ) if ctx . invoked_subcommand in ( 'up' , 'down' , 'status' , 'attach' ): from trecover.utils.docker import is_docker_running from trecover.utils.cli import parse_config if not is_docker_running (): log . project_console . print ( 'Docker engine is not running' , style = 'red' ) ctx . exit ( 1 ) if not config_file . exists (): log . project_console . print ( 'Defined TRecover configuration file does not exist' , style = 'red' ) ctx . exit ( 1 ) ctx . params [ 'conf' ] = parse_config ( config_file ) init init ( base = Option ( Path () . absolute (), \"--base\" , \"-b\" , help = \"Path to the project's base directory\" , ), relocate = Option ( False , \"--relocate\" , \"-r\" , is_flag = True , help = \"Relocate an existing environment\" , ), ) Initialize project's environment. Parameters: Name Type Description Default base Path , default Path to the project's base directory. Option(Path().absolute(), '--base', '-b', help=\"Path to the project's base directory\") relocate bool , default Relocate an existing environment Option(False, '--relocate', '-r', is_flag=True, help='Relocate an existing environment') Source code in src/trecover/app/cli/trecovercli.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 @cli . command ( help = \"Initialize project's environment\" ) def init ( base : Path = Option ( Path () . absolute (), '--base' , '-b' , help = \"Path to the project's base directory\" ), relocate : bool = Option ( False , '--relocate' , '-r' , is_flag = True , help = 'Relocate an existing environment' ) ) -> None : \"\"\" Initialize project's environment. Parameters ---------- base : Path, default='./' Path to the project's base directory. relocate : bool, default=False Relocate an existing environment \"\"\" from shutil import move , Error def rebase ( src : Path , dst : Path ) -> None : try : move ( str ( src ), str ( dst )) except ( PermissionError , Error ): typer . echo ( typer . style ( f 'Failed to relocate: { src } ' , fg = typer . colors . YELLOW )) else : typer . echo ( typer . style ( f 'Relocated: { src } ' , fg = typer . colors . BRIGHT_BLUE )) base . mkdir ( parents = True , exist_ok = True ) with var . BASE_INIT . open ( mode = 'w' ) as f : f . write ( base . as_posix ()) typer . echo ( typer . style ( \"Project's environment is initialized.\" , fg = typer . colors . BRIGHT_BLUE )) if relocate : if var . LOGS_DIR . exists (): rebase ( var . LOGS_DIR , base ) if var . INFERENCE_DIR . exists (): rebase ( var . INFERENCE_DIR , base ) if var . DATA_DIR . exists (): rebase ( var . DATA_DIR , base ) if var . EXPERIMENTS_DIR . exists (): rebase ( var . EXPERIMENTS_DIR , base ) up up ( ctx , attach_stream = Option ( False , \"--attach\" , \"-a\" , is_flag = True , help = \"Attach output and error streams\" , ), ) Start services: Dashboard, API, Worker, Broker, Backend. Command uses trecover-compose.toml config file specified by --file and attaches output and error streams if --attach flag is set. Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required attach_stream bool , default Attach output and error streams. Option(False, '--attach', '-a', is_flag=True, help='Attach output and error streams') Config Variables dashboard host : str, default=ENV(STREAMLIT_HOST) or 'localhost' The address where the server will listen for client and browser connections. Use this if you want to bind the server to a specific address. If set, the server will only be accessible from this address, and not from any aliases (like localhost). dashboard port : int, default=ENV(STREAMLIT_PORT) or 8000 The port where the server will listen for browser connections. dashboard loglevel : {'debug', 'info', 'warning', 'error', 'critical'}, default='info' Level of logging. api host : str, default=ENV(FASTAPI_HOST) or 'localhost' Bind socket to this host. api port : int, default=ENV(FASTAPI_PORT) or 8001 Bind socket to this port. api loglevel : {'debug', 'info', 'warning', 'error', 'critical'}, default='info' Level of logging. api concurrency : int, default=ENV(FASTAPI_WORKERS) or 1 The number of worker processes. worker name : str, default='TRecoverWorker' Custom worker hostname. worker pool : str, {'prefork', 'eventlet', 'gevent', 'processes', 'solo'}, default='solo' Worker processes/threads pool type. worker loglevel : {'debug', 'info', 'warning', 'error', 'critical'}, default='info' Level of logging. worker concurrency : int, default=ENV(CELERY_WORKERS) or 1 The number of worker processes. worker broker_url : str, default=ENV(CELERY_BROKER) or 'pyamqp://guest@localhost' Broker url. worker backend_url : str, default=ENV(CELERY_BACKEND) or 'redis://localhost' Backend url. broker port : int, default=ENV(BROKER_PORT) or 5672 Bind broker socket to this port. broker ui_port : int, default=ENV(BROKER_UI_PORT) or 15672 Bind UI socket to this port. broker auto_remove : bool, default=False Remove broker docker container after service exit. backend port : int, default=ENV(BACKEND_PORT) or 6379 Bind backend socket to this port. backend auto_remove : bool, default=False Remove backend docker container after service exit. Examples: trecover-compose.toml [dashboard] host = \"localhost\" port = 8000 loglevel = 'info' [api] host = \"localhost\" port = 8001 loglevel = \"info\" concurrency = 1 [worker] name = \"TRecoverWorker\" pool = \"solo\" loglevel = \"info\" concurrency = 1 broker_url = \"pyamqp://guest@localhost:5672\" backend_url = \"redis://localhost:6379\" [broker] port = 5672 ui_port = 15672 auto_remove = false [backend] port = 6379 auto_remove = false Source code in src/trecover/app/cli/trecovercli.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 @cli . command ( help = 'Start services' ) def up ( ctx : Context , attach_stream : bool = Option ( False , '--attach' , '-a' , is_flag = True , help = 'Attach output and error streams' ) ) -> None : \"\"\" Start services: Dashboard, API, Worker, Broker, Backend. Command uses trecover-compose.toml config file specified by --file and attaches output and error streams if --attach flag is set. Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. attach_stream : bool, default=False Attach output and error streams. Config Variables ---------------- dashboard host : str, default=ENV(STREAMLIT_HOST) or 'localhost' The address where the server will listen for client and browser connections. Use this if you want to bind the server to a specific address. If set, the server will only be accessible from this address, and not from any aliases (like localhost). dashboard port : int, default=ENV(STREAMLIT_PORT) or 8000 The port where the server will listen for browser connections. dashboard loglevel : {'debug', 'info', 'warning', 'error', 'critical'}, default='info' Level of logging. api host : str, default=ENV(FASTAPI_HOST) or 'localhost' Bind socket to this host. api port : int, default=ENV(FASTAPI_PORT) or 8001 Bind socket to this port. api loglevel : {'debug', 'info', 'warning', 'error', 'critical'}, default='info' Level of logging. api concurrency : int, default=ENV(FASTAPI_WORKERS) or 1 The number of worker processes. worker name : str, default='TRecoverWorker' Custom worker hostname. worker pool : str, {'prefork', 'eventlet', 'gevent', 'processes', 'solo'}, default='solo' Worker processes/threads pool type. worker loglevel : {'debug', 'info', 'warning', 'error', 'critical'}, default='info' Level of logging. worker concurrency : int, default=ENV(CELERY_WORKERS) or 1 The number of worker processes. worker broker_url : str, default=ENV(CELERY_BROKER) or 'pyamqp://guest@localhost' Broker url. worker backend_url : str, default=ENV(CELERY_BACKEND) or 'redis://localhost' Backend url. broker port : int, default=ENV(BROKER_PORT) or 5672 Bind broker socket to this port. broker ui_port : int, default=ENV(BROKER_UI_PORT) or 15672 Bind UI socket to this port. broker auto_remove : bool, default=False Remove broker docker container after service exit. backend port : int, default=ENV(BACKEND_PORT) or 6379 Bind backend socket to this port. backend auto_remove : bool, default=False Remove backend docker container after service exit. Examples -------- # trecover-compose.toml [dashboard] host = \"localhost\" port = 8000 loglevel = 'info' [api] host = \"localhost\" port = 8001 loglevel = \"info\" concurrency = 1 [worker] name = \"TRecoverWorker\" pool = \"solo\" loglevel = \"info\" concurrency = 1 broker_url = \"pyamqp://guest@localhost:5672\" backend_url = \"redis://localhost:6379\" [broker] port = 5672 ui_port = 15672 auto_remove = false [backend] port = 6379 auto_remove = false \"\"\" from trecover.config import log from trecover.utils.docker import get_container from trecover.utils.cli import check_service conf = ctx . parent . params [ 'conf' ] if var . DASHBOARD_PID . exists (): check_service ( name = 'dashboard' , pidfile = var . DASHBOARD_PID ) else : dashboard . dashboard_start ( host = conf . dashboard . host , port = conf . dashboard . port , loglevel = conf . dashboard . loglevel , attach = False , no_daemon = False ) if var . API_PID . exists (): check_service ( name = 'API' , pidfile = var . API_PID ) else : api . api_start ( host = conf . api . host , port = conf . api . port , loglevel = conf . api . loglevel , concurrency = conf . api . concurrency , attach = False , no_daemon = False ) if var . WORKER_PID . exists (): check_service ( name = 'worker' , pidfile = var . WORKER_PID ) else : worker . worker_start ( name = conf . worker . name , pool = conf . worker . pool , loglevel = conf . worker . loglevel , concurrency = conf . worker . concurrency , broker_url = conf . worker . broker_url , backend_url = conf . worker . backend_url , attach = False , no_daemon = False ) if ( container := get_container ( var . BROKER_ID )) and container . status == 'running' : log . project_console . print ( ':rocket: The broker status: running' , style = 'bright_blue' ) else : broker . broker_start ( port = conf . broker . port , ui_port = conf . broker . ui_port , auto_remove = conf . broker . auto_remove , attach = False ) if ( container := get_container ( var . BACKEND_ID )) and container . status == 'running' : log . project_console . print ( ':rocket: The backend status: running' , style = 'bright_blue' ) else : backend . backend_start ( port = conf . backend . port , auto_remove = conf . backend . auto_remove , attach = False ) if attach_stream : try : attach ( live = False ) finally : down ( prune = False , v = False ) down down ( prune = Option ( False , \"--prune\" , \"-p\" , is_flag = True , help = \"Prune all docker containers after exit.\" , ), v = Option ( False , \"--volume\" , \"-v\" , is_flag = True , help = \"Remove the volumes associated with the all docker containers and the log files.\" , ), ) Stop services: Dashboard, API, Worker, Broker, Backend. Parameters: Name Type Description Default prune bool , default Prune all docker containers after exit. Option(False, '--prune', '-p', is_flag=True, help='Prune all docker containers after exit.') v bool , default Remove the volumes associated with the all docker containers and the log files. Option(False, '--volume', '-v', is_flag=True, help='Remove the volumes associated with the all docker containers and the log files.') Source code in src/trecover/app/cli/trecovercli.py 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 @cli . command ( help = 'Stop services' ) def down ( prune : bool = Option ( False , '--prune' , '-p' , is_flag = True , help = 'Prune all docker containers after exit.' ), v : bool = Option ( False , '--volume' , '-v' , is_flag = True , help = 'Remove the volumes associated with the all docker containers and the log files.' ) ) -> None : \"\"\" Stop services: Dashboard, API, Worker, Broker, Backend. Parameters ---------- prune : bool, default=False Prune all docker containers after exit. v : bool, default=False Remove the volumes associated with the all docker containers and the log files. \"\"\" from trecover.config import log from trecover.utils.docker import get_container from trecover.utils.cli import stop_service if var . DASHBOARD_PID . exists (): stop_service ( name = 'dashboard' , pidfile = var . DASHBOARD_PID , logfile = log . DASHBOARD_LOG ) if var . API_PID . exists (): stop_service ( name = 'API' , pidfile = var . API_PID , logfile = log . API_LOG ) if var . WORKER_PID . exists (): stop_service ( name = 'worker' , pidfile = var . WORKER_PID , logfile = log . WORKER_LOG ) if ( container := get_container ( var . BROKER_ID )) and container . status == 'running' : container . stop () log . project_console . print ( 'The broker service is stopped' , style = 'bright_blue' ) if prune : broker . broker_prune ( force = False , v = v ) if ( container := get_container ( var . BACKEND_ID )) and container . status == 'running' : container . stop () log . project_console . print ( 'The backend service is stopped' , style = 'bright_blue' ) if prune : backend . backend_prune ( force = False , v = v ) status status () Display services status Source code in src/trecover/app/cli/trecovercli.py 497 498 499 500 501 502 503 504 505 506 507 508 509 @cli . command ( name = 'status' , help = 'Display services status' ) def status () -> None : \"\"\" Display services status \"\"\" from trecover.utils.cli import check_service from trecover.app.cli.broker import broker_status from trecover.app.cli.backend import backend_status check_service ( name = 'dashboard' , pidfile = var . DASHBOARD_PID ) check_service ( name = 'API' , pidfile = var . API_PID ) check_service ( name = 'worker' , pidfile = var . WORKER_PID ) broker_status () backend_status () attach attach ( live = Option ( False , \"--live\" , \"-l\" , is_flag = True , help = \"Stream only fresh log records\" , ) ) Attach local output stream to a running services. Parameters: Name Type Description Default live bool , Default Stream only fresh log records Option(False, '--live', '-l', is_flag=True, help='Stream only fresh log records') Source code in src/trecover/app/cli/trecovercli.py 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 @cli . command ( name = 'attach' , help = 'Attach local output stream to a running services' ) def attach ( live : bool = Option ( False , '--live' , '-l' , is_flag = True , help = 'Stream only fresh log records' ) ) -> None : \"\"\" Attach local output stream to a running services. Parameters ---------- live : bool, Default=False Stream only fresh log records \"\"\" from trecover.config import log from trecover.utils.cli import stream with log . project_console . screen (): for record in stream (( 'dashboard' , log . DASHBOARD_LOG ), ( 'API' , log . API_LOG ), ( 'worker' , log . WORKER_LOG ), live = live ): log . project_console . print ( record ) log . project_console . clear ()","title":"Command Line Interface"},{"location":"src/trecover/app/cli/#src.trecover.app.cli.trecovercli.recover","text":"recover ( data_path = Argument ( Ellipsis , help = \"Path to file or dir for data\" , exists = True , ), model_params = Option ( var . INFERENCE_PARAMS_PATH , help = \"Path to model params json file\" , exists = True , ), weights_path = Option ( var . INFERENCE_WEIGHTS_PATH , help = \"Path to model weights\" , exists = True , ), cuda = Option ( var . CUDA , envvar = \"CUDA\" , help = \"CUDA enabled\" ), gpu_id = Option ( 0 , help = \"GPU id\" ), separator = Option ( \" \" , help = \"Columns separator in the input files\" ), noisy = Option ( False , help = \"Input files are noisy texts\" ), min_noise = Option ( 3 , help = \"Min noise parameter. Minimum value is zero\" ), max_noise = Option ( 5 , help = \"Max noise parameter. Maximum value is alphabet size\" , ), beam_width = Option ( 5 , help = \"Width for beam search algorithm. Maximum value is alphabet size\" , ), n_to_show = Option ( 0 , help = \"Number of columns to visualize. Zero value means for no restriction's\" , ), delimiter = Option ( \"\" , help = \"Delimiter for columns visualization\" ), ) Perform keyless reading locally. Parameters: Name Type Description Default data_path Path Path to file or dir for data. Argument(Ellipsis, help='Path to file or dir for data', exists=True) model_params Path Path to model params json file. Option(var.INFERENCE_PARAMS_PATH, help='Path to model params json file', exists=True) weights_path Path Path to model weights. Option(var.INFERENCE_WEIGHTS_PATH, help='Path to model weights', exists=True) cuda bool CUDA enabled. Option(var.CUDA, envvar='CUDA', help='CUDA enabled') gpu_id int , default GPU id on which perform computations. Option(0, help='GPU id') separator str , default Columns separator in the input files. Option(' ', help='Columns separator in the input files') noisy bool , default Indicates that input files are noisy texts. Option(False, help='Input files are noisy texts') min_noise int , default Min noise size per column. Minimum value is zero. Option(3, help='Min noise parameter. Minimum value is zero') max_noise int , default Max noise size per column. Maximum value is alphabet size. Option(5, help='Max noise parameter. Maximum value is alphabet size') beam_width int , default Width for beam search algorithm. Maximum value is alphabet size. Option(5, help='Width for beam search algorithm. Maximum value is alphabet size') n_to_show int , default Number of columns to visualize. Zero value means for no restriction's. Option(0, help=\"Number of columns to visualize. Zero value means for no restriction's\") delimiter str , default Delimiter for columns visualization. Option('', help='Delimiter for columns visualization') Examples: >>> trecover recover examples / example_1 . txt \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 example_1.txt \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Columns \u2502 \u2502 ajocmbfeafodadbddciafqnahdfeihhkieeaacacafkdchddakhecmmlibfinaehbcbdiicejkeahnfemaeaadbkagacbdmahbibacfddfbbbca\u2026 \u2502 \u2502 enpenkhgglrifflheioentrmjenkjnrmlhphdddeihliekeeeolflonpmctjolgkdeljjmljmmjiisjknjghgeelhkbddlpjjekrkdkilgiocii\u2026 \u2502 \u2502 gsxtoplqkrtknksinktipwvnlnqqrstotoqspoejtsnoiuoflpohvtovqeutunjojlmksonosskpvxporrltnfgoprdemstnshnssgnronjreqj\u2026 \u2502 \u2502 xvzwttqtxvxuoptowuxnxyzrwrrtwtyqwqvutrwrxvtxxwurrtqlwuqzvnwvxossmmpnutosuxlswyuvtttvqulrqzrrwuxtyqouwiuupwsxnrm\u2026 \u2502 \u2502 y y yz zy y w zy uz yys u tzs x u wx wy w tuvpuwu x yyowyz z wxyu xyy v yr t yvw\u2026 \u2502 \u2502 Predicted \u2502 \u2502 enpeoplearoundthecountrywereintothestreetstickedatheconvictionsspewditnessesinpentlandboardeddytheirwindowsbyra\u2026 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Elapsed: 4.716 s","title":"recover()"},{"location":"src/trecover/app/cli/#src.trecover.app.cli.trecovercli.recover--notes","text":"A larger \"beam_width\" parameter value can improve keyless reading, but it will also take longer to compute. Source code in src/trecover/app/cli/trecovercli.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @cli . command ( help = 'Perform keyless reading' ) def recover ( data_path : Path = Argument ( ... , help = 'Path to file or dir for data' , exists = True ), model_params : Path = Option ( var . INFERENCE_PARAMS_PATH , help = 'Path to model params json file' , exists = True ), weights_path : Path = Option ( var . INFERENCE_WEIGHTS_PATH , help = 'Path to model weights' , exists = True ), cuda : bool = Option ( var . CUDA , envvar = 'CUDA' , help = 'CUDA enabled' ), gpu_id : int = Option ( 0 , help = 'GPU id' ), separator : str = Option ( ' ' , help = 'Columns separator in the input files' ), noisy : bool = Option ( False , help = 'Input files are noisy texts' ), min_noise : int = Option ( 3 , help = 'Min noise parameter. Minimum value is zero' ), max_noise : int = Option ( 5 , help = 'Max noise parameter. Maximum value is alphabet size' ), beam_width : int = Option ( 5 , help = 'Width for beam search algorithm. Maximum value is alphabet size' ), n_to_show : int = Option ( 0 , help = \"Number of columns to visualize. Zero value means for no restriction's\" ), delimiter : str = Option ( '' , help = 'Delimiter for columns visualization' ) ) -> None : \"\"\" Perform keyless reading locally. Parameters ---------- data_path : Path Path to file or dir for data. model_params : Path Path to model params json file. weights_path : Path Path to model weights. cuda : bool CUDA enabled. gpu_id : int, default=0 GPU id on which perform computations. separator : str, default=' ' Columns separator in the input files. noisy : bool, default=False Indicates that input files are noisy texts. min_noise : int, default=3 Min noise size per column. Minimum value is zero. max_noise : int, default=5 Max noise size per column. Maximum value is alphabet size. beam_width : int, default=5 Width for beam search algorithm. Maximum value is alphabet size. n_to_show : int, default=0 Number of columns to visualize. Zero value means for no restriction's. delimiter : str, default='' Delimiter for columns visualization. Examples -------- >>> trecover recover examples/example_1.txt \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 example_1.txt \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Columns \u2502 \u2502 ajocmbfeafodadbddciafqnahdfeihhkieeaacacafkdchddakhecmmlibfinaehbcbdiicejkeahnfemaeaadbkagacbdmahbibacfddfbbbca\u2026 \u2502 \u2502 enpenkhgglrifflheioentrmjenkjnrmlhphdddeihliekeeeolflonpmctjolgkdeljjmljmmjiisjknjghgeelhkbddlpjjekrkdkilgiocii\u2026 \u2502 \u2502 gsxtoplqkrtknksinktipwvnlnqqrstotoqspoejtsnoiuoflpohvtovqeutunjojlmksonosskpvxporrltnfgoprdemstnshnssgnronjreqj\u2026 \u2502 \u2502 xvzwttqtxvxuoptowuxnxyzrwrrtwtyqwqvutrwrxvtxxwurrtqlwuqzvnwvxossmmpnutosuxlswyuvtttvqulrqzrrwuxtyqouwiuupwsxnrm\u2026 \u2502 \u2502 y y yz zy y w zy uz yys u tzs x u wx wy w tuvpuwu x yyowyz z wxyu xyy v yr t yvw\u2026 \u2502 \u2502 Predicted \u2502 \u2502 enpeoplearoundthecountrywereintothestreetstickedatheconvictionsspewditnessesinpentlandboardeddytheirwindowsbyra\u2026 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Elapsed: 4.716 s Notes ----- A larger \"beam_width\" parameter value can improve keyless reading, but it will also take longer to compute. \"\"\" from time import time import torch from rich.console import Group from rich.panel import Panel from rich.progress import Progress , TextColumn from rich.text import Text from trecover.config import log from trecover.utils.beam_search import beam_search , cli_interactive_loop from trecover.utils.cli import get_files_columns from trecover.utils.transform import files_columns_to_tensors from trecover.utils.model import get_model from trecover.utils.train import load_params from trecover.utils.transform import tensor_to_columns , tensor_to_target from trecover.utils.visualization import visualize_columns , visualize_target data_path = Path ( data_path ) params = load_params ( Path ( model_params )) if not noisy and min_noise >= max_noise : log . project_logger . error ( '[red]Maximum noise range must be grater than minimum noise range' ) return if not any ([ data_path . is_file (), data_path . is_dir ()]): log . project_logger . error ( '[red]Files for inference needed to be specified' ) return if params . pe_max_len < n_to_show : log . project_logger . error ( f '[red]Parameter n_to_show= { n_to_show } must be less than { params . pe_max_len } ' ) return elif n_to_show == 0 : n_to_show = params . pe_max_len device = torch . device ( f 'cuda: { gpu_id } ' if cuda and torch . cuda . is_available () else 'cpu' ) with Progress ( TextColumn ( ' {task.description} ' , style = 'bright_blue' ), transient = True , console = log . project_console ) as progress : progress . add_task ( 'Model loading...' ) model = get_model ( params . token_size , params . pe_max_len , params . num_layers , params . d_model , params . n_heads , params . d_ff , params . dropout , device , weights = Path ( weights_path ), silently = True ) model . eval () files , files_columns = get_files_columns ( data_path , separator , noisy , min_noise , max_noise , n_to_show ) files_src = files_columns_to_tensors ( files_columns , device ) for file_id , ( file , src ) in enumerate ( zip ( files , files_src ), start = 1 ): start_time = time () loop_label = f ' { file_id } / { len ( files_src ) } Processing { file . name } ' chains = beam_search ( src , model , beam_width , device , beam_loop = cli_interactive_loop ( label = loop_label )) chains = [ Text ( visualize_target ( tensor_to_target ( chain ), delimiter = delimiter ), style = 'cyan' , justify = 'center' , overflow = 'ellipsis' , end = ' \\n\\n ' ) for ( chain , _ ) in chains ] columns = visualize_columns ( tensor_to_columns ( src ), delimiter = delimiter , as_rows = True ) columns = ( Text ( row , style = 'bright_blue' , overflow = 'ellipsis' , no_wrap = True , justify = 'left' ) for row in columns ) panel_group = Group ( Text ( 'Columns' , style = 'magenta' , justify = 'center' ), * columns , Text ( 'Predicted' , style = 'magenta' , justify = 'center' ), * chains ) log . project_console . print ( Panel ( panel_group , title = file . name , border_style = 'magenta' ), justify = 'center' ) log . project_console . print ( f ' \\n Elapsed: { time () - start_time : >7.3f } s \\n ' , style = 'bright_blue' )","title":"Notes"},{"location":"src/trecover/app/cli/#src.trecover.app.cli.trecovercli.train","text":"train ( ctx , show_help = Option ( False , \"--help\" , \"-h\" , is_flag = True , help = \"Show help message and exit.\" , ), ) Start local training. Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required show_help bool , default Show local train options. Option(False, '--help', '-h', is_flag=True, help='Show help message and exit.') Source code in src/trecover/app/cli/trecovercli.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 @cli . command ( add_help_option = False , help = 'Start local training' , context_settings = { 'allow_extra_args' : True , 'ignore_unknown_options' : True }) def train ( ctx : Context , show_help : bool = Option ( False , '--help' , '-h' , is_flag = True , help = 'Show help message and exit.' )) -> None : \"\"\" Start local training. Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. show_help : bool, default=False Show local train options. \"\"\" from trecover.train.local import train , get_local_parser if show_help : get_local_parser () . print_help () else : train ( cli_args = ctx . args )","title":"train()"},{"location":"src/trecover/app/cli/#src.trecover.app.cli.trecovercli.cli_state_verification","text":"cli_state_verification ( ctx , config_file = Option ( var . BASE_DIR / \"trecover-compose.toml\" , \"--file\" , \"-f\" , file_okay = True , help = 'Path to TRecover configuration file for \"up\" command' , ), ) Perform cli commands verification (state checking) and config file parsing. Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required config_file Path , default Path to TRecover configuration file for \"up\" command. Option(var.BASE_DIR / 'trecover-compose.toml', '--file', '-f', file_okay=True, help='Path to TRecover configuration file for \"up\" command') Source code in src/trecover/app/cli/trecovercli.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 @cli . callback ( invoke_without_command = True , help = '' ) def cli_state_verification ( ctx : Context , config_file : Path = Option ( var . BASE_DIR / 'trecover-compose.toml' , '--file' , '-f' , file_okay = True , help = 'Path to TRecover configuration file for \"up\" command' ), ) -> None : \"\"\" Perform cli commands verification (state checking) and config file parsing. Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. config_file : Path, default=var.BASE_DIR / 'trecover-compose.toml' Path to TRecover configuration file for \"up\" command. \"\"\" if ctx . invoked_subcommand == 'init' : return elif not var . BASE_INIT . exists (): typer . echo ( typer . style ( 'You need to initialize the project environment. \\n ' 'For more information use: trecover init --help' , fg = typer . colors . RED )) ctx . exit ( 1 ) from trecover.config import log if ctx . invoked_subcommand is None : log . project_console . print ( ctx . get_help (), markup = False ) ctx . exit ( 0 ) if ctx . invoked_subcommand in ( 'up' , 'down' , 'status' , 'attach' ): from trecover.utils.docker import is_docker_running from trecover.utils.cli import parse_config if not is_docker_running (): log . project_console . print ( 'Docker engine is not running' , style = 'red' ) ctx . exit ( 1 ) if not config_file . exists (): log . project_console . print ( 'Defined TRecover configuration file does not exist' , style = 'red' ) ctx . exit ( 1 ) ctx . params [ 'conf' ] = parse_config ( config_file )","title":"cli_state_verification()"},{"location":"src/trecover/app/cli/#src.trecover.app.cli.trecovercli.init","text":"init ( base = Option ( Path () . absolute (), \"--base\" , \"-b\" , help = \"Path to the project's base directory\" , ), relocate = Option ( False , \"--relocate\" , \"-r\" , is_flag = True , help = \"Relocate an existing environment\" , ), ) Initialize project's environment. Parameters: Name Type Description Default base Path , default Path to the project's base directory. Option(Path().absolute(), '--base', '-b', help=\"Path to the project's base directory\") relocate bool , default Relocate an existing environment Option(False, '--relocate', '-r', is_flag=True, help='Relocate an existing environment') Source code in src/trecover/app/cli/trecovercli.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 @cli . command ( help = \"Initialize project's environment\" ) def init ( base : Path = Option ( Path () . absolute (), '--base' , '-b' , help = \"Path to the project's base directory\" ), relocate : bool = Option ( False , '--relocate' , '-r' , is_flag = True , help = 'Relocate an existing environment' ) ) -> None : \"\"\" Initialize project's environment. Parameters ---------- base : Path, default='./' Path to the project's base directory. relocate : bool, default=False Relocate an existing environment \"\"\" from shutil import move , Error def rebase ( src : Path , dst : Path ) -> None : try : move ( str ( src ), str ( dst )) except ( PermissionError , Error ): typer . echo ( typer . style ( f 'Failed to relocate: { src } ' , fg = typer . colors . YELLOW )) else : typer . echo ( typer . style ( f 'Relocated: { src } ' , fg = typer . colors . BRIGHT_BLUE )) base . mkdir ( parents = True , exist_ok = True ) with var . BASE_INIT . open ( mode = 'w' ) as f : f . write ( base . as_posix ()) typer . echo ( typer . style ( \"Project's environment is initialized.\" , fg = typer . colors . BRIGHT_BLUE )) if relocate : if var . LOGS_DIR . exists (): rebase ( var . LOGS_DIR , base ) if var . INFERENCE_DIR . exists (): rebase ( var . INFERENCE_DIR , base ) if var . DATA_DIR . exists (): rebase ( var . DATA_DIR , base ) if var . EXPERIMENTS_DIR . exists (): rebase ( var . EXPERIMENTS_DIR , base )","title":"init()"},{"location":"src/trecover/app/cli/#src.trecover.app.cli.trecovercli.up","text":"up ( ctx , attach_stream = Option ( False , \"--attach\" , \"-a\" , is_flag = True , help = \"Attach output and error streams\" , ), ) Start services: Dashboard, API, Worker, Broker, Backend. Command uses trecover-compose.toml config file specified by --file and attaches output and error streams if --attach flag is set. Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required attach_stream bool , default Attach output and error streams. Option(False, '--attach', '-a', is_flag=True, help='Attach output and error streams')","title":"up()"},{"location":"src/trecover/app/cli/#src.trecover.app.cli.trecovercli.up--config-variables","text":"dashboard host : str, default=ENV(STREAMLIT_HOST) or 'localhost' The address where the server will listen for client and browser connections. Use this if you want to bind the server to a specific address. If set, the server will only be accessible from this address, and not from any aliases (like localhost). dashboard port : int, default=ENV(STREAMLIT_PORT) or 8000 The port where the server will listen for browser connections. dashboard loglevel : {'debug', 'info', 'warning', 'error', 'critical'}, default='info' Level of logging. api host : str, default=ENV(FASTAPI_HOST) or 'localhost' Bind socket to this host. api port : int, default=ENV(FASTAPI_PORT) or 8001 Bind socket to this port. api loglevel : {'debug', 'info', 'warning', 'error', 'critical'}, default='info' Level of logging. api concurrency : int, default=ENV(FASTAPI_WORKERS) or 1 The number of worker processes. worker name : str, default='TRecoverWorker' Custom worker hostname. worker pool : str, {'prefork', 'eventlet', 'gevent', 'processes', 'solo'}, default='solo' Worker processes/threads pool type. worker loglevel : {'debug', 'info', 'warning', 'error', 'critical'}, default='info' Level of logging. worker concurrency : int, default=ENV(CELERY_WORKERS) or 1 The number of worker processes. worker broker_url : str, default=ENV(CELERY_BROKER) or 'pyamqp://guest@localhost' Broker url. worker backend_url : str, default=ENV(CELERY_BACKEND) or 'redis://localhost' Backend url. broker port : int, default=ENV(BROKER_PORT) or 5672 Bind broker socket to this port. broker ui_port : int, default=ENV(BROKER_UI_PORT) or 15672 Bind UI socket to this port. broker auto_remove : bool, default=False Remove broker docker container after service exit. backend port : int, default=ENV(BACKEND_PORT) or 6379 Bind backend socket to this port. backend auto_remove : bool, default=False Remove backend docker container after service exit. Examples:","title":"Config Variables"},{"location":"src/trecover/app/cli/#src.trecover.app.cli.trecovercli.up--trecover-composetoml","text":"[dashboard] host = \"localhost\" port = 8000 loglevel = 'info' [api] host = \"localhost\" port = 8001 loglevel = \"info\" concurrency = 1 [worker] name = \"TRecoverWorker\" pool = \"solo\" loglevel = \"info\" concurrency = 1 broker_url = \"pyamqp://guest@localhost:5672\" backend_url = \"redis://localhost:6379\" [broker] port = 5672 ui_port = 15672 auto_remove = false [backend] port = 6379 auto_remove = false Source code in src/trecover/app/cli/trecovercli.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 @cli . command ( help = 'Start services' ) def up ( ctx : Context , attach_stream : bool = Option ( False , '--attach' , '-a' , is_flag = True , help = 'Attach output and error streams' ) ) -> None : \"\"\" Start services: Dashboard, API, Worker, Broker, Backend. Command uses trecover-compose.toml config file specified by --file and attaches output and error streams if --attach flag is set. Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. attach_stream : bool, default=False Attach output and error streams. Config Variables ---------------- dashboard host : str, default=ENV(STREAMLIT_HOST) or 'localhost' The address where the server will listen for client and browser connections. Use this if you want to bind the server to a specific address. If set, the server will only be accessible from this address, and not from any aliases (like localhost). dashboard port : int, default=ENV(STREAMLIT_PORT) or 8000 The port where the server will listen for browser connections. dashboard loglevel : {'debug', 'info', 'warning', 'error', 'critical'}, default='info' Level of logging. api host : str, default=ENV(FASTAPI_HOST) or 'localhost' Bind socket to this host. api port : int, default=ENV(FASTAPI_PORT) or 8001 Bind socket to this port. api loglevel : {'debug', 'info', 'warning', 'error', 'critical'}, default='info' Level of logging. api concurrency : int, default=ENV(FASTAPI_WORKERS) or 1 The number of worker processes. worker name : str, default='TRecoverWorker' Custom worker hostname. worker pool : str, {'prefork', 'eventlet', 'gevent', 'processes', 'solo'}, default='solo' Worker processes/threads pool type. worker loglevel : {'debug', 'info', 'warning', 'error', 'critical'}, default='info' Level of logging. worker concurrency : int, default=ENV(CELERY_WORKERS) or 1 The number of worker processes. worker broker_url : str, default=ENV(CELERY_BROKER) or 'pyamqp://guest@localhost' Broker url. worker backend_url : str, default=ENV(CELERY_BACKEND) or 'redis://localhost' Backend url. broker port : int, default=ENV(BROKER_PORT) or 5672 Bind broker socket to this port. broker ui_port : int, default=ENV(BROKER_UI_PORT) or 15672 Bind UI socket to this port. broker auto_remove : bool, default=False Remove broker docker container after service exit. backend port : int, default=ENV(BACKEND_PORT) or 6379 Bind backend socket to this port. backend auto_remove : bool, default=False Remove backend docker container after service exit. Examples -------- # trecover-compose.toml [dashboard] host = \"localhost\" port = 8000 loglevel = 'info' [api] host = \"localhost\" port = 8001 loglevel = \"info\" concurrency = 1 [worker] name = \"TRecoverWorker\" pool = \"solo\" loglevel = \"info\" concurrency = 1 broker_url = \"pyamqp://guest@localhost:5672\" backend_url = \"redis://localhost:6379\" [broker] port = 5672 ui_port = 15672 auto_remove = false [backend] port = 6379 auto_remove = false \"\"\" from trecover.config import log from trecover.utils.docker import get_container from trecover.utils.cli import check_service conf = ctx . parent . params [ 'conf' ] if var . DASHBOARD_PID . exists (): check_service ( name = 'dashboard' , pidfile = var . DASHBOARD_PID ) else : dashboard . dashboard_start ( host = conf . dashboard . host , port = conf . dashboard . port , loglevel = conf . dashboard . loglevel , attach = False , no_daemon = False ) if var . API_PID . exists (): check_service ( name = 'API' , pidfile = var . API_PID ) else : api . api_start ( host = conf . api . host , port = conf . api . port , loglevel = conf . api . loglevel , concurrency = conf . api . concurrency , attach = False , no_daemon = False ) if var . WORKER_PID . exists (): check_service ( name = 'worker' , pidfile = var . WORKER_PID ) else : worker . worker_start ( name = conf . worker . name , pool = conf . worker . pool , loglevel = conf . worker . loglevel , concurrency = conf . worker . concurrency , broker_url = conf . worker . broker_url , backend_url = conf . worker . backend_url , attach = False , no_daemon = False ) if ( container := get_container ( var . BROKER_ID )) and container . status == 'running' : log . project_console . print ( ':rocket: The broker status: running' , style = 'bright_blue' ) else : broker . broker_start ( port = conf . broker . port , ui_port = conf . broker . ui_port , auto_remove = conf . broker . auto_remove , attach = False ) if ( container := get_container ( var . BACKEND_ID )) and container . status == 'running' : log . project_console . print ( ':rocket: The backend status: running' , style = 'bright_blue' ) else : backend . backend_start ( port = conf . backend . port , auto_remove = conf . backend . auto_remove , attach = False ) if attach_stream : try : attach ( live = False ) finally : down ( prune = False , v = False )","title":"trecover-compose.toml"},{"location":"src/trecover/app/cli/#src.trecover.app.cli.trecovercli.down","text":"down ( prune = Option ( False , \"--prune\" , \"-p\" , is_flag = True , help = \"Prune all docker containers after exit.\" , ), v = Option ( False , \"--volume\" , \"-v\" , is_flag = True , help = \"Remove the volumes associated with the all docker containers and the log files.\" , ), ) Stop services: Dashboard, API, Worker, Broker, Backend. Parameters: Name Type Description Default prune bool , default Prune all docker containers after exit. Option(False, '--prune', '-p', is_flag=True, help='Prune all docker containers after exit.') v bool , default Remove the volumes associated with the all docker containers and the log files. Option(False, '--volume', '-v', is_flag=True, help='Remove the volumes associated with the all docker containers and the log files.') Source code in src/trecover/app/cli/trecovercli.py 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 @cli . command ( help = 'Stop services' ) def down ( prune : bool = Option ( False , '--prune' , '-p' , is_flag = True , help = 'Prune all docker containers after exit.' ), v : bool = Option ( False , '--volume' , '-v' , is_flag = True , help = 'Remove the volumes associated with the all docker containers and the log files.' ) ) -> None : \"\"\" Stop services: Dashboard, API, Worker, Broker, Backend. Parameters ---------- prune : bool, default=False Prune all docker containers after exit. v : bool, default=False Remove the volumes associated with the all docker containers and the log files. \"\"\" from trecover.config import log from trecover.utils.docker import get_container from trecover.utils.cli import stop_service if var . DASHBOARD_PID . exists (): stop_service ( name = 'dashboard' , pidfile = var . DASHBOARD_PID , logfile = log . DASHBOARD_LOG ) if var . API_PID . exists (): stop_service ( name = 'API' , pidfile = var . API_PID , logfile = log . API_LOG ) if var . WORKER_PID . exists (): stop_service ( name = 'worker' , pidfile = var . WORKER_PID , logfile = log . WORKER_LOG ) if ( container := get_container ( var . BROKER_ID )) and container . status == 'running' : container . stop () log . project_console . print ( 'The broker service is stopped' , style = 'bright_blue' ) if prune : broker . broker_prune ( force = False , v = v ) if ( container := get_container ( var . BACKEND_ID )) and container . status == 'running' : container . stop () log . project_console . print ( 'The backend service is stopped' , style = 'bright_blue' ) if prune : backend . backend_prune ( force = False , v = v )","title":"down()"},{"location":"src/trecover/app/cli/#src.trecover.app.cli.trecovercli.status","text":"status () Display services status Source code in src/trecover/app/cli/trecovercli.py 497 498 499 500 501 502 503 504 505 506 507 508 509 @cli . command ( name = 'status' , help = 'Display services status' ) def status () -> None : \"\"\" Display services status \"\"\" from trecover.utils.cli import check_service from trecover.app.cli.broker import broker_status from trecover.app.cli.backend import backend_status check_service ( name = 'dashboard' , pidfile = var . DASHBOARD_PID ) check_service ( name = 'API' , pidfile = var . API_PID ) check_service ( name = 'worker' , pidfile = var . WORKER_PID ) broker_status () backend_status ()","title":"status()"},{"location":"src/trecover/app/cli/#src.trecover.app.cli.trecovercli.attach","text":"attach ( live = Option ( False , \"--live\" , \"-l\" , is_flag = True , help = \"Stream only fresh log records\" , ) ) Attach local output stream to a running services. Parameters: Name Type Description Default live bool , Default Stream only fresh log records Option(False, '--live', '-l', is_flag=True, help='Stream only fresh log records') Source code in src/trecover/app/cli/trecovercli.py 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 @cli . command ( name = 'attach' , help = 'Attach local output stream to a running services' ) def attach ( live : bool = Option ( False , '--live' , '-l' , is_flag = True , help = 'Stream only fresh log records' ) ) -> None : \"\"\" Attach local output stream to a running services. Parameters ---------- live : bool, Default=False Stream only fresh log records \"\"\" from trecover.config import log from trecover.utils.cli import stream with log . project_console . screen (): for record in stream (( 'dashboard' , log . DASHBOARD_LOG ), ( 'API' , log . API_LOG ), ( 'worker' , log . WORKER_LOG ), live = live ): log . project_console . print ( record ) log . project_console . clear ()","title":"attach()"},{"location":"src/trecover/app/cli/api/","text":"api_state_verification api_state_verification ( ctx ) Perform cli commands verification (state checking). Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required Source code in src/trecover/app/cli/api.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @cli . callback ( invoke_without_command = True ) def api_state_verification ( ctx : Context ) -> None : \"\"\" Perform cli commands verification (state checking). Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. \"\"\" from trecover.config import log if var . API_PID . exists (): if ctx . invoked_subcommand in ( 'start' , None ): log . project_console . print ( ':rocket: The API service is already started' , style = 'bright_blue' ) ctx . exit ( 0 ) elif ctx . invoked_subcommand is None : api_start ( host = var . FASTAPI_HOST , port = var . FASTAPI_PORT , loglevel = var . LogLevel . info , concurrency = var . FASTAPI_WORKERS , attach = False , no_daemon = False ) elif ctx . invoked_subcommand not in ( 'start' , 'params' , 'recover' ): log . project_console . print ( 'The API service is not started' , style = 'yellow' ) ctx . exit ( 1 ) api_config api_config ( url = Option ( var . FASTAPI_URL , help = \"API url\" ), param = Option ( None , help = \"Param name to receive\" ), ) Receive configuration or specific parameter of the model used for inference. Parameters: Name Type Description Default url str , default API url. Option(var.FASTAPI_URL, help='API url') param str , default Param name whose value to receive. Receive all configuration values if None. Option(None, help='Param name to receive') Source code in src/trecover/app/cli/api.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 @cli . command ( name = 'config' , help = 'Receive configuration or specific parameter of the model used for inference' ) def api_config ( url : str = Option ( var . FASTAPI_URL , help = 'API url' ), param : str = Option ( None , help = 'Param name to receive' ) ) -> None : \"\"\" Receive configuration or specific parameter of the model used for inference. Parameters ---------- url : str, default=ENV(FASTAPI_URL) or 'http://localhost:8001' API url. param : str, default=None Param name whose value to receive. Receive all configuration values if None. \"\"\" import json import requests from trecover.config import log if param : response = requests . get ( url = f ' { url } /config/ { param } ' ) else : response = requests . get ( url = f ' { url } /config' ) log . project_console . print ( json . dumps ( response . json (), indent = 4 )) api_recover api_recover ( data_path = Argument ( Ellipsis , help = \"Path to file or dir for data\" ), url = Option ( var . FASTAPI_URL , help = \"API url\" ), separator = Option ( \" \" , help = \"Columns separator in the input files\" ), noisy = Option ( False , help = \"Input files are noisy texts\" ), min_noise = Option ( 3 , help = \"Min noise parameter. Minimum value is alphabet size\" , ), max_noise = Option ( 5 , help = \"Max noise parameter. Maximum value is alphabet size\" , ), beam_width = Option ( 1 , help = \"Width for beam search algorithm. Maximum value is alphabet size\" , ), n_to_show = Option ( 0 , help = \"Number of columns to visualize. Zero value means for no restrictions\" , ), delimiter = Option ( \"\" , help = \"Delimiter for columns visualization\" ), ) Send keyless reading API request. Parameters: Name Type Description Default data_path Path Path to file or dir for data. Argument(Ellipsis, help='Path to file or dir for data') url str API url. Option(var.FASTAPI_URL, help='API url') separator str , default Columns separator in the input files. Option(' ', help='Columns separator in the input files') noisy bool , default Indicates that input files are noisy texts. Option(False, help='Input files are noisy texts') min_noise int , default Min noise size per column. Minimum value is zero. Option(3, help='Min noise parameter. Minimum value is alphabet size') max_noise int , default Max noise size per column. Maximum value is alphabet size. Option(5, help='Max noise parameter. Maximum value is alphabet size') beam_width int , default Width for beam search algorithm. Maximum value is alphabet size. Option(1, help='Width for beam search algorithm. Maximum value is alphabet size') n_to_show int , default Number of columns to visualize. Zero value means for no restriction's. Option(0, help='Number of columns to visualize. Zero value means for no restrictions') delimiter str , default Delimiter for columns visualization. Option('', help='Delimiter for columns visualization') Examples: >>> trecover api recover examples / example_1 . txt \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 example_1.txt \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Columns \u2502 \u2502 ajocmbfeafodadbddciafqnahdfeihhkieeaacacafkdchddakhecmmlibfinaehbcbdiicejkeahnfemaeaadbkagacbdmahbibacfddfbbbca\u2026 \u2502 \u2502 enpenkhgglrifflheioentrmjenkjnrmlhphdddeihliekeeeolflonpmctjolgkdeljjmljmmjiisjknjghgeelhkbddlpjjekrkdkilgiocii\u2026 \u2502 \u2502 gsxtoplqkrtknksinktipwvnlnqqrstotoqspoejtsnoiuoflpohvtovqeutunjojlmksonosskpvxporrltnfgoprdemstnshnssgnronjreqj\u2026 \u2502 \u2502 xvzwttqtxvxuoptowuxnxyzrwrrtwtyqwqvutrwrxvtxxwurrtqlwuqzvnwvxossmmpnutosuxlswyuvtttvqulrqzrrwuxtyqouwiuupwsxnrm\u2026 \u2502 \u2502 y y yz zy y w zy uz yys u tzs x u wx wy w tuvpuwu x yyowyz z wxyu xyy v yr t yvw\u2026 \u2502 \u2502 Predicted \u2502 \u2502 enpeoplearoundthecountrywereintothestreetstickedatheconvictionsspewditnessesinpentlandboardeddytheirwindowsbyra\u2026 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Elapsed: 4.716 s Notes A larger \"beam_width\" parameter value can improve keyless reading, but it will also take longer to compute. Source code in src/trecover/app/cli/api.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 @cli . command ( name = 'recover' , help = 'Send keyless reading API request' ) def api_recover ( data_path : str = Argument ( ... , help = 'Path to file or dir for data' ), url : str = Option ( var . FASTAPI_URL , help = 'API url' ), separator : str = Option ( ' ' , help = 'Columns separator in the input files' ), noisy : bool = Option ( False , help = 'Input files are noisy texts' ), min_noise : int = Option ( 3 , help = 'Min noise parameter. Minimum value is alphabet size' ), max_noise : int = Option ( 5 , help = 'Max noise parameter. Maximum value is alphabet size' ), beam_width : int = Option ( 1 , help = 'Width for beam search algorithm. Maximum value is alphabet size' ), n_to_show : int = Option ( 0 , help = 'Number of columns to visualize. Zero value means for no restrictions' ), delimiter : str = Option ( '' , help = 'Delimiter for columns visualization' ) ) -> None : \"\"\" Send keyless reading API request. Parameters ---------- data_path : Path Path to file or dir for data. url : str API url. separator : str, default=' ' Columns separator in the input files. noisy : bool, default=False Indicates that input files are noisy texts. min_noise : int, default=3 Min noise size per column. Minimum value is zero. max_noise : int, default=5 Max noise size per column. Maximum value is alphabet size. beam_width : int, default=5 Width for beam search algorithm. Maximum value is alphabet size. n_to_show : int, default=0 Number of columns to visualize. Zero value means for no restriction's. delimiter : str, default='' Delimiter for columns visualization. Examples -------- >>> trecover api recover examples/example_1.txt \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 example_1.txt \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Columns \u2502 \u2502 ajocmbfeafodadbddciafqnahdfeihhkieeaacacafkdchddakhecmmlibfinaehbcbdiicejkeahnfemaeaadbkagacbdmahbibacfddfbbbca\u2026 \u2502 \u2502 enpenkhgglrifflheioentrmjenkjnrmlhphdddeihliekeeeolflonpmctjolgkdeljjmljmmjiisjknjghgeelhkbddlpjjekrkdkilgiocii\u2026 \u2502 \u2502 gsxtoplqkrtknksinktipwvnlnqqrstotoqspoejtsnoiuoflpohvtovqeutunjojlmksonosskpvxporrltnfgoprdemstnshnssgnronjreqj\u2026 \u2502 \u2502 xvzwttqtxvxuoptowuxnxyzrwrrtwtyqwqvutrwrxvtxxwurrtqlwuqzvnwvxossmmpnutosuxlswyuvtttvqulrqzrrwuxtyqouwiuupwsxnrm\u2026 \u2502 \u2502 y y yz zy y w zy uz yys u tzs x u wx wy w tuvpuwu x yyowyz z wxyu xyy v yr t yvw\u2026 \u2502 \u2502 Predicted \u2502 \u2502 enpeoplearoundthecountrywereintothestreetstickedatheconvictionsspewditnessesinpentlandboardeddytheirwindowsbyra\u2026 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Elapsed: 4.716 s Notes ----- A larger \"beam_width\" parameter value can improve keyless reading, but it will also take longer to compute. \"\"\" import requests from http import HTTPStatus from time import time , sleep from pathlib import Path from rich.console import Group from rich.panel import Panel from rich.progress import Progress , TextColumn , BarColumn , TimeRemainingColumn , TimeElapsedColumn from rich.text import Text from trecover.config import log from trecover.utils.cli import get_files_columns from trecover.utils.visualization import visualize_columns data_path = Path ( data_path ) . absolute () if not noisy and min_noise >= max_noise : log . project_logger . error ( '[red]Maximum noise range must be grater than minimum noise range' ) return if not any ([ data_path . is_file (), data_path . is_dir ()]): log . project_logger . error ( '[red]Files for inference needed to be specified' ) return files , files_columns = get_files_columns ( data_path , separator , noisy , min_noise , max_noise , n_to_show ) payload = { 'columns' : None , 'beam_width' : beam_width , 'delimiter' : delimiter } for file_id , ( file , file_columns ) in enumerate ( zip ( files , files_columns ), start = 1 ): start_time = time () file_columns = [ '' . join ( set ( c )) for c in file_columns ] payload [ 'columns' ] = file_columns task_info = requests . post ( url = f ' { url } /recover' , json = payload ) task_info = task_info . json () if not task_info [ 'task_id' ]: log . project_logger . error ( f ' { file_id } / { len ( files_columns ) } [red]Failed { file . name } : \\n ' f ' { task_info [ \"message\" ] } ' ) continue task_status = requests . get ( url = f ' { url } /status/ { task_info [ \"task_id\" ] } ' ) task_status = task_status . json () label = f ' { file_id } / { len ( files_columns ) } Processing { file . name } ' with Progress ( TextColumn ( ' {task.description} ' , style = 'bright_blue' ), BarColumn ( complete_style = 'bright_blue' ), TextColumn ( ' {task.percentage:>3.0f} %' , style = 'bright_blue' ), TextColumn ( 'Remaining' , style = 'bright_blue' ), TimeRemainingColumn (), TextColumn ( 'Elapsed' , style = 'bright_blue' ), TimeElapsedColumn (), transient = True , ) as progress : request_progress = progress . add_task ( label , total = len ( file_columns )) while task_status [ 'status_code' ] == HTTPStatus . PROCESSING : task_status = requests . get ( url = f ' { url } /status/ { task_info [ \"task_id\" ] } ' ) task_status = task_status . json () progress . update ( request_progress , completed = task_status [ 'progress' ]) sleep ( 0.5 ) requests . delete ( url = f ' { url } / { task_info [ \"task_id\" ] } ' ) if task_status [ 'status_code' ] != HTTPStatus . OK : log . project_logger . error ( f ' { file_id } / { len ( files_columns ) } [red]Failed { file . name } : \\n ' f ' { task_status [ \"message\" ] } ' ) continue columns = visualize_columns ( file_columns , delimiter = delimiter , as_rows = True ) columns = ( Text ( row , style = 'bright_blue' , overflow = 'ellipsis' , no_wrap = True ) for row in columns ) chains = [ Text ( chain , style = 'cyan' , justify = 'center' , overflow = 'ellipsis' , end = ' \\n\\n ' ) for ( chain , _ ) in task_status [ 'chains' ]] panel_group = Group ( Text ( 'Columns' , style = 'magenta' , justify = 'center' ), * columns , Text ( 'Predicted' , style = 'magenta' , justify = 'center' ), * chains ) log . project_console . print ( Panel ( panel_group , title = file . name , border_style = 'magenta' ), justify = 'center' ) log . project_console . print ( f ' \\n Elapsed: { time () - start_time : >7.3f } s \\n ' , style = 'bright_blue' ) api_start api_start ( host = Option ( var . FASTAPI_HOST , \"--host\" , \"-h\" , help = \"Bind socket to this host.\" , ), port = Option ( var . FASTAPI_PORT , \"--port\" , \"-p\" , help = \"Bind socket to this port.\" , ), loglevel = Option ( var . LogLevel . info , \"--loglevel\" , \"-l\" , help = \"Logging level.\" , ), concurrency = Option ( var . FASTAPI_WORKERS , \"-c\" , help = \"The number of worker processes.\" , ), attach = Option ( False , \"--attach\" , \"-a\" , is_flag = True , help = \"Attach output and error streams\" , ), no_daemon = Option ( False , \"--no-daemon\" , is_flag = True , help = \"Do not run as a daemon process\" , ), ) Start API service. Parameters: Name Type Description Default host str , default Bind socket to this host. Option(var.FASTAPI_HOST, '--host', '-h', help='Bind socket to this host.') port int , default Bind socket to this port. Option(var.FASTAPI_PORT, '--port', '-p', help='Bind socket to this port.') loglevel var . LogLevel Level of logging. 'debug' concurrency int , default The number of worker processes. Option(var.FASTAPI_WORKERS, '-c', help='The number of worker processes.') attach bool , default Attach output and error streams. Option(False, '--attach', '-a', is_flag=True, help='Attach output and error streams') no_daemon bool , default Do not run as a daemon process. Option(False, '--no-daemon', is_flag=True, help='Do not run as a daemon process') Source code in src/trecover/app/cli/api.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 @cli . command ( name = 'start' , help = 'Start service' ) def api_start ( host : str = Option ( var . FASTAPI_HOST , '--host' , '-h' , help = 'Bind socket to this host.' ), port : int = Option ( var . FASTAPI_PORT , '--port' , '-p' , help = 'Bind socket to this port.' ), loglevel : var . LogLevel = Option ( var . LogLevel . info , '--loglevel' , '-l' , help = 'Logging level.' ), concurrency : int = Option ( var . FASTAPI_WORKERS , '-c' , help = 'The number of worker processes.' ), attach : bool = Option ( False , '--attach' , '-a' , is_flag = True , help = 'Attach output and error streams' ), no_daemon : bool = Option ( False , '--no-daemon' , is_flag = True , help = 'Do not run as a daemon process' ) ) -> None : \"\"\" Start API service. Parameters ---------- host : str, default=ENV(FASTAPI_HOST) or 'localhost' Bind socket to this host. port : int, default=ENV(FASTAPI_PORT) or 8001 Bind socket to this port. loglevel : {'debug', 'info', 'warning', 'error', 'critical'}, default='info' Level of logging. concurrency : int, default=ENV(FASTAPI_WORKERS) or 1 The number of worker processes. attach : bool, default=False Attach output and error streams. no_daemon : bool, default=False Do not run as a daemon process. \"\"\" from subprocess import run from trecover.config import log from trecover.utils.cli import start_service argv = [ 'uvicorn' , 'trecover.app.api.trecoverapi:api' , '--host' , host , '--port' , str ( port ), '--workers' , str ( concurrency ), '--log-level' , loglevel ] if no_daemon : run ( argv ) else : start_service ( argv , name = 'API' , logfile = log . API_LOG , pidfile = var . API_PID ) if attach : api_attach ( live = False ) api_stop api_stop () Stop API service. Source code in src/trecover/app/cli/api.py 272 273 274 275 276 277 278 279 @cli . command ( name = 'stop' , help = 'Stop service' ) def api_stop () -> None : \"\"\" Stop API service. \"\"\" from trecover.config import log from trecover.utils.cli import stop_service stop_service ( name = 'API' , pidfile = var . API_PID , logfile = log . API_LOG ) api_status api_status () Display API service status. Source code in src/trecover/app/cli/api.py 282 283 284 285 286 287 288 @cli . command ( name = 'status' , help = 'Display service status' ) def api_status () -> None : \"\"\" Display API service status. \"\"\" from trecover.utils.cli import check_service check_service ( name = 'API' , pidfile = var . API_PID ) api_attach api_attach ( live = Option ( False , \"--live\" , \"-l\" , is_flag = True , help = \"Stream only fresh log records\" , ) ) Attach local output stream to a running API service. Parameters: Name Type Description Default live bool , Default Stream only fresh log records Option(False, '--live', '-l', is_flag=True, help='Stream only fresh log records') Source code in src/trecover/app/cli/api.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 @cli . command ( name = 'attach' , help = 'Attach local output stream to a service' ) def api_attach ( live : bool = Option ( False , '--live' , '-l' , is_flag = True , help = 'Stream only fresh log records' ) ) -> None : \"\"\" Attach local output stream to a running API service. Parameters ---------- live : bool, Default=False Stream only fresh log records \"\"\" from trecover.config import log from trecover.utils.cli import stream with log . project_console . screen (): for record in stream (( 'API' , log . API_LOG ), live = live ): log . project_console . print ( record ) log . project_console . clear ()","title":"API"},{"location":"src/trecover/app/cli/api/#src.trecover.app.cli.api.api_state_verification","text":"api_state_verification ( ctx ) Perform cli commands verification (state checking). Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required Source code in src/trecover/app/cli/api.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @cli . callback ( invoke_without_command = True ) def api_state_verification ( ctx : Context ) -> None : \"\"\" Perform cli commands verification (state checking). Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. \"\"\" from trecover.config import log if var . API_PID . exists (): if ctx . invoked_subcommand in ( 'start' , None ): log . project_console . print ( ':rocket: The API service is already started' , style = 'bright_blue' ) ctx . exit ( 0 ) elif ctx . invoked_subcommand is None : api_start ( host = var . FASTAPI_HOST , port = var . FASTAPI_PORT , loglevel = var . LogLevel . info , concurrency = var . FASTAPI_WORKERS , attach = False , no_daemon = False ) elif ctx . invoked_subcommand not in ( 'start' , 'params' , 'recover' ): log . project_console . print ( 'The API service is not started' , style = 'yellow' ) ctx . exit ( 1 )","title":"api_state_verification()"},{"location":"src/trecover/app/cli/api/#src.trecover.app.cli.api.api_config","text":"api_config ( url = Option ( var . FASTAPI_URL , help = \"API url\" ), param = Option ( None , help = \"Param name to receive\" ), ) Receive configuration or specific parameter of the model used for inference. Parameters: Name Type Description Default url str , default API url. Option(var.FASTAPI_URL, help='API url') param str , default Param name whose value to receive. Receive all configuration values if None. Option(None, help='Param name to receive') Source code in src/trecover/app/cli/api.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 @cli . command ( name = 'config' , help = 'Receive configuration or specific parameter of the model used for inference' ) def api_config ( url : str = Option ( var . FASTAPI_URL , help = 'API url' ), param : str = Option ( None , help = 'Param name to receive' ) ) -> None : \"\"\" Receive configuration or specific parameter of the model used for inference. Parameters ---------- url : str, default=ENV(FASTAPI_URL) or 'http://localhost:8001' API url. param : str, default=None Param name whose value to receive. Receive all configuration values if None. \"\"\" import json import requests from trecover.config import log if param : response = requests . get ( url = f ' { url } /config/ { param } ' ) else : response = requests . get ( url = f ' { url } /config' ) log . project_console . print ( json . dumps ( response . json (), indent = 4 ))","title":"api_config()"},{"location":"src/trecover/app/cli/api/#src.trecover.app.cli.api.api_recover","text":"api_recover ( data_path = Argument ( Ellipsis , help = \"Path to file or dir for data\" ), url = Option ( var . FASTAPI_URL , help = \"API url\" ), separator = Option ( \" \" , help = \"Columns separator in the input files\" ), noisy = Option ( False , help = \"Input files are noisy texts\" ), min_noise = Option ( 3 , help = \"Min noise parameter. Minimum value is alphabet size\" , ), max_noise = Option ( 5 , help = \"Max noise parameter. Maximum value is alphabet size\" , ), beam_width = Option ( 1 , help = \"Width for beam search algorithm. Maximum value is alphabet size\" , ), n_to_show = Option ( 0 , help = \"Number of columns to visualize. Zero value means for no restrictions\" , ), delimiter = Option ( \"\" , help = \"Delimiter for columns visualization\" ), ) Send keyless reading API request. Parameters: Name Type Description Default data_path Path Path to file or dir for data. Argument(Ellipsis, help='Path to file or dir for data') url str API url. Option(var.FASTAPI_URL, help='API url') separator str , default Columns separator in the input files. Option(' ', help='Columns separator in the input files') noisy bool , default Indicates that input files are noisy texts. Option(False, help='Input files are noisy texts') min_noise int , default Min noise size per column. Minimum value is zero. Option(3, help='Min noise parameter. Minimum value is alphabet size') max_noise int , default Max noise size per column. Maximum value is alphabet size. Option(5, help='Max noise parameter. Maximum value is alphabet size') beam_width int , default Width for beam search algorithm. Maximum value is alphabet size. Option(1, help='Width for beam search algorithm. Maximum value is alphabet size') n_to_show int , default Number of columns to visualize. Zero value means for no restriction's. Option(0, help='Number of columns to visualize. Zero value means for no restrictions') delimiter str , default Delimiter for columns visualization. Option('', help='Delimiter for columns visualization') Examples: >>> trecover api recover examples / example_1 . txt \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 example_1.txt \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Columns \u2502 \u2502 ajocmbfeafodadbddciafqnahdfeihhkieeaacacafkdchddakhecmmlibfinaehbcbdiicejkeahnfemaeaadbkagacbdmahbibacfddfbbbca\u2026 \u2502 \u2502 enpenkhgglrifflheioentrmjenkjnrmlhphdddeihliekeeeolflonpmctjolgkdeljjmljmmjiisjknjghgeelhkbddlpjjekrkdkilgiocii\u2026 \u2502 \u2502 gsxtoplqkrtknksinktipwvnlnqqrstotoqspoejtsnoiuoflpohvtovqeutunjojlmksonosskpvxporrltnfgoprdemstnshnssgnronjreqj\u2026 \u2502 \u2502 xvzwttqtxvxuoptowuxnxyzrwrrtwtyqwqvutrwrxvtxxwurrtqlwuqzvnwvxossmmpnutosuxlswyuvtttvqulrqzrrwuxtyqouwiuupwsxnrm\u2026 \u2502 \u2502 y y yz zy y w zy uz yys u tzs x u wx wy w tuvpuwu x yyowyz z wxyu xyy v yr t yvw\u2026 \u2502 \u2502 Predicted \u2502 \u2502 enpeoplearoundthecountrywereintothestreetstickedatheconvictionsspewditnessesinpentlandboardeddytheirwindowsbyra\u2026 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Elapsed: 4.716 s","title":"api_recover()"},{"location":"src/trecover/app/cli/api/#src.trecover.app.cli.api.api_recover--notes","text":"A larger \"beam_width\" parameter value can improve keyless reading, but it will also take longer to compute. Source code in src/trecover/app/cli/api.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 @cli . command ( name = 'recover' , help = 'Send keyless reading API request' ) def api_recover ( data_path : str = Argument ( ... , help = 'Path to file or dir for data' ), url : str = Option ( var . FASTAPI_URL , help = 'API url' ), separator : str = Option ( ' ' , help = 'Columns separator in the input files' ), noisy : bool = Option ( False , help = 'Input files are noisy texts' ), min_noise : int = Option ( 3 , help = 'Min noise parameter. Minimum value is alphabet size' ), max_noise : int = Option ( 5 , help = 'Max noise parameter. Maximum value is alphabet size' ), beam_width : int = Option ( 1 , help = 'Width for beam search algorithm. Maximum value is alphabet size' ), n_to_show : int = Option ( 0 , help = 'Number of columns to visualize. Zero value means for no restrictions' ), delimiter : str = Option ( '' , help = 'Delimiter for columns visualization' ) ) -> None : \"\"\" Send keyless reading API request. Parameters ---------- data_path : Path Path to file or dir for data. url : str API url. separator : str, default=' ' Columns separator in the input files. noisy : bool, default=False Indicates that input files are noisy texts. min_noise : int, default=3 Min noise size per column. Minimum value is zero. max_noise : int, default=5 Max noise size per column. Maximum value is alphabet size. beam_width : int, default=5 Width for beam search algorithm. Maximum value is alphabet size. n_to_show : int, default=0 Number of columns to visualize. Zero value means for no restriction's. delimiter : str, default='' Delimiter for columns visualization. Examples -------- >>> trecover api recover examples/example_1.txt \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 example_1.txt \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Columns \u2502 \u2502 ajocmbfeafodadbddciafqnahdfeihhkieeaacacafkdchddakhecmmlibfinaehbcbdiicejkeahnfemaeaadbkagacbdmahbibacfddfbbbca\u2026 \u2502 \u2502 enpenkhgglrifflheioentrmjenkjnrmlhphdddeihliekeeeolflonpmctjolgkdeljjmljmmjiisjknjghgeelhkbddlpjjekrkdkilgiocii\u2026 \u2502 \u2502 gsxtoplqkrtknksinktipwvnlnqqrstotoqspoejtsnoiuoflpohvtovqeutunjojlmksonosskpvxporrltnfgoprdemstnshnssgnronjreqj\u2026 \u2502 \u2502 xvzwttqtxvxuoptowuxnxyzrwrrtwtyqwqvutrwrxvtxxwurrtqlwuqzvnwvxossmmpnutosuxlswyuvtttvqulrqzrrwuxtyqouwiuupwsxnrm\u2026 \u2502 \u2502 y y yz zy y w zy uz yys u tzs x u wx wy w tuvpuwu x yyowyz z wxyu xyy v yr t yvw\u2026 \u2502 \u2502 Predicted \u2502 \u2502 enpeoplearoundthecountrywereintothestreetstickedatheconvictionsspewditnessesinpentlandboardeddytheirwindowsbyra\u2026 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Elapsed: 4.716 s Notes ----- A larger \"beam_width\" parameter value can improve keyless reading, but it will also take longer to compute. \"\"\" import requests from http import HTTPStatus from time import time , sleep from pathlib import Path from rich.console import Group from rich.panel import Panel from rich.progress import Progress , TextColumn , BarColumn , TimeRemainingColumn , TimeElapsedColumn from rich.text import Text from trecover.config import log from trecover.utils.cli import get_files_columns from trecover.utils.visualization import visualize_columns data_path = Path ( data_path ) . absolute () if not noisy and min_noise >= max_noise : log . project_logger . error ( '[red]Maximum noise range must be grater than minimum noise range' ) return if not any ([ data_path . is_file (), data_path . is_dir ()]): log . project_logger . error ( '[red]Files for inference needed to be specified' ) return files , files_columns = get_files_columns ( data_path , separator , noisy , min_noise , max_noise , n_to_show ) payload = { 'columns' : None , 'beam_width' : beam_width , 'delimiter' : delimiter } for file_id , ( file , file_columns ) in enumerate ( zip ( files , files_columns ), start = 1 ): start_time = time () file_columns = [ '' . join ( set ( c )) for c in file_columns ] payload [ 'columns' ] = file_columns task_info = requests . post ( url = f ' { url } /recover' , json = payload ) task_info = task_info . json () if not task_info [ 'task_id' ]: log . project_logger . error ( f ' { file_id } / { len ( files_columns ) } [red]Failed { file . name } : \\n ' f ' { task_info [ \"message\" ] } ' ) continue task_status = requests . get ( url = f ' { url } /status/ { task_info [ \"task_id\" ] } ' ) task_status = task_status . json () label = f ' { file_id } / { len ( files_columns ) } Processing { file . name } ' with Progress ( TextColumn ( ' {task.description} ' , style = 'bright_blue' ), BarColumn ( complete_style = 'bright_blue' ), TextColumn ( ' {task.percentage:>3.0f} %' , style = 'bright_blue' ), TextColumn ( 'Remaining' , style = 'bright_blue' ), TimeRemainingColumn (), TextColumn ( 'Elapsed' , style = 'bright_blue' ), TimeElapsedColumn (), transient = True , ) as progress : request_progress = progress . add_task ( label , total = len ( file_columns )) while task_status [ 'status_code' ] == HTTPStatus . PROCESSING : task_status = requests . get ( url = f ' { url } /status/ { task_info [ \"task_id\" ] } ' ) task_status = task_status . json () progress . update ( request_progress , completed = task_status [ 'progress' ]) sleep ( 0.5 ) requests . delete ( url = f ' { url } / { task_info [ \"task_id\" ] } ' ) if task_status [ 'status_code' ] != HTTPStatus . OK : log . project_logger . error ( f ' { file_id } / { len ( files_columns ) } [red]Failed { file . name } : \\n ' f ' { task_status [ \"message\" ] } ' ) continue columns = visualize_columns ( file_columns , delimiter = delimiter , as_rows = True ) columns = ( Text ( row , style = 'bright_blue' , overflow = 'ellipsis' , no_wrap = True ) for row in columns ) chains = [ Text ( chain , style = 'cyan' , justify = 'center' , overflow = 'ellipsis' , end = ' \\n\\n ' ) for ( chain , _ ) in task_status [ 'chains' ]] panel_group = Group ( Text ( 'Columns' , style = 'magenta' , justify = 'center' ), * columns , Text ( 'Predicted' , style = 'magenta' , justify = 'center' ), * chains ) log . project_console . print ( Panel ( panel_group , title = file . name , border_style = 'magenta' ), justify = 'center' ) log . project_console . print ( f ' \\n Elapsed: { time () - start_time : >7.3f } s \\n ' , style = 'bright_blue' )","title":"Notes"},{"location":"src/trecover/app/cli/api/#src.trecover.app.cli.api.api_start","text":"api_start ( host = Option ( var . FASTAPI_HOST , \"--host\" , \"-h\" , help = \"Bind socket to this host.\" , ), port = Option ( var . FASTAPI_PORT , \"--port\" , \"-p\" , help = \"Bind socket to this port.\" , ), loglevel = Option ( var . LogLevel . info , \"--loglevel\" , \"-l\" , help = \"Logging level.\" , ), concurrency = Option ( var . FASTAPI_WORKERS , \"-c\" , help = \"The number of worker processes.\" , ), attach = Option ( False , \"--attach\" , \"-a\" , is_flag = True , help = \"Attach output and error streams\" , ), no_daemon = Option ( False , \"--no-daemon\" , is_flag = True , help = \"Do not run as a daemon process\" , ), ) Start API service. Parameters: Name Type Description Default host str , default Bind socket to this host. Option(var.FASTAPI_HOST, '--host', '-h', help='Bind socket to this host.') port int , default Bind socket to this port. Option(var.FASTAPI_PORT, '--port', '-p', help='Bind socket to this port.') loglevel var . LogLevel Level of logging. 'debug' concurrency int , default The number of worker processes. Option(var.FASTAPI_WORKERS, '-c', help='The number of worker processes.') attach bool , default Attach output and error streams. Option(False, '--attach', '-a', is_flag=True, help='Attach output and error streams') no_daemon bool , default Do not run as a daemon process. Option(False, '--no-daemon', is_flag=True, help='Do not run as a daemon process') Source code in src/trecover/app/cli/api.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 @cli . command ( name = 'start' , help = 'Start service' ) def api_start ( host : str = Option ( var . FASTAPI_HOST , '--host' , '-h' , help = 'Bind socket to this host.' ), port : int = Option ( var . FASTAPI_PORT , '--port' , '-p' , help = 'Bind socket to this port.' ), loglevel : var . LogLevel = Option ( var . LogLevel . info , '--loglevel' , '-l' , help = 'Logging level.' ), concurrency : int = Option ( var . FASTAPI_WORKERS , '-c' , help = 'The number of worker processes.' ), attach : bool = Option ( False , '--attach' , '-a' , is_flag = True , help = 'Attach output and error streams' ), no_daemon : bool = Option ( False , '--no-daemon' , is_flag = True , help = 'Do not run as a daemon process' ) ) -> None : \"\"\" Start API service. Parameters ---------- host : str, default=ENV(FASTAPI_HOST) or 'localhost' Bind socket to this host. port : int, default=ENV(FASTAPI_PORT) or 8001 Bind socket to this port. loglevel : {'debug', 'info', 'warning', 'error', 'critical'}, default='info' Level of logging. concurrency : int, default=ENV(FASTAPI_WORKERS) or 1 The number of worker processes. attach : bool, default=False Attach output and error streams. no_daemon : bool, default=False Do not run as a daemon process. \"\"\" from subprocess import run from trecover.config import log from trecover.utils.cli import start_service argv = [ 'uvicorn' , 'trecover.app.api.trecoverapi:api' , '--host' , host , '--port' , str ( port ), '--workers' , str ( concurrency ), '--log-level' , loglevel ] if no_daemon : run ( argv ) else : start_service ( argv , name = 'API' , logfile = log . API_LOG , pidfile = var . API_PID ) if attach : api_attach ( live = False )","title":"api_start()"},{"location":"src/trecover/app/cli/api/#src.trecover.app.cli.api.api_stop","text":"api_stop () Stop API service. Source code in src/trecover/app/cli/api.py 272 273 274 275 276 277 278 279 @cli . command ( name = 'stop' , help = 'Stop service' ) def api_stop () -> None : \"\"\" Stop API service. \"\"\" from trecover.config import log from trecover.utils.cli import stop_service stop_service ( name = 'API' , pidfile = var . API_PID , logfile = log . API_LOG )","title":"api_stop()"},{"location":"src/trecover/app/cli/api/#src.trecover.app.cli.api.api_status","text":"api_status () Display API service status. Source code in src/trecover/app/cli/api.py 282 283 284 285 286 287 288 @cli . command ( name = 'status' , help = 'Display service status' ) def api_status () -> None : \"\"\" Display API service status. \"\"\" from trecover.utils.cli import check_service check_service ( name = 'API' , pidfile = var . API_PID )","title":"api_status()"},{"location":"src/trecover/app/cli/api/#src.trecover.app.cli.api.api_attach","text":"api_attach ( live = Option ( False , \"--live\" , \"-l\" , is_flag = True , help = \"Stream only fresh log records\" , ) ) Attach local output stream to a running API service. Parameters: Name Type Description Default live bool , Default Stream only fresh log records Option(False, '--live', '-l', is_flag=True, help='Stream only fresh log records') Source code in src/trecover/app/cli/api.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 @cli . command ( name = 'attach' , help = 'Attach local output stream to a service' ) def api_attach ( live : bool = Option ( False , '--live' , '-l' , is_flag = True , help = 'Stream only fresh log records' ) ) -> None : \"\"\" Attach local output stream to a running API service. Parameters ---------- live : bool, Default=False Stream only fresh log records \"\"\" from trecover.config import log from trecover.utils.cli import stream with log . project_console . screen (): for record in stream (( 'API' , log . API_LOG ), live = live ): log . project_console . print ( record ) log . project_console . clear ()","title":"api_attach()"},{"location":"src/trecover/app/cli/backend/","text":"backend_state_verification backend_state_verification ( ctx ) Perform cli commands and docker engine verification (state checking). Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required Source code in src/trecover/app/cli/backend.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @cli . callback ( invoke_without_command = True ) def backend_state_verification ( ctx : Context ) -> None : \"\"\" Perform cli commands and docker engine verification (state checking). Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. \"\"\" from trecover.config import log from trecover.utils.docker import is_docker_running , get_container if not is_docker_running (): log . project_console . print ( 'Docker engine is not running' , style = 'red' ) ctx . exit ( 1 ) elif container := get_container ( var . BACKEND_ID ): if container . status == 'running' and ctx . invoked_subcommand in ( 'start' , None ): log . project_console . print ( ':rocket: The backend service is already started' , style = 'bright_blue' ) ctx . exit ( 0 ) elif ctx . invoked_subcommand is None : backend_start ( port = var . BACKEND_PORT , auto_remove = False , attach = False ) elif ctx . invoked_subcommand != 'start' : log . project_console . print ( 'The backend service is not started' , style = 'yellow' ) ctx . exit ( 1 ) backend_start backend_start ( port = Option ( var . BACKEND_PORT , \"--port\" , \"-p\" , help = \"Bind socket to this port.\" , ), auto_remove = Option ( False , \"--rm\" , is_flag = True , help = \"Remove docker container after service exit\" , ), attach = Option ( False , \"--attach\" , \"-a\" , is_flag = True , help = \"Attach local standard input, output, and error streams\" , ), ) Start backend service. Parameters: Name Type Description Default port int , default Bind backend socket to this port. Option(var.BACKEND_PORT, '--port', '-p', help='Bind socket to this port.') auto_remove bool , default Remove backend docker container after service exit. Option(False, '--rm', is_flag=True, help='Remove docker container after service exit') attach bool , default Attach output and error streams. Option(False, '--attach', '-a', is_flag=True, help='Attach local standard input, output, and error streams') Source code in src/trecover/app/cli/backend.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 @cli . command ( name = 'start' , help = 'Start service' ) def backend_start ( port : int = Option ( var . BACKEND_PORT , '--port' , '-p' , help = 'Bind socket to this port.' ), auto_remove : bool = Option ( False , '--rm' , is_flag = True , help = 'Remove docker container after service exit' ), attach : bool = Option ( False , '--attach' , '-a' , is_flag = True , help = 'Attach local standard input, output, and error streams' ) ) -> None : \"\"\" Start backend service. Parameters ---------- port : int, default=ENV(BACKEND_PORT) or 6379 Bind backend socket to this port. auto_remove : bool, default=False Remove backend docker container after service exit. attach : bool, default=False Attach output and error streams. \"\"\" from rich.prompt import Confirm from trecover.config import log from trecover.utils.docker import get_client , get_container , get_image , pull_image if not ( image := get_image ( var . BACKEND_IMAGE )): with log . project_console . screen ( hide_cursor = False ): if not Confirm . ask ( f \"The backend image ' { var . BACKEND_IMAGE } ' is needed to be pulled. \\n Continue?\" , default = True ): return image = pull_image ( var . BACKEND_IMAGE ) if container := get_container ( var . BACKEND_ID ): container . start () log . project_console . print ( f 'The backend service is started' , style = 'bright_blue' ) else : get_client () . containers . run ( image = image . id , name = var . BACKEND_ID , auto_remove = auto_remove , detach = True , stdin_open = True , stdout = True , tty = True , stop_signal = 'SIGTERM' , ports = { 6379 : port }, volumes = [ f ' { var . BACKEND_VOLUME_ID } :/data' ]) log . project_console . print ( f 'The backend service is launched' , style = 'bright_blue' ) if attach : backend_attach () backend_stop backend_stop ( prune = Option ( False , \"--prune\" , \"-p\" , is_flag = True , help = \"Prune backend.\" , ), v = Option ( False , \"--volume\" , \"-v\" , is_flag = True , help = \"Remove the volumes associated with the container\" , ), ) Stop backend service. Source code in src/trecover/app/cli/backend.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 @cli . command ( name = 'stop' , help = 'Stop service' ) def backend_stop ( prune : bool = Option ( False , '--prune' , '-p' , is_flag = True , help = 'Prune backend.' ), v : bool = Option ( False , '--volume' , '-v' , is_flag = True , help = 'Remove the volumes associated with the container' ) ) -> None : \"\"\" Stop backend service. \"\"\" from trecover.config import log from trecover.utils.docker import get_container get_container ( var . BACKEND_ID ) . stop () log . project_console . print ( 'The backend service is stopped' , style = 'bright_blue' ) if prune : backend_prune ( force = False , v = v ) backend_prune backend_prune ( force = Option ( False , \"--force\" , \"-f\" , is_flag = True , help = \"Force the removal of a running container\" , ), v = Option ( False , \"--volume\" , \"-v\" , is_flag = True , help = \"Remove the volumes associated with the container\" , ), ) Prune backend service docker container. Parameters: Name Type Description Default force bool , default Force the removal of a running container. Option(False, '--force', '-f', is_flag=True, help='Force the removal of a running container') v bool , default Remove the volumes associated with the container. Option(False, '--volume', '-v', is_flag=True, help='Remove the volumes associated with the container') Source code in src/trecover/app/cli/backend.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 @cli . command ( name = 'prune' , help = 'Prune docker container' ) def backend_prune ( force : bool = Option ( False , '--force' , '-f' , is_flag = True , help = 'Force the removal of a running container' ), v : bool = Option ( False , '--volume' , '-v' , is_flag = True , help = 'Remove the volumes associated with the container' ) ) -> None : \"\"\" Prune backend service docker container. Parameters ---------- force : bool, default=False Force the removal of a running container. v : bool, default=False Remove the volumes associated with the container. \"\"\" from trecover.config import log from trecover.utils.docker import get_container , get_volume container = get_container ( var . BACKEND_ID ) if container . status == 'running' and not force : log . project_console . print ( 'You need to stop backend service before pruning or use --force flag' , style = 'yellow' ) else : container . remove ( force = force ) if v and ( volume := get_volume ( var . BACKEND_VOLUME_ID )): volume . remove ( force = force ) log . project_console . print ( 'The backend service is pruned' , style = 'bright_blue' ) backend_status backend_status () Display backend service status. Source code in src/trecover/app/cli/backend.py 152 153 154 155 156 157 158 159 160 161 162 @cli . command ( name = 'status' , help = 'Display service status' ) def backend_status () -> None : \"\"\" Display backend service status. \"\"\" from trecover.config import log from trecover.utils.docker import get_container if ( container := get_container ( var . BACKEND_ID )) and container . status == 'running' : log . project_console . print ( ':rocket: The backend status: running' , style = 'bright_blue' ) else : log . project_console . print ( 'The backend service is not started' , style = 'yellow' ) backend_attach backend_attach () Attach local output stream to a running backend service. Source code in src/trecover/app/cli/backend.py 165 166 167 168 169 170 171 172 173 174 175 176 @cli . command ( name = 'attach' , help = 'Attach local output stream to a service' ) def backend_attach () -> None : \"\"\" Attach local output stream to a running backend service. \"\"\" from trecover.config import log from trecover.utils.docker import get_container with log . project_console . screen ( hide_cursor = True ): for line in get_container ( var . BACKEND_ID ) . attach ( stream = True , logs = True ): log . project_console . print ( line . decode () . strip ()) log . project_console . clear ()","title":"Backend"},{"location":"src/trecover/app/cli/backend/#src.trecover.app.cli.backend.backend_state_verification","text":"backend_state_verification ( ctx ) Perform cli commands and docker engine verification (state checking). Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required Source code in src/trecover/app/cli/backend.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @cli . callback ( invoke_without_command = True ) def backend_state_verification ( ctx : Context ) -> None : \"\"\" Perform cli commands and docker engine verification (state checking). Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. \"\"\" from trecover.config import log from trecover.utils.docker import is_docker_running , get_container if not is_docker_running (): log . project_console . print ( 'Docker engine is not running' , style = 'red' ) ctx . exit ( 1 ) elif container := get_container ( var . BACKEND_ID ): if container . status == 'running' and ctx . invoked_subcommand in ( 'start' , None ): log . project_console . print ( ':rocket: The backend service is already started' , style = 'bright_blue' ) ctx . exit ( 0 ) elif ctx . invoked_subcommand is None : backend_start ( port = var . BACKEND_PORT , auto_remove = False , attach = False ) elif ctx . invoked_subcommand != 'start' : log . project_console . print ( 'The backend service is not started' , style = 'yellow' ) ctx . exit ( 1 )","title":"backend_state_verification()"},{"location":"src/trecover/app/cli/backend/#src.trecover.app.cli.backend.backend_start","text":"backend_start ( port = Option ( var . BACKEND_PORT , \"--port\" , \"-p\" , help = \"Bind socket to this port.\" , ), auto_remove = Option ( False , \"--rm\" , is_flag = True , help = \"Remove docker container after service exit\" , ), attach = Option ( False , \"--attach\" , \"-a\" , is_flag = True , help = \"Attach local standard input, output, and error streams\" , ), ) Start backend service. Parameters: Name Type Description Default port int , default Bind backend socket to this port. Option(var.BACKEND_PORT, '--port', '-p', help='Bind socket to this port.') auto_remove bool , default Remove backend docker container after service exit. Option(False, '--rm', is_flag=True, help='Remove docker container after service exit') attach bool , default Attach output and error streams. Option(False, '--attach', '-a', is_flag=True, help='Attach local standard input, output, and error streams') Source code in src/trecover/app/cli/backend.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 @cli . command ( name = 'start' , help = 'Start service' ) def backend_start ( port : int = Option ( var . BACKEND_PORT , '--port' , '-p' , help = 'Bind socket to this port.' ), auto_remove : bool = Option ( False , '--rm' , is_flag = True , help = 'Remove docker container after service exit' ), attach : bool = Option ( False , '--attach' , '-a' , is_flag = True , help = 'Attach local standard input, output, and error streams' ) ) -> None : \"\"\" Start backend service. Parameters ---------- port : int, default=ENV(BACKEND_PORT) or 6379 Bind backend socket to this port. auto_remove : bool, default=False Remove backend docker container after service exit. attach : bool, default=False Attach output and error streams. \"\"\" from rich.prompt import Confirm from trecover.config import log from trecover.utils.docker import get_client , get_container , get_image , pull_image if not ( image := get_image ( var . BACKEND_IMAGE )): with log . project_console . screen ( hide_cursor = False ): if not Confirm . ask ( f \"The backend image ' { var . BACKEND_IMAGE } ' is needed to be pulled. \\n Continue?\" , default = True ): return image = pull_image ( var . BACKEND_IMAGE ) if container := get_container ( var . BACKEND_ID ): container . start () log . project_console . print ( f 'The backend service is started' , style = 'bright_blue' ) else : get_client () . containers . run ( image = image . id , name = var . BACKEND_ID , auto_remove = auto_remove , detach = True , stdin_open = True , stdout = True , tty = True , stop_signal = 'SIGTERM' , ports = { 6379 : port }, volumes = [ f ' { var . BACKEND_VOLUME_ID } :/data' ]) log . project_console . print ( f 'The backend service is launched' , style = 'bright_blue' ) if attach : backend_attach ()","title":"backend_start()"},{"location":"src/trecover/app/cli/backend/#src.trecover.app.cli.backend.backend_stop","text":"backend_stop ( prune = Option ( False , \"--prune\" , \"-p\" , is_flag = True , help = \"Prune backend.\" , ), v = Option ( False , \"--volume\" , \"-v\" , is_flag = True , help = \"Remove the volumes associated with the container\" , ), ) Stop backend service. Source code in src/trecover/app/cli/backend.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 @cli . command ( name = 'stop' , help = 'Stop service' ) def backend_stop ( prune : bool = Option ( False , '--prune' , '-p' , is_flag = True , help = 'Prune backend.' ), v : bool = Option ( False , '--volume' , '-v' , is_flag = True , help = 'Remove the volumes associated with the container' ) ) -> None : \"\"\" Stop backend service. \"\"\" from trecover.config import log from trecover.utils.docker import get_container get_container ( var . BACKEND_ID ) . stop () log . project_console . print ( 'The backend service is stopped' , style = 'bright_blue' ) if prune : backend_prune ( force = False , v = v )","title":"backend_stop()"},{"location":"src/trecover/app/cli/backend/#src.trecover.app.cli.backend.backend_prune","text":"backend_prune ( force = Option ( False , \"--force\" , \"-f\" , is_flag = True , help = \"Force the removal of a running container\" , ), v = Option ( False , \"--volume\" , \"-v\" , is_flag = True , help = \"Remove the volumes associated with the container\" , ), ) Prune backend service docker container. Parameters: Name Type Description Default force bool , default Force the removal of a running container. Option(False, '--force', '-f', is_flag=True, help='Force the removal of a running container') v bool , default Remove the volumes associated with the container. Option(False, '--volume', '-v', is_flag=True, help='Remove the volumes associated with the container') Source code in src/trecover/app/cli/backend.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 @cli . command ( name = 'prune' , help = 'Prune docker container' ) def backend_prune ( force : bool = Option ( False , '--force' , '-f' , is_flag = True , help = 'Force the removal of a running container' ), v : bool = Option ( False , '--volume' , '-v' , is_flag = True , help = 'Remove the volumes associated with the container' ) ) -> None : \"\"\" Prune backend service docker container. Parameters ---------- force : bool, default=False Force the removal of a running container. v : bool, default=False Remove the volumes associated with the container. \"\"\" from trecover.config import log from trecover.utils.docker import get_container , get_volume container = get_container ( var . BACKEND_ID ) if container . status == 'running' and not force : log . project_console . print ( 'You need to stop backend service before pruning or use --force flag' , style = 'yellow' ) else : container . remove ( force = force ) if v and ( volume := get_volume ( var . BACKEND_VOLUME_ID )): volume . remove ( force = force ) log . project_console . print ( 'The backend service is pruned' , style = 'bright_blue' )","title":"backend_prune()"},{"location":"src/trecover/app/cli/backend/#src.trecover.app.cli.backend.backend_status","text":"backend_status () Display backend service status. Source code in src/trecover/app/cli/backend.py 152 153 154 155 156 157 158 159 160 161 162 @cli . command ( name = 'status' , help = 'Display service status' ) def backend_status () -> None : \"\"\" Display backend service status. \"\"\" from trecover.config import log from trecover.utils.docker import get_container if ( container := get_container ( var . BACKEND_ID )) and container . status == 'running' : log . project_console . print ( ':rocket: The backend status: running' , style = 'bright_blue' ) else : log . project_console . print ( 'The backend service is not started' , style = 'yellow' )","title":"backend_status()"},{"location":"src/trecover/app/cli/backend/#src.trecover.app.cli.backend.backend_attach","text":"backend_attach () Attach local output stream to a running backend service. Source code in src/trecover/app/cli/backend.py 165 166 167 168 169 170 171 172 173 174 175 176 @cli . command ( name = 'attach' , help = 'Attach local output stream to a service' ) def backend_attach () -> None : \"\"\" Attach local output stream to a running backend service. \"\"\" from trecover.config import log from trecover.utils.docker import get_container with log . project_console . screen ( hide_cursor = True ): for line in get_container ( var . BACKEND_ID ) . attach ( stream = True , logs = True ): log . project_console . print ( line . decode () . strip ()) log . project_console . clear ()","title":"backend_attach()"},{"location":"src/trecover/app/cli/broker/","text":"broker_state_verification broker_state_verification ( ctx ) Perform cli commands and docker engine verification (state checking). Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required Source code in src/trecover/app/cli/broker.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @cli . callback ( invoke_without_command = True ) def broker_state_verification ( ctx : Context ) -> None : \"\"\" Perform cli commands and docker engine verification (state checking). Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. \"\"\" from trecover.config import log from trecover.utils.docker import is_docker_running , get_container if not is_docker_running (): log . project_console . print ( 'Docker engine is not running' , style = 'red' ) ctx . exit ( 1 ) elif container := get_container ( var . BROKER_ID ): if container . status == 'running' and ctx . invoked_subcommand in ( 'start' , None ): log . project_console . print ( ':rocket: The broker service is already started' , style = 'bright_blue' ) ctx . exit ( 0 ) elif ctx . invoked_subcommand is None : broker_start ( port = var . BROKER_PORT , ui_port = var . BROKER_UI_PORT , auto_remove = False , attach = False ) elif ctx . invoked_subcommand != 'start' : log . project_console . print ( 'The broker service is not started' , style = 'yellow' ) ctx . exit ( 1 ) broker_start broker_start ( port = Option ( var . BROKER_PORT , \"--port\" , \"-p\" , help = \"Bind socket to this port.\" , ), ui_port = Option ( var . BROKER_UI_PORT , \"--port\" , \"-p\" , help = \"Bind UI socket to this port.\" , ), auto_remove = Option ( False , \"--rm\" , is_flag = True , help = \"Remove docker container after service exit\" , ), attach = Option ( False , \"--attach\" , \"-a\" , is_flag = True , help = \"Attach local standard input, output, and error streams\" , ), ) Start broker service. Parameters: Name Type Description Default port int , default Bind broker socket to this port. Option(var.BROKER_PORT, '--port', '-p', help='Bind socket to this port.') ui_port int , default Bind UI socket to this port. Option(var.BROKER_UI_PORT, '--port', '-p', help='Bind UI socket to this port.') auto_remove bool , default Remove broker docker container after service exit. Option(False, '--rm', is_flag=True, help='Remove docker container after service exit') attach bool , default Attach output and error streams. Option(False, '--attach', '-a', is_flag=True, help='Attach local standard input, output, and error streams') Source code in src/trecover/app/cli/broker.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @cli . command ( name = 'start' , help = 'Start service' ) def broker_start ( port : int = Option ( var . BROKER_PORT , '--port' , '-p' , help = 'Bind socket to this port.' ), ui_port : int = Option ( var . BROKER_UI_PORT , '--port' , '-p' , help = 'Bind UI socket to this port.' ), auto_remove : bool = Option ( False , '--rm' , is_flag = True , help = 'Remove docker container after service exit' ), attach : bool = Option ( False , '--attach' , '-a' , is_flag = True , help = 'Attach local standard input, output, and error streams' ) ) -> None : \"\"\" Start broker service. Parameters ---------- port : int, default=ENV(BROKER_PORT) or 5672 Bind broker socket to this port. ui_port : int, default=ENV(BROKER_UI_PORT) or 15672 Bind UI socket to this port. auto_remove : bool, default=False Remove broker docker container after service exit. attach : bool, default=False Attach output and error streams. \"\"\" from rich.prompt import Confirm from trecover.config import log from trecover.utils.docker import get_client , get_container , get_image , pull_image if not ( image := get_image ( var . BROKER_IMAGE )): with log . project_console . screen ( hide_cursor = False ): if not Confirm . ask ( f \"The broker image ' { var . BROKER_IMAGE } ' is needed to be pulled. \\n Continue?\" , default = True ): return image = pull_image ( var . BROKER_IMAGE ) if container := get_container ( var . BROKER_ID ): container . start () log . project_console . print ( f 'The broker service is started' , style = 'bright_blue' ) else : get_client () . containers . run ( image = image . id , name = var . BROKER_ID , auto_remove = auto_remove , detach = True , stdin_open = True , stdout = True , tty = True , stop_signal = 'SIGTERM' , ports = { 5672 : port , 15672 : ui_port }, volumes = [ f ' { var . BROKER_VOLUME_ID } :/data' ]) log . project_console . print ( f 'The broker service is launched' , style = 'bright_blue' ) if attach : broker_attach () broker_stop broker_stop ( prune = Option ( False , \"--prune\" , \"-p\" , is_flag = True , help = \"Prune broker.\" , ), v = Option ( False , \"--volume\" , \"-v\" , is_flag = True , help = \"Remove the volumes associated with the container\" , ), ) Stop broker service. Source code in src/trecover/app/cli/broker.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 @cli . command ( name = 'stop' , help = 'Stop service' ) def broker_stop ( prune : bool = Option ( False , '--prune' , '-p' , is_flag = True , help = 'Prune broker.' ), v : bool = Option ( False , '--volume' , '-v' , is_flag = True , help = 'Remove the volumes associated with the container' ) ) -> None : \"\"\" Stop broker service. \"\"\" from trecover.config import log from trecover.utils.docker import get_container get_container ( var . BROKER_ID ) . stop () log . project_console . print ( 'The broker service is stopped' , style = 'bright_blue' ) if prune : broker_prune ( force = False , v = v ) broker_prune broker_prune ( force = Option ( False , \"--force\" , \"-f\" , is_flag = True , help = \"Force the removal of a running container\" , ), v = Option ( False , \"--volume\" , \"-v\" , is_flag = True , help = \"Remove the volumes associated with the container\" , ), ) Prune broker service docker container. Parameters: Name Type Description Default force bool , default Force the removal of a running container. Option(False, '--force', '-f', is_flag=True, help='Force the removal of a running container') v bool , default Remove the volumes associated with the container. Option(False, '--volume', '-v', is_flag=True, help='Remove the volumes associated with the container') Source code in src/trecover/app/cli/broker.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 @cli . command ( name = 'prune' , help = 'Prune docker container' ) def broker_prune ( force : bool = Option ( False , '--force' , '-f' , is_flag = True , help = 'Force the removal of a running container' ), v : bool = Option ( False , '--volume' , '-v' , is_flag = True , help = 'Remove the volumes associated with the container' ) ) -> None : \"\"\" Prune broker service docker container. Parameters ---------- force : bool, default=False Force the removal of a running container. v : bool, default=False Remove the volumes associated with the container. \"\"\" from trecover.config import log from trecover.utils.docker import get_container , get_volume container = get_container ( var . BROKER_ID ) if container . status == 'running' and not force : log . project_console . print ( 'You need to stop broker service before pruning or use --force flag' , style = 'yellow' ) else : container . remove ( force = force ) if v and ( volume := get_volume ( var . BROKER_VOLUME_ID )): volume . remove ( force = force ) log . project_console . print ( 'The broker service is pruned' , style = 'bright_blue' ) broker_status broker_status () Display broker service status. Source code in src/trecover/app/cli/broker.py 157 158 159 160 161 162 163 164 165 166 167 @cli . command ( name = 'status' , help = 'Display service status' ) def broker_status () -> None : \"\"\" Display broker service status. \"\"\" from trecover.config import log from trecover.utils.docker import get_container if ( container := get_container ( var . BROKER_ID )) and container . status == 'running' : log . project_console . print ( ':rocket: The broker status: running' , style = 'bright_blue' ) else : log . project_console . print ( 'The broker service is not started' , style = 'yellow' ) broker_attach broker_attach () Attach local output stream to a running broker service. Source code in src/trecover/app/cli/broker.py 170 171 172 173 174 175 176 177 178 179 180 181 @cli . command ( name = 'attach' , help = 'Attach local output stream to a service' ) def broker_attach () -> None : \"\"\" Attach local output stream to a running broker service. \"\"\" from trecover.config import log from trecover.utils.docker import get_container with log . project_console . screen ( hide_cursor = True ): for line in get_container ( var . BROKER_ID ) . attach ( stream = True , logs = True ): log . project_console . print ( line . decode () . strip ()) log . project_console . clear ()","title":"Broker"},{"location":"src/trecover/app/cli/broker/#src.trecover.app.cli.broker.broker_state_verification","text":"broker_state_verification ( ctx ) Perform cli commands and docker engine verification (state checking). Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required Source code in src/trecover/app/cli/broker.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @cli . callback ( invoke_without_command = True ) def broker_state_verification ( ctx : Context ) -> None : \"\"\" Perform cli commands and docker engine verification (state checking). Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. \"\"\" from trecover.config import log from trecover.utils.docker import is_docker_running , get_container if not is_docker_running (): log . project_console . print ( 'Docker engine is not running' , style = 'red' ) ctx . exit ( 1 ) elif container := get_container ( var . BROKER_ID ): if container . status == 'running' and ctx . invoked_subcommand in ( 'start' , None ): log . project_console . print ( ':rocket: The broker service is already started' , style = 'bright_blue' ) ctx . exit ( 0 ) elif ctx . invoked_subcommand is None : broker_start ( port = var . BROKER_PORT , ui_port = var . BROKER_UI_PORT , auto_remove = False , attach = False ) elif ctx . invoked_subcommand != 'start' : log . project_console . print ( 'The broker service is not started' , style = 'yellow' ) ctx . exit ( 1 )","title":"broker_state_verification()"},{"location":"src/trecover/app/cli/broker/#src.trecover.app.cli.broker.broker_start","text":"broker_start ( port = Option ( var . BROKER_PORT , \"--port\" , \"-p\" , help = \"Bind socket to this port.\" , ), ui_port = Option ( var . BROKER_UI_PORT , \"--port\" , \"-p\" , help = \"Bind UI socket to this port.\" , ), auto_remove = Option ( False , \"--rm\" , is_flag = True , help = \"Remove docker container after service exit\" , ), attach = Option ( False , \"--attach\" , \"-a\" , is_flag = True , help = \"Attach local standard input, output, and error streams\" , ), ) Start broker service. Parameters: Name Type Description Default port int , default Bind broker socket to this port. Option(var.BROKER_PORT, '--port', '-p', help='Bind socket to this port.') ui_port int , default Bind UI socket to this port. Option(var.BROKER_UI_PORT, '--port', '-p', help='Bind UI socket to this port.') auto_remove bool , default Remove broker docker container after service exit. Option(False, '--rm', is_flag=True, help='Remove docker container after service exit') attach bool , default Attach output and error streams. Option(False, '--attach', '-a', is_flag=True, help='Attach local standard input, output, and error streams') Source code in src/trecover/app/cli/broker.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @cli . command ( name = 'start' , help = 'Start service' ) def broker_start ( port : int = Option ( var . BROKER_PORT , '--port' , '-p' , help = 'Bind socket to this port.' ), ui_port : int = Option ( var . BROKER_UI_PORT , '--port' , '-p' , help = 'Bind UI socket to this port.' ), auto_remove : bool = Option ( False , '--rm' , is_flag = True , help = 'Remove docker container after service exit' ), attach : bool = Option ( False , '--attach' , '-a' , is_flag = True , help = 'Attach local standard input, output, and error streams' ) ) -> None : \"\"\" Start broker service. Parameters ---------- port : int, default=ENV(BROKER_PORT) or 5672 Bind broker socket to this port. ui_port : int, default=ENV(BROKER_UI_PORT) or 15672 Bind UI socket to this port. auto_remove : bool, default=False Remove broker docker container after service exit. attach : bool, default=False Attach output and error streams. \"\"\" from rich.prompt import Confirm from trecover.config import log from trecover.utils.docker import get_client , get_container , get_image , pull_image if not ( image := get_image ( var . BROKER_IMAGE )): with log . project_console . screen ( hide_cursor = False ): if not Confirm . ask ( f \"The broker image ' { var . BROKER_IMAGE } ' is needed to be pulled. \\n Continue?\" , default = True ): return image = pull_image ( var . BROKER_IMAGE ) if container := get_container ( var . BROKER_ID ): container . start () log . project_console . print ( f 'The broker service is started' , style = 'bright_blue' ) else : get_client () . containers . run ( image = image . id , name = var . BROKER_ID , auto_remove = auto_remove , detach = True , stdin_open = True , stdout = True , tty = True , stop_signal = 'SIGTERM' , ports = { 5672 : port , 15672 : ui_port }, volumes = [ f ' { var . BROKER_VOLUME_ID } :/data' ]) log . project_console . print ( f 'The broker service is launched' , style = 'bright_blue' ) if attach : broker_attach ()","title":"broker_start()"},{"location":"src/trecover/app/cli/broker/#src.trecover.app.cli.broker.broker_stop","text":"broker_stop ( prune = Option ( False , \"--prune\" , \"-p\" , is_flag = True , help = \"Prune broker.\" , ), v = Option ( False , \"--volume\" , \"-v\" , is_flag = True , help = \"Remove the volumes associated with the container\" , ), ) Stop broker service. Source code in src/trecover/app/cli/broker.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 @cli . command ( name = 'stop' , help = 'Stop service' ) def broker_stop ( prune : bool = Option ( False , '--prune' , '-p' , is_flag = True , help = 'Prune broker.' ), v : bool = Option ( False , '--volume' , '-v' , is_flag = True , help = 'Remove the volumes associated with the container' ) ) -> None : \"\"\" Stop broker service. \"\"\" from trecover.config import log from trecover.utils.docker import get_container get_container ( var . BROKER_ID ) . stop () log . project_console . print ( 'The broker service is stopped' , style = 'bright_blue' ) if prune : broker_prune ( force = False , v = v )","title":"broker_stop()"},{"location":"src/trecover/app/cli/broker/#src.trecover.app.cli.broker.broker_prune","text":"broker_prune ( force = Option ( False , \"--force\" , \"-f\" , is_flag = True , help = \"Force the removal of a running container\" , ), v = Option ( False , \"--volume\" , \"-v\" , is_flag = True , help = \"Remove the volumes associated with the container\" , ), ) Prune broker service docker container. Parameters: Name Type Description Default force bool , default Force the removal of a running container. Option(False, '--force', '-f', is_flag=True, help='Force the removal of a running container') v bool , default Remove the volumes associated with the container. Option(False, '--volume', '-v', is_flag=True, help='Remove the volumes associated with the container') Source code in src/trecover/app/cli/broker.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 @cli . command ( name = 'prune' , help = 'Prune docker container' ) def broker_prune ( force : bool = Option ( False , '--force' , '-f' , is_flag = True , help = 'Force the removal of a running container' ), v : bool = Option ( False , '--volume' , '-v' , is_flag = True , help = 'Remove the volumes associated with the container' ) ) -> None : \"\"\" Prune broker service docker container. Parameters ---------- force : bool, default=False Force the removal of a running container. v : bool, default=False Remove the volumes associated with the container. \"\"\" from trecover.config import log from trecover.utils.docker import get_container , get_volume container = get_container ( var . BROKER_ID ) if container . status == 'running' and not force : log . project_console . print ( 'You need to stop broker service before pruning or use --force flag' , style = 'yellow' ) else : container . remove ( force = force ) if v and ( volume := get_volume ( var . BROKER_VOLUME_ID )): volume . remove ( force = force ) log . project_console . print ( 'The broker service is pruned' , style = 'bright_blue' )","title":"broker_prune()"},{"location":"src/trecover/app/cli/broker/#src.trecover.app.cli.broker.broker_status","text":"broker_status () Display broker service status. Source code in src/trecover/app/cli/broker.py 157 158 159 160 161 162 163 164 165 166 167 @cli . command ( name = 'status' , help = 'Display service status' ) def broker_status () -> None : \"\"\" Display broker service status. \"\"\" from trecover.config import log from trecover.utils.docker import get_container if ( container := get_container ( var . BROKER_ID )) and container . status == 'running' : log . project_console . print ( ':rocket: The broker status: running' , style = 'bright_blue' ) else : log . project_console . print ( 'The broker service is not started' , style = 'yellow' )","title":"broker_status()"},{"location":"src/trecover/app/cli/broker/#src.trecover.app.cli.broker.broker_attach","text":"broker_attach () Attach local output stream to a running broker service. Source code in src/trecover/app/cli/broker.py 170 171 172 173 174 175 176 177 178 179 180 181 @cli . command ( name = 'attach' , help = 'Attach local output stream to a service' ) def broker_attach () -> None : \"\"\" Attach local output stream to a running broker service. \"\"\" from trecover.config import log from trecover.utils.docker import get_container with log . project_console . screen ( hide_cursor = True ): for line in get_container ( var . BROKER_ID ) . attach ( stream = True , logs = True ): log . project_console . print ( line . decode () . strip ()) log . project_console . clear ()","title":"broker_attach()"},{"location":"src/trecover/app/cli/collab/","text":"monitor monitor ( ctx , show_help = Option ( False , \"--help\" , \"-h\" , is_flag = True , help = \"Show help message and exit.\" , ), ) Start collaborative training monitor. Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required show_help bool , default Show remote train options. Option(False, '--help', '-h', is_flag=True, help='Show help message and exit.') Source code in src/trecover/app/cli/collab.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 @cli . command ( context_settings = { 'allow_extra_args' : True , 'ignore_unknown_options' : True }, add_help_option = False , help = 'Start collaborative training monitor' ) def monitor ( ctx : Context , show_help : bool = Option ( False , '--help' , '-h' , is_flag = True , help = 'Show help message and exit.' ) ) -> None : \"\"\" Start collaborative training monitor. Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. show_help : bool, default=False Show remote train options. \"\"\" from trecover.train.collab import monitor , get_monitor_parser if show_help : get_monitor_parser () . print_help () else : monitor ( cli_args = ctx . args ) visualize visualize ( ctx , show_help = Option ( False , \"--help\" , \"-h\" , is_flag = True , help = \"Show help message and exit.\" , ), ) Visualize collaborative training progress. Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required show_help bool , default Show remote train options. Option(False, '--help', '-h', is_flag=True, help='Show help message and exit.') Source code in src/trecover/app/cli/collab.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 @cli . command ( context_settings = { 'allow_extra_args' : True , 'ignore_unknown_options' : True }, add_help_option = False , help = 'Visualize collaborative training progress' ) def visualize ( ctx : Context , show_help : bool = Option ( False , '--help' , '-h' , is_flag = True , help = 'Show help message and exit.' ) ) -> None : \"\"\" Visualize collaborative training progress. Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. show_help : bool, default=False Show remote train options. \"\"\" from trecover.train.collab import visualize , get_visualization_parser if show_help : get_visualization_parser () . print_help () else : visualize ( cli_args = ctx . args ) tune tune ( ctx , show_help = Option ( False , \"--help\" , \"-h\" , is_flag = True , help = \"Show help message and exit.\" , ), ) Tune batch size for this machine. Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required show_help bool , default Show tune options. Option(False, '--help', '-h', is_flag=True, help='Show help message and exit.') Source code in src/trecover/app/cli/collab.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 @cli . command ( context_settings = { 'allow_extra_args' : True , 'ignore_unknown_options' : True }, add_help_option = False , help = 'Tune batch size for this machine' ) def tune ( ctx : Context , show_help : bool = Option ( False , '--help' , '-h' , is_flag = True , help = 'Show help message and exit.' ) ) -> None : \"\"\" Tune batch size for this machine. Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. show_help : bool, default=False Show tune options. \"\"\" from trecover.train.collab import tune , get_train_parser if show_help : get_train_parser () . print_help () else : tune ( cli_args = ctx . args ) train train ( ctx , show_help = Option ( False , \"--help\" , \"-h\" , is_flag = True , help = \"Show help message and exit.\" , ), ) Start collaborative training. Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required show_help bool , default Show collab train options. Option(False, '--help', '-h', is_flag=True, help='Show help message and exit.') Source code in src/trecover/app/cli/collab.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 @cli . command ( context_settings = { 'allow_extra_args' : True , 'ignore_unknown_options' : True }, add_help_option = False , help = 'Start collaborative training' ) def train ( ctx : Context , show_help : bool = Option ( False , '--help' , '-h' , is_flag = True , help = 'Show help message and exit.' ) ) -> None : \"\"\" Start collaborative training. Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. show_help : bool, default=False Show collab train options. \"\"\" from trecover.train.collab import train , get_train_parser if show_help : get_train_parser () . print_help () else : train ( cli_args = ctx . args ) aux aux ( ctx , show_help = Option ( False , \"--help\" , \"-h\" , is_flag = True , help = \"Show help message and exit.\" , ), ) Start auxiliary peers for gradient averaging (for cpu-only workers). Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required show_help bool , default Show collab train options. Option(False, '--help', '-h', is_flag=True, help='Show help message and exit.') Source code in src/trecover/app/cli/collab.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 @cli . command ( context_settings = { 'allow_extra_args' : True , 'ignore_unknown_options' : True }, add_help_option = False , help = 'Start auxiliary peer (for cpu-only workers)' ) def aux ( ctx : Context , show_help : bool = Option ( False , '--help' , '-h' , is_flag = True , help = 'Show help message and exit.' ) ) -> None : \"\"\" Start auxiliary peers for gradient averaging (for cpu-only workers). Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. show_help : bool, default=False Show collab train options. \"\"\" from trecover.train.collab import auxiliary , get_auxiliary_parser if show_help : get_auxiliary_parser () . print_help () else : auxiliary ( cli_args = ctx . args )","title":"Collab"},{"location":"src/trecover/app/cli/collab/#src.trecover.app.cli.collab.monitor","text":"monitor ( ctx , show_help = Option ( False , \"--help\" , \"-h\" , is_flag = True , help = \"Show help message and exit.\" , ), ) Start collaborative training monitor. Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required show_help bool , default Show remote train options. Option(False, '--help', '-h', is_flag=True, help='Show help message and exit.') Source code in src/trecover/app/cli/collab.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 @cli . command ( context_settings = { 'allow_extra_args' : True , 'ignore_unknown_options' : True }, add_help_option = False , help = 'Start collaborative training monitor' ) def monitor ( ctx : Context , show_help : bool = Option ( False , '--help' , '-h' , is_flag = True , help = 'Show help message and exit.' ) ) -> None : \"\"\" Start collaborative training monitor. Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. show_help : bool, default=False Show remote train options. \"\"\" from trecover.train.collab import monitor , get_monitor_parser if show_help : get_monitor_parser () . print_help () else : monitor ( cli_args = ctx . args )","title":"monitor()"},{"location":"src/trecover/app/cli/collab/#src.trecover.app.cli.collab.visualize","text":"visualize ( ctx , show_help = Option ( False , \"--help\" , \"-h\" , is_flag = True , help = \"Show help message and exit.\" , ), ) Visualize collaborative training progress. Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required show_help bool , default Show remote train options. Option(False, '--help', '-h', is_flag=True, help='Show help message and exit.') Source code in src/trecover/app/cli/collab.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 @cli . command ( context_settings = { 'allow_extra_args' : True , 'ignore_unknown_options' : True }, add_help_option = False , help = 'Visualize collaborative training progress' ) def visualize ( ctx : Context , show_help : bool = Option ( False , '--help' , '-h' , is_flag = True , help = 'Show help message and exit.' ) ) -> None : \"\"\" Visualize collaborative training progress. Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. show_help : bool, default=False Show remote train options. \"\"\" from trecover.train.collab import visualize , get_visualization_parser if show_help : get_visualization_parser () . print_help () else : visualize ( cli_args = ctx . args )","title":"visualize()"},{"location":"src/trecover/app/cli/collab/#src.trecover.app.cli.collab.tune","text":"tune ( ctx , show_help = Option ( False , \"--help\" , \"-h\" , is_flag = True , help = \"Show help message and exit.\" , ), ) Tune batch size for this machine. Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required show_help bool , default Show tune options. Option(False, '--help', '-h', is_flag=True, help='Show help message and exit.') Source code in src/trecover/app/cli/collab.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 @cli . command ( context_settings = { 'allow_extra_args' : True , 'ignore_unknown_options' : True }, add_help_option = False , help = 'Tune batch size for this machine' ) def tune ( ctx : Context , show_help : bool = Option ( False , '--help' , '-h' , is_flag = True , help = 'Show help message and exit.' ) ) -> None : \"\"\" Tune batch size for this machine. Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. show_help : bool, default=False Show tune options. \"\"\" from trecover.train.collab import tune , get_train_parser if show_help : get_train_parser () . print_help () else : tune ( cli_args = ctx . args )","title":"tune()"},{"location":"src/trecover/app/cli/collab/#src.trecover.app.cli.collab.train","text":"train ( ctx , show_help = Option ( False , \"--help\" , \"-h\" , is_flag = True , help = \"Show help message and exit.\" , ), ) Start collaborative training. Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required show_help bool , default Show collab train options. Option(False, '--help', '-h', is_flag=True, help='Show help message and exit.') Source code in src/trecover/app/cli/collab.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 @cli . command ( context_settings = { 'allow_extra_args' : True , 'ignore_unknown_options' : True }, add_help_option = False , help = 'Start collaborative training' ) def train ( ctx : Context , show_help : bool = Option ( False , '--help' , '-h' , is_flag = True , help = 'Show help message and exit.' ) ) -> None : \"\"\" Start collaborative training. Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. show_help : bool, default=False Show collab train options. \"\"\" from trecover.train.collab import train , get_train_parser if show_help : get_train_parser () . print_help () else : train ( cli_args = ctx . args )","title":"train()"},{"location":"src/trecover/app/cli/collab/#src.trecover.app.cli.collab.aux","text":"aux ( ctx , show_help = Option ( False , \"--help\" , \"-h\" , is_flag = True , help = \"Show help message and exit.\" , ), ) Start auxiliary peers for gradient averaging (for cpu-only workers). Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required show_help bool , default Show collab train options. Option(False, '--help', '-h', is_flag=True, help='Show help message and exit.') Source code in src/trecover/app/cli/collab.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 @cli . command ( context_settings = { 'allow_extra_args' : True , 'ignore_unknown_options' : True }, add_help_option = False , help = 'Start auxiliary peer (for cpu-only workers)' ) def aux ( ctx : Context , show_help : bool = Option ( False , '--help' , '-h' , is_flag = True , help = 'Show help message and exit.' ) ) -> None : \"\"\" Start auxiliary peers for gradient averaging (for cpu-only workers). Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. show_help : bool, default=False Show collab train options. \"\"\" from trecover.train.collab import auxiliary , get_auxiliary_parser if show_help : get_auxiliary_parser () . print_help () else : auxiliary ( cli_args = ctx . args )","title":"aux()"},{"location":"src/trecover/app/cli/dashboard/","text":"dashboard_state_verification dashboard_state_verification ( ctx ) Perform cli commands verification (state checking). Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required Source code in src/trecover/app/cli/dashboard.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @cli . callback ( invoke_without_command = True ) def dashboard_state_verification ( ctx : Context ) -> None : \"\"\" Perform cli commands verification (state checking). Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. \"\"\" from trecover.config import log if var . DASHBOARD_PID . exists (): if ctx . invoked_subcommand in ( 'start' , None ): log . project_console . print ( ':rocket: The dashboard service is already started' , style = 'bright_blue' ) ctx . exit ( 0 ) elif ctx . invoked_subcommand is None : dashboard_start ( host = var . STREAMLIT_HOST , port = var . STREAMLIT_PORT , loglevel = var . LogLevel . info , attach = False , no_daemon = False ) elif ctx . invoked_subcommand != 'start' : log . project_console . print ( 'The dashboard service is not started' , style = 'yellow' ) ctx . exit ( 1 ) dashboard_start dashboard_start ( host = Option ( var . STREAMLIT_HOST , \"--host\" , \"-h\" , help = \"Bind socket to this host.\" , ), port = Option ( var . STREAMLIT_PORT , \"--port\" , \"-p\" , help = \"Bind socket to this port.\" , ), loglevel = Option ( var . LogLevel . info , \"--loglevel\" , \"-l\" , help = \"Logging level.\" , ), attach = Option ( False , \"--attach\" , \"-a\" , is_flag = True , help = \"Attach output and error streams\" , ), no_daemon = Option ( False , \"--no-daemon\" , is_flag = True , help = \"Do not run as a daemon process\" , ), ) Start dashboard service. Parameters: Name Type Description Default host str , default The address where the server will listen for client and browser connections. Use this if you want to bind the server to a specific address. If set, the server will only be accessible from this address, and not from any aliases (like localhost). Option(var.STREAMLIT_HOST, '--host', '-h', help='Bind socket to this host.') port int , default The port where the server will listen for browser connections. Option(var.STREAMLIT_PORT, '--port', '-p', help='Bind socket to this port.') loglevel var . LogLevel Level of logging. 'debug' attach bool , default Attach output and error streams. Option(False, '--attach', '-a', is_flag=True, help='Attach output and error streams') no_daemon bool , default Do not run as a daemon process. Option(False, '--no-daemon', is_flag=True, help='Do not run as a daemon process') Source code in src/trecover/app/cli/dashboard.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 @cli . command ( name = 'start' , help = 'Start service' ) def dashboard_start ( host : str = Option ( var . STREAMLIT_HOST , '--host' , '-h' , help = 'Bind socket to this host.' ), port : int = Option ( var . STREAMLIT_PORT , '--port' , '-p' , help = 'Bind socket to this port.' ), loglevel : var . LogLevel = Option ( var . LogLevel . info , '--loglevel' , '-l' , help = 'Logging level.' ), attach : bool = Option ( False , '--attach' , '-a' , is_flag = True , help = 'Attach output and error streams' ), no_daemon : bool = Option ( False , '--no-daemon' , is_flag = True , help = 'Do not run as a daemon process' ) ) -> None : \"\"\" Start dashboard service. Parameters ---------- host : str, default=ENV(STREAMLIT_HOST) or 'localhost' The address where the server will listen for client and browser connections. Use this if you want to bind the server to a specific address. If set, the server will only be accessible from this address, and not from any aliases (like localhost). port : int, default=ENV(STREAMLIT_PORT) or 8000 The port where the server will listen for browser connections. loglevel : {'debug', 'info', 'warning', 'error', 'critical'}, default='info' Level of logging. attach : bool, default=False Attach output and error streams. no_daemon : bool, default=False Do not run as a daemon process. \"\"\" from subprocess import run from trecover.config import log from trecover.app import dashboard from trecover.utils.cli import start_service argv = [ 'streamlit' , 'run' , dashboard . __file__ , '--server.address' , host , '--server.port' , str ( port ), '--logger.level' , loglevel , '--global.suppressDeprecationWarnings' , 'True' , '--theme.backgroundColor' , '#FFFFFF' , '--theme.secondaryBackgroundColor' , '#EAEAF2' , '--theme.primaryColor' , '#FF8068' , '--theme.textColor' , '#48466D' ] if no_daemon : run ( argv ) else : start_service ( argv , name = 'dashboard' , logfile = log . DASHBOARD_LOG , pidfile = var . DASHBOARD_PID ) if attach : dashboard_attach ( live = False ) dashboard_stop dashboard_stop () Stop dashboard service. Source code in src/trecover/app/cli/dashboard.py 92 93 94 95 96 97 98 @cli . command ( name = 'stop' , help = 'Stop service' ) def dashboard_stop () -> None : \"\"\" Stop dashboard service. \"\"\" from trecover.utils.cli import stop_service stop_service ( name = 'dashboard' , pidfile = var . DASHBOARD_PID , logfile = log . DASHBOARD_LOG ) dashboard_status dashboard_status () Display dashboard service status. Source code in src/trecover/app/cli/dashboard.py 101 102 103 104 105 106 107 @cli . command ( name = 'status' , help = 'Display service status' ) def dashboard_status () -> None : \"\"\" Display dashboard service status. \"\"\" from trecover.utils.cli import check_service check_service ( name = 'dashboard' , pidfile = var . DASHBOARD_PID ) dashboard_attach dashboard_attach ( live = Option ( False , \"--live\" , \"-l\" , is_flag = True , help = \"Stream only fresh log records\" , ) ) Attach local output stream to a running dashboard service. Parameters: Name Type Description Default live bool , Default Stream only fresh log records Option(False, '--live', '-l', is_flag=True, help='Stream only fresh log records') Source code in src/trecover/app/cli/dashboard.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 @cli . command ( name = 'attach' , help = 'Attach local output stream to a service' ) def dashboard_attach ( live : bool = Option ( False , '--live' , '-l' , is_flag = True , help = 'Stream only fresh log records' ) ) -> None : \"\"\" Attach local output stream to a running dashboard service. Parameters ---------- live : bool, Default=False Stream only fresh log records \"\"\" from trecover.config import log from trecover.utils.cli import stream with log . project_console . screen (): for record in stream (( 'dashboard' , log . DASHBOARD_LOG ), live = live ): log . project_console . print ( record ) log . project_console . clear ()","title":"Dashboard"},{"location":"src/trecover/app/cli/dashboard/#src.trecover.app.cli.dashboard.dashboard_state_verification","text":"dashboard_state_verification ( ctx ) Perform cli commands verification (state checking). Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required Source code in src/trecover/app/cli/dashboard.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @cli . callback ( invoke_without_command = True ) def dashboard_state_verification ( ctx : Context ) -> None : \"\"\" Perform cli commands verification (state checking). Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. \"\"\" from trecover.config import log if var . DASHBOARD_PID . exists (): if ctx . invoked_subcommand in ( 'start' , None ): log . project_console . print ( ':rocket: The dashboard service is already started' , style = 'bright_blue' ) ctx . exit ( 0 ) elif ctx . invoked_subcommand is None : dashboard_start ( host = var . STREAMLIT_HOST , port = var . STREAMLIT_PORT , loglevel = var . LogLevel . info , attach = False , no_daemon = False ) elif ctx . invoked_subcommand != 'start' : log . project_console . print ( 'The dashboard service is not started' , style = 'yellow' ) ctx . exit ( 1 )","title":"dashboard_state_verification()"},{"location":"src/trecover/app/cli/dashboard/#src.trecover.app.cli.dashboard.dashboard_start","text":"dashboard_start ( host = Option ( var . STREAMLIT_HOST , \"--host\" , \"-h\" , help = \"Bind socket to this host.\" , ), port = Option ( var . STREAMLIT_PORT , \"--port\" , \"-p\" , help = \"Bind socket to this port.\" , ), loglevel = Option ( var . LogLevel . info , \"--loglevel\" , \"-l\" , help = \"Logging level.\" , ), attach = Option ( False , \"--attach\" , \"-a\" , is_flag = True , help = \"Attach output and error streams\" , ), no_daemon = Option ( False , \"--no-daemon\" , is_flag = True , help = \"Do not run as a daemon process\" , ), ) Start dashboard service. Parameters: Name Type Description Default host str , default The address where the server will listen for client and browser connections. Use this if you want to bind the server to a specific address. If set, the server will only be accessible from this address, and not from any aliases (like localhost). Option(var.STREAMLIT_HOST, '--host', '-h', help='Bind socket to this host.') port int , default The port where the server will listen for browser connections. Option(var.STREAMLIT_PORT, '--port', '-p', help='Bind socket to this port.') loglevel var . LogLevel Level of logging. 'debug' attach bool , default Attach output and error streams. Option(False, '--attach', '-a', is_flag=True, help='Attach output and error streams') no_daemon bool , default Do not run as a daemon process. Option(False, '--no-daemon', is_flag=True, help='Do not run as a daemon process') Source code in src/trecover/app/cli/dashboard.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 @cli . command ( name = 'start' , help = 'Start service' ) def dashboard_start ( host : str = Option ( var . STREAMLIT_HOST , '--host' , '-h' , help = 'Bind socket to this host.' ), port : int = Option ( var . STREAMLIT_PORT , '--port' , '-p' , help = 'Bind socket to this port.' ), loglevel : var . LogLevel = Option ( var . LogLevel . info , '--loglevel' , '-l' , help = 'Logging level.' ), attach : bool = Option ( False , '--attach' , '-a' , is_flag = True , help = 'Attach output and error streams' ), no_daemon : bool = Option ( False , '--no-daemon' , is_flag = True , help = 'Do not run as a daemon process' ) ) -> None : \"\"\" Start dashboard service. Parameters ---------- host : str, default=ENV(STREAMLIT_HOST) or 'localhost' The address where the server will listen for client and browser connections. Use this if you want to bind the server to a specific address. If set, the server will only be accessible from this address, and not from any aliases (like localhost). port : int, default=ENV(STREAMLIT_PORT) or 8000 The port where the server will listen for browser connections. loglevel : {'debug', 'info', 'warning', 'error', 'critical'}, default='info' Level of logging. attach : bool, default=False Attach output and error streams. no_daemon : bool, default=False Do not run as a daemon process. \"\"\" from subprocess import run from trecover.config import log from trecover.app import dashboard from trecover.utils.cli import start_service argv = [ 'streamlit' , 'run' , dashboard . __file__ , '--server.address' , host , '--server.port' , str ( port ), '--logger.level' , loglevel , '--global.suppressDeprecationWarnings' , 'True' , '--theme.backgroundColor' , '#FFFFFF' , '--theme.secondaryBackgroundColor' , '#EAEAF2' , '--theme.primaryColor' , '#FF8068' , '--theme.textColor' , '#48466D' ] if no_daemon : run ( argv ) else : start_service ( argv , name = 'dashboard' , logfile = log . DASHBOARD_LOG , pidfile = var . DASHBOARD_PID ) if attach : dashboard_attach ( live = False )","title":"dashboard_start()"},{"location":"src/trecover/app/cli/dashboard/#src.trecover.app.cli.dashboard.dashboard_stop","text":"dashboard_stop () Stop dashboard service. Source code in src/trecover/app/cli/dashboard.py 92 93 94 95 96 97 98 @cli . command ( name = 'stop' , help = 'Stop service' ) def dashboard_stop () -> None : \"\"\" Stop dashboard service. \"\"\" from trecover.utils.cli import stop_service stop_service ( name = 'dashboard' , pidfile = var . DASHBOARD_PID , logfile = log . DASHBOARD_LOG )","title":"dashboard_stop()"},{"location":"src/trecover/app/cli/dashboard/#src.trecover.app.cli.dashboard.dashboard_status","text":"dashboard_status () Display dashboard service status. Source code in src/trecover/app/cli/dashboard.py 101 102 103 104 105 106 107 @cli . command ( name = 'status' , help = 'Display service status' ) def dashboard_status () -> None : \"\"\" Display dashboard service status. \"\"\" from trecover.utils.cli import check_service check_service ( name = 'dashboard' , pidfile = var . DASHBOARD_PID )","title":"dashboard_status()"},{"location":"src/trecover/app/cli/dashboard/#src.trecover.app.cli.dashboard.dashboard_attach","text":"dashboard_attach ( live = Option ( False , \"--live\" , \"-l\" , is_flag = True , help = \"Stream only fresh log records\" , ) ) Attach local output stream to a running dashboard service. Parameters: Name Type Description Default live bool , Default Stream only fresh log records Option(False, '--live', '-l', is_flag=True, help='Stream only fresh log records') Source code in src/trecover/app/cli/dashboard.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 @cli . command ( name = 'attach' , help = 'Attach local output stream to a service' ) def dashboard_attach ( live : bool = Option ( False , '--live' , '-l' , is_flag = True , help = 'Stream only fresh log records' ) ) -> None : \"\"\" Attach local output stream to a running dashboard service. Parameters ---------- live : bool, Default=False Stream only fresh log records \"\"\" from trecover.config import log from trecover.utils.cli import stream with log . project_console . screen (): for record in stream (( 'dashboard' , log . DASHBOARD_LOG ), live = live ): log . project_console . print ( record ) log . project_console . clear ()","title":"dashboard_attach()"},{"location":"src/trecover/app/cli/download/","text":"download_data download_data ( link = Option ( var . TRAIN_DATA_URL , help = \"Link to the train data on Yandex disk or GitHub\" , ), save_dir = Option ( var . DATA_DIR , help = \"Path where to store downloaded data\" , ), yandex_disk = Option ( False , is_flag = True , help = \"If the link is to Yandex disk\" , ), ) Download train data from Yandex disk or GitHub. Parameters: Name Type Description Default link str , default Sharing link to the train data on Yandex disk or GitHub. Option(var.TRAIN_DATA_URL, help='Link to the train data on Yandex disk or GitHub') save_dir Path , default Path where to store downloaded data. Option(var.DATA_DIR, help='Path where to store downloaded data') yandex_disk bool , default If the link is to Yandex disk. Option(False, is_flag=True, help='If the link is to Yandex disk') Source code in src/trecover/app/cli/download.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 @cli . command ( name = 'data' , help = 'Download train data' ) def download_data ( link : str = Option ( var . TRAIN_DATA_URL , help = 'Link to the train data on Yandex disk or GitHub' ), save_dir : Path = Option ( var . DATA_DIR , help = 'Path where to store downloaded data' ), yandex_disk : bool = Option ( False , is_flag = True , help = 'If the link is to Yandex disk' ) ) -> None : \"\"\" Download train data from Yandex disk or GitHub. Parameters ---------- link : str, default=var.TRAIN_DATA_URL Sharing link to the train data on Yandex disk or GitHub. save_dir : Path, default=var.DATA_DIR Path where to store downloaded data. yandex_disk : bool, default=False If the link is to Yandex disk. \"\"\" from trecover.utils.cli import download_archive download_archive ( link = link , save_dir = save_dir , yandex_disk = yandex_disk ) download_artifacts download_artifacts ( version = Option ( \"latest\" , help = \"Artifacts' version\" ), archive_link = Option ( None , help = \"Link to the artifacts archive on Yandex disk or GitHub\" , ), save_dir = Option ( var . INFERENCE_DIR , help = \"Path where to save downloaded artifacts\" , ), yandex_disk = Option ( False , is_flag = True , help = \"If the archive_link is to Yandex disk\" , ), show = Option ( False , is_flag = True , help = \"Print available artifacts' versions\" , ), ) Download model artifacts by specified version or archive_link to Yandex disk or GitHub. Parameters: Name Type Description Default version str , default Artifacts' version. Option('latest', help=\"Artifacts' version\") archive_link str , default Sharing link to the model artifacts archive on Yandex disk or GitHub. Option(None, help='Link to the artifacts archive on Yandex disk or GitHub') save_dir Path , default Path where to save downloaded artifacts. Option(var.INFERENCE_DIR, help='Path where to save downloaded artifacts') yandex_disk bool , default If the link is to Yandex disk. Option(False, is_flag=True, help='If the archive_link is to Yandex disk') show bool , default Print available artifacts' versions. Option(False, is_flag=True, help=\"Print available artifacts' versions\") Source code in src/trecover/app/cli/download.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 @cli . command ( name = 'artifacts' , help = 'Download model artifacts by specified version or archive_link' ) def download_artifacts ( version : str = Option ( 'latest' , help = \"Artifacts' version\" ), archive_link : str = Option ( None , help = 'Link to the artifacts archive on Yandex disk or GitHub' ), save_dir : Path = Option ( var . INFERENCE_DIR , help = 'Path where to save downloaded artifacts' ), yandex_disk : bool = Option ( False , is_flag = True , help = 'If the archive_link is to Yandex disk' ), show : bool = Option ( False , is_flag = True , help = \"Print available artifacts' versions\" ) ) -> None : \"\"\" Download model artifacts by specified version or archive_link to Yandex disk or GitHub. Parameters ---------- version : str, default='latest' Artifacts' version. archive_link : str, default=None Sharing link to the model artifacts archive on Yandex disk or GitHub. save_dir : Path, default=var.INFERENCE_DIR Path where to save downloaded artifacts. yandex_disk : bool, default=False If the link is to Yandex disk. show : bool, default=False Print available artifacts' versions. \"\"\" from rich.prompt import Confirm from trecover.utils.cli import download_archive , download_from_github if show : log . project_console . print ( var . CHECKPOINT_URLS . keys ()) elif archive_link : download_archive ( link = archive_link , save_dir = save_dir , yandex_disk = yandex_disk ) elif version in var . CHECKPOINT_URLS : download_from_github ( direct_link = var . CHECKPOINT_URLS [ version ][ 'model' ], save_dir = save_dir ) download_from_github ( direct_link = var . CHECKPOINT_URLS [ version ][ 'config' ], save_dir = save_dir ) elif Confirm . ask ( prompt = '[bright_blue]Specified version was not found. Continue downloading the latest version?' , default = True , console = log . project_console ): download_from_github ( direct_link = var . CHECKPOINT_URLS [ 'latest' ][ 'model' ], save_dir = save_dir ) download_from_github ( direct_link = var . CHECKPOINT_URLS [ 'latest' ][ 'config' ], save_dir = save_dir )","title":"Download"},{"location":"src/trecover/app/cli/download/#src.trecover.app.cli.download.download_data","text":"download_data ( link = Option ( var . TRAIN_DATA_URL , help = \"Link to the train data on Yandex disk or GitHub\" , ), save_dir = Option ( var . DATA_DIR , help = \"Path where to store downloaded data\" , ), yandex_disk = Option ( False , is_flag = True , help = \"If the link is to Yandex disk\" , ), ) Download train data from Yandex disk or GitHub. Parameters: Name Type Description Default link str , default Sharing link to the train data on Yandex disk or GitHub. Option(var.TRAIN_DATA_URL, help='Link to the train data on Yandex disk or GitHub') save_dir Path , default Path where to store downloaded data. Option(var.DATA_DIR, help='Path where to store downloaded data') yandex_disk bool , default If the link is to Yandex disk. Option(False, is_flag=True, help='If the link is to Yandex disk') Source code in src/trecover/app/cli/download.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 @cli . command ( name = 'data' , help = 'Download train data' ) def download_data ( link : str = Option ( var . TRAIN_DATA_URL , help = 'Link to the train data on Yandex disk or GitHub' ), save_dir : Path = Option ( var . DATA_DIR , help = 'Path where to store downloaded data' ), yandex_disk : bool = Option ( False , is_flag = True , help = 'If the link is to Yandex disk' ) ) -> None : \"\"\" Download train data from Yandex disk or GitHub. Parameters ---------- link : str, default=var.TRAIN_DATA_URL Sharing link to the train data on Yandex disk or GitHub. save_dir : Path, default=var.DATA_DIR Path where to store downloaded data. yandex_disk : bool, default=False If the link is to Yandex disk. \"\"\" from trecover.utils.cli import download_archive download_archive ( link = link , save_dir = save_dir , yandex_disk = yandex_disk )","title":"download_data()"},{"location":"src/trecover/app/cli/download/#src.trecover.app.cli.download.download_artifacts","text":"download_artifacts ( version = Option ( \"latest\" , help = \"Artifacts' version\" ), archive_link = Option ( None , help = \"Link to the artifacts archive on Yandex disk or GitHub\" , ), save_dir = Option ( var . INFERENCE_DIR , help = \"Path where to save downloaded artifacts\" , ), yandex_disk = Option ( False , is_flag = True , help = \"If the archive_link is to Yandex disk\" , ), show = Option ( False , is_flag = True , help = \"Print available artifacts' versions\" , ), ) Download model artifacts by specified version or archive_link to Yandex disk or GitHub. Parameters: Name Type Description Default version str , default Artifacts' version. Option('latest', help=\"Artifacts' version\") archive_link str , default Sharing link to the model artifacts archive on Yandex disk or GitHub. Option(None, help='Link to the artifacts archive on Yandex disk or GitHub') save_dir Path , default Path where to save downloaded artifacts. Option(var.INFERENCE_DIR, help='Path where to save downloaded artifacts') yandex_disk bool , default If the link is to Yandex disk. Option(False, is_flag=True, help='If the archive_link is to Yandex disk') show bool , default Print available artifacts' versions. Option(False, is_flag=True, help=\"Print available artifacts' versions\") Source code in src/trecover/app/cli/download.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 @cli . command ( name = 'artifacts' , help = 'Download model artifacts by specified version or archive_link' ) def download_artifacts ( version : str = Option ( 'latest' , help = \"Artifacts' version\" ), archive_link : str = Option ( None , help = 'Link to the artifacts archive on Yandex disk or GitHub' ), save_dir : Path = Option ( var . INFERENCE_DIR , help = 'Path where to save downloaded artifacts' ), yandex_disk : bool = Option ( False , is_flag = True , help = 'If the archive_link is to Yandex disk' ), show : bool = Option ( False , is_flag = True , help = \"Print available artifacts' versions\" ) ) -> None : \"\"\" Download model artifacts by specified version or archive_link to Yandex disk or GitHub. Parameters ---------- version : str, default='latest' Artifacts' version. archive_link : str, default=None Sharing link to the model artifacts archive on Yandex disk or GitHub. save_dir : Path, default=var.INFERENCE_DIR Path where to save downloaded artifacts. yandex_disk : bool, default=False If the link is to Yandex disk. show : bool, default=False Print available artifacts' versions. \"\"\" from rich.prompt import Confirm from trecover.utils.cli import download_archive , download_from_github if show : log . project_console . print ( var . CHECKPOINT_URLS . keys ()) elif archive_link : download_archive ( link = archive_link , save_dir = save_dir , yandex_disk = yandex_disk ) elif version in var . CHECKPOINT_URLS : download_from_github ( direct_link = var . CHECKPOINT_URLS [ version ][ 'model' ], save_dir = save_dir ) download_from_github ( direct_link = var . CHECKPOINT_URLS [ version ][ 'config' ], save_dir = save_dir ) elif Confirm . ask ( prompt = '[bright_blue]Specified version was not found. Continue downloading the latest version?' , default = True , console = log . project_console ): download_from_github ( direct_link = var . CHECKPOINT_URLS [ 'latest' ][ 'model' ], save_dir = save_dir ) download_from_github ( direct_link = var . CHECKPOINT_URLS [ 'latest' ][ 'config' ], save_dir = save_dir )","title":"download_artifacts()"},{"location":"src/trecover/app/cli/mlflow/","text":"mlflow_state_verification mlflow_state_verification ( ctx ) Perform cli commands verification (state checking). Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required Source code in src/trecover/app/cli/mlflow.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @cli . callback ( invoke_without_command = True ) def mlflow_state_verification ( ctx : Context ) -> None : \"\"\" Perform cli commands verification (state checking). Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. \"\"\" from trecover.config import log , exp_var if var . MLFLOW_PID . exists (): if ctx . invoked_subcommand in ( 'start' , None ): log . project_console . print ( ':rocket: The mlflow service is already started' , style = 'bright_blue' ) ctx . exit ( 0 ) elif ctx . invoked_subcommand is None : mlflow_start ( host = var . MLFLOW_HOST , port = var . MLFLOW_PORT , concurrency = var . MLFLOW_WORKERS , registry = exp_var . MLFLOW_REGISTRY_DIR . as_uri (), backend_uri = exp_var . MLFLOW_BACKEND , only_ui = False , attach = False ) elif ctx . invoked_subcommand != 'start' : log . project_console . print ( 'The mlflow service is not started' , style = 'yellow' ) ctx . exit ( 1 ) mlflow_start mlflow_start ( host = Option ( var . MLFLOW_HOST , \"--host\" , \"-h\" , help = \"Bind socket to this host.\" , ), port = Option ( var . MLFLOW_PORT , \"--port\" , \"-p\" , help = \"Bind socket to this port.\" , ), concurrency = Option ( var . MLFLOW_WORKERS , \"-c\" , help = \"The number of mlflow server workers.\" , ), registry = Option ( None , \"--registry\" , \"-r\" , help = \"Path to local directory to store artifacts.\" , ), backend_uri = Option ( None , \"--backend\" , help = \"Backend uri.\" ), only_ui = Option ( False , \"--only-ui\" , is_flag = True , help = \"Launch only the Mlflow tracking UI\" , ), attach = Option ( False , \"--attach\" , \"-a\" , is_flag = True , help = \"Attach output and error streams\" , ), ) Start dashboard service. Parameters: Name Type Description Default host str , default The address where the server will listen for client and browser connections. Use this if you want to bind the server to a specific address. If set, the server will only be accessible from this address, and not from any aliases (like localhost). Option(var.MLFLOW_HOST, '--host', '-h', help='Bind socket to this host.') port int , default The port where the server will listen for browser connections. Option(var.MLFLOW_PORT, '--port', '-p', help='Bind socket to this port.') concurrency int , default The number of mlflow server workers. Option(var.MLFLOW_WORKERS, '-c', help='The number of mlflow server workers.') registry str , default URI to which to persist experiment and run data. Acceptable URIs are SQLAlchemy-compatible database connection strings (e.g. 'sqlite:///path/to/file.db') or local filesystem URIs (e.g. 'file:///absolute/path/to/directory'). By default, data will be logged to the ./mlruns directory. Option(None, '--registry', '-r', help='Path to local directory to store artifacts.') backend_uri str , default Local or S3 URI to store artifacts, for new experiments. Note that this flag does not impact already-created experiments. Option(None, '--backend', help='Backend uri.') only_ui bool , default Launch only the Mlflow tracking UI. Option(False, '--only-ui', is_flag=True, help='Launch only the Mlflow tracking UI') attach bool , default Attach output and error streams. Option(False, '--attach', '-a', is_flag=True, help='Attach output and error streams') Raises: Type Description typer.BadParameter: If concurrency option is not equal to one for windows platform. Source code in src/trecover/app/cli/mlflow.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 @cli . command ( name = 'start' , help = 'Start service' ) def mlflow_start ( host : str = Option ( var . MLFLOW_HOST , '--host' , '-h' , help = 'Bind socket to this host.' ), port : int = Option ( var . MLFLOW_PORT , '--port' , '-p' , help = 'Bind socket to this port.' ), concurrency : int = Option ( var . MLFLOW_WORKERS , '-c' , help = 'The number of mlflow server workers.' ), registry : Optional [ str ] = Option ( None , '--registry' , '-r' , help = 'Path to local directory to store artifacts.' ), backend_uri : Optional [ str ] = Option ( None , '--backend' , help = 'Backend uri.' ), only_ui : bool = Option ( False , '--only-ui' , is_flag = True , help = 'Launch only the Mlflow tracking UI' ), attach : bool = Option ( False , '--attach' , '-a' , is_flag = True , help = 'Attach output and error streams' ) ) -> None : \"\"\" Start dashboard service. Parameters ---------- host : str, default=ENV(MLFLOW_HOST) or 'localhost' The address where the server will listen for client and browser connections. Use this if you want to bind the server to a specific address. If set, the server will only be accessible from this address, and not from any aliases (like localhost). port : int, default=ENV(MLFLOW_PORT) or 8002 The port where the server will listen for browser connections. concurrency : int, default=ENV(MLFLOW_WORKERS) or 1 The number of mlflow server workers. registry : str, default=ENV(MLFLOW_REGISTRY_DIR) or 'file:///<BASE_DIR>/experiments/mlflow_registry' URI to which to persist experiment and run data. Acceptable URIs are SQLAlchemy-compatible database connection strings (e.g. 'sqlite:///path/to/file.db') or local filesystem URIs (e.g. 'file:///absolute/path/to/directory'). By default, data will be logged to the ./mlruns directory. backend_uri : str, default=ENV(MLFLOW_BACKEND) or 'sqlite:///<BASE_DIR>/experiments/mlflow_registry/mlflow.db' Local or S3 URI to store artifacts, for new experiments. Note that this flag does not impact already-created experiments. only_ui : bool, default=False Launch only the Mlflow tracking UI. attach : bool, default=False Attach output and error streams. Raises ------ typer.BadParameter: If concurrency option is not equal to one for windows platform. \"\"\" import platform from trecover.config import log , exp_var from trecover.utils.cli import start_service if ( is_windows := platform . system () == 'Windows' ) and concurrency != 1 : raise BadParameter ( \"Windows platform does not support concurrency option\" ) command = 'ui' if only_ui else 'server' registry = registry or str ( exp_var . MLFLOW_REGISTRY_DIR . as_uri ()) backend_uri = backend_uri or str ( exp_var . MLFLOW_BACKEND ) argv = [ 'mlflow' , command , '--host' , host , '--port' , str ( port ), '--default-artifact-root' , backend_uri , '--backend-store-uri' , registry ] if not only_ui and not is_windows : argv . extend ([ '--workers' , str ( concurrency )]) start_service ( argv , name = 'mlflow' , logfile = log . MLFLOW_LOG , pidfile = var . MLFLOW_PID ) if attach : mlflow_attach ( live = False ) mlflow_stop mlflow_stop () Stop dashboard service. Source code in src/trecover/app/cli/mlflow.py 113 114 115 116 117 118 119 120 @cli . command ( name = 'stop' , help = 'Stop service' ) def mlflow_stop () -> None : \"\"\" Stop dashboard service. \"\"\" from trecover.config import log from trecover.utils.cli import stop_service stop_service ( name = 'mlflow' , pidfile = var . MLFLOW_PID , logfile = log . MLFLOW_LOG ) mlflow_status mlflow_status () Display dashboard service status. Source code in src/trecover/app/cli/mlflow.py 123 124 125 126 127 128 129 @cli . command ( name = 'status' , help = 'Display service status' ) def mlflow_status () -> None : \"\"\" Display dashboard service status. \"\"\" from trecover.utils.cli import check_service check_service ( name = 'mlflow' , pidfile = var . MLFLOW_PID ) mlflow_attach mlflow_attach ( live = Option ( False , \"--live\" , \"-l\" , is_flag = True , help = \"Stream only fresh log records\" , ) ) Attach local output stream to a running dashboard service. Parameters: Name Type Description Default live bool , Default Stream only fresh log records. Option(False, '--live', '-l', is_flag=True, help='Stream only fresh log records') Source code in src/trecover/app/cli/mlflow.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 @cli . command ( name = 'attach' , help = 'Attach local output stream to a service' ) def mlflow_attach ( live : bool = Option ( False , '--live' , '-l' , is_flag = True , help = 'Stream only fresh log records' ) ) -> None : \"\"\" Attach local output stream to a running dashboard service. Parameters ---------- live : bool, Default=False Stream only fresh log records. \"\"\" from trecover.config import log from trecover.utils.cli import stream with log . project_console . screen (): for record in stream (( 'mlflow' , log . MLFLOW_LOG ), live = live ): log . project_console . print ( record ) log . project_console . clear ()","title":"Mlflow"},{"location":"src/trecover/app/cli/mlflow/#src.trecover.app.cli.mlflow.mlflow_state_verification","text":"mlflow_state_verification ( ctx ) Perform cli commands verification (state checking). Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required Source code in src/trecover/app/cli/mlflow.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @cli . callback ( invoke_without_command = True ) def mlflow_state_verification ( ctx : Context ) -> None : \"\"\" Perform cli commands verification (state checking). Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. \"\"\" from trecover.config import log , exp_var if var . MLFLOW_PID . exists (): if ctx . invoked_subcommand in ( 'start' , None ): log . project_console . print ( ':rocket: The mlflow service is already started' , style = 'bright_blue' ) ctx . exit ( 0 ) elif ctx . invoked_subcommand is None : mlflow_start ( host = var . MLFLOW_HOST , port = var . MLFLOW_PORT , concurrency = var . MLFLOW_WORKERS , registry = exp_var . MLFLOW_REGISTRY_DIR . as_uri (), backend_uri = exp_var . MLFLOW_BACKEND , only_ui = False , attach = False ) elif ctx . invoked_subcommand != 'start' : log . project_console . print ( 'The mlflow service is not started' , style = 'yellow' ) ctx . exit ( 1 )","title":"mlflow_state_verification()"},{"location":"src/trecover/app/cli/mlflow/#src.trecover.app.cli.mlflow.mlflow_start","text":"mlflow_start ( host = Option ( var . MLFLOW_HOST , \"--host\" , \"-h\" , help = \"Bind socket to this host.\" , ), port = Option ( var . MLFLOW_PORT , \"--port\" , \"-p\" , help = \"Bind socket to this port.\" , ), concurrency = Option ( var . MLFLOW_WORKERS , \"-c\" , help = \"The number of mlflow server workers.\" , ), registry = Option ( None , \"--registry\" , \"-r\" , help = \"Path to local directory to store artifacts.\" , ), backend_uri = Option ( None , \"--backend\" , help = \"Backend uri.\" ), only_ui = Option ( False , \"--only-ui\" , is_flag = True , help = \"Launch only the Mlflow tracking UI\" , ), attach = Option ( False , \"--attach\" , \"-a\" , is_flag = True , help = \"Attach output and error streams\" , ), ) Start dashboard service. Parameters: Name Type Description Default host str , default The address where the server will listen for client and browser connections. Use this if you want to bind the server to a specific address. If set, the server will only be accessible from this address, and not from any aliases (like localhost). Option(var.MLFLOW_HOST, '--host', '-h', help='Bind socket to this host.') port int , default The port where the server will listen for browser connections. Option(var.MLFLOW_PORT, '--port', '-p', help='Bind socket to this port.') concurrency int , default The number of mlflow server workers. Option(var.MLFLOW_WORKERS, '-c', help='The number of mlflow server workers.') registry str , default URI to which to persist experiment and run data. Acceptable URIs are SQLAlchemy-compatible database connection strings (e.g. 'sqlite:///path/to/file.db') or local filesystem URIs (e.g. 'file:///absolute/path/to/directory'). By default, data will be logged to the ./mlruns directory. Option(None, '--registry', '-r', help='Path to local directory to store artifacts.') backend_uri str , default Local or S3 URI to store artifacts, for new experiments. Note that this flag does not impact already-created experiments. Option(None, '--backend', help='Backend uri.') only_ui bool , default Launch only the Mlflow tracking UI. Option(False, '--only-ui', is_flag=True, help='Launch only the Mlflow tracking UI') attach bool , default Attach output and error streams. Option(False, '--attach', '-a', is_flag=True, help='Attach output and error streams') Raises: Type Description typer.BadParameter: If concurrency option is not equal to one for windows platform. Source code in src/trecover/app/cli/mlflow.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 @cli . command ( name = 'start' , help = 'Start service' ) def mlflow_start ( host : str = Option ( var . MLFLOW_HOST , '--host' , '-h' , help = 'Bind socket to this host.' ), port : int = Option ( var . MLFLOW_PORT , '--port' , '-p' , help = 'Bind socket to this port.' ), concurrency : int = Option ( var . MLFLOW_WORKERS , '-c' , help = 'The number of mlflow server workers.' ), registry : Optional [ str ] = Option ( None , '--registry' , '-r' , help = 'Path to local directory to store artifacts.' ), backend_uri : Optional [ str ] = Option ( None , '--backend' , help = 'Backend uri.' ), only_ui : bool = Option ( False , '--only-ui' , is_flag = True , help = 'Launch only the Mlflow tracking UI' ), attach : bool = Option ( False , '--attach' , '-a' , is_flag = True , help = 'Attach output and error streams' ) ) -> None : \"\"\" Start dashboard service. Parameters ---------- host : str, default=ENV(MLFLOW_HOST) or 'localhost' The address where the server will listen for client and browser connections. Use this if you want to bind the server to a specific address. If set, the server will only be accessible from this address, and not from any aliases (like localhost). port : int, default=ENV(MLFLOW_PORT) or 8002 The port where the server will listen for browser connections. concurrency : int, default=ENV(MLFLOW_WORKERS) or 1 The number of mlflow server workers. registry : str, default=ENV(MLFLOW_REGISTRY_DIR) or 'file:///<BASE_DIR>/experiments/mlflow_registry' URI to which to persist experiment and run data. Acceptable URIs are SQLAlchemy-compatible database connection strings (e.g. 'sqlite:///path/to/file.db') or local filesystem URIs (e.g. 'file:///absolute/path/to/directory'). By default, data will be logged to the ./mlruns directory. backend_uri : str, default=ENV(MLFLOW_BACKEND) or 'sqlite:///<BASE_DIR>/experiments/mlflow_registry/mlflow.db' Local or S3 URI to store artifacts, for new experiments. Note that this flag does not impact already-created experiments. only_ui : bool, default=False Launch only the Mlflow tracking UI. attach : bool, default=False Attach output and error streams. Raises ------ typer.BadParameter: If concurrency option is not equal to one for windows platform. \"\"\" import platform from trecover.config import log , exp_var from trecover.utils.cli import start_service if ( is_windows := platform . system () == 'Windows' ) and concurrency != 1 : raise BadParameter ( \"Windows platform does not support concurrency option\" ) command = 'ui' if only_ui else 'server' registry = registry or str ( exp_var . MLFLOW_REGISTRY_DIR . as_uri ()) backend_uri = backend_uri or str ( exp_var . MLFLOW_BACKEND ) argv = [ 'mlflow' , command , '--host' , host , '--port' , str ( port ), '--default-artifact-root' , backend_uri , '--backend-store-uri' , registry ] if not only_ui and not is_windows : argv . extend ([ '--workers' , str ( concurrency )]) start_service ( argv , name = 'mlflow' , logfile = log . MLFLOW_LOG , pidfile = var . MLFLOW_PID ) if attach : mlflow_attach ( live = False )","title":"mlflow_start()"},{"location":"src/trecover/app/cli/mlflow/#src.trecover.app.cli.mlflow.mlflow_stop","text":"mlflow_stop () Stop dashboard service. Source code in src/trecover/app/cli/mlflow.py 113 114 115 116 117 118 119 120 @cli . command ( name = 'stop' , help = 'Stop service' ) def mlflow_stop () -> None : \"\"\" Stop dashboard service. \"\"\" from trecover.config import log from trecover.utils.cli import stop_service stop_service ( name = 'mlflow' , pidfile = var . MLFLOW_PID , logfile = log . MLFLOW_LOG )","title":"mlflow_stop()"},{"location":"src/trecover/app/cli/mlflow/#src.trecover.app.cli.mlflow.mlflow_status","text":"mlflow_status () Display dashboard service status. Source code in src/trecover/app/cli/mlflow.py 123 124 125 126 127 128 129 @cli . command ( name = 'status' , help = 'Display service status' ) def mlflow_status () -> None : \"\"\" Display dashboard service status. \"\"\" from trecover.utils.cli import check_service check_service ( name = 'mlflow' , pidfile = var . MLFLOW_PID )","title":"mlflow_status()"},{"location":"src/trecover/app/cli/mlflow/#src.trecover.app.cli.mlflow.mlflow_attach","text":"mlflow_attach ( live = Option ( False , \"--live\" , \"-l\" , is_flag = True , help = \"Stream only fresh log records\" , ) ) Attach local output stream to a running dashboard service. Parameters: Name Type Description Default live bool , Default Stream only fresh log records. Option(False, '--live', '-l', is_flag=True, help='Stream only fresh log records') Source code in src/trecover/app/cli/mlflow.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 @cli . command ( name = 'attach' , help = 'Attach local output stream to a service' ) def mlflow_attach ( live : bool = Option ( False , '--live' , '-l' , is_flag = True , help = 'Stream only fresh log records' ) ) -> None : \"\"\" Attach local output stream to a running dashboard service. Parameters ---------- live : bool, Default=False Stream only fresh log records. \"\"\" from trecover.config import log from trecover.utils.cli import stream with log . project_console . screen (): for record in stream (( 'mlflow' , log . MLFLOW_LOG ), live = live ): log . project_console . print ( record ) log . project_console . clear ()","title":"mlflow_attach()"},{"location":"src/trecover/app/cli/worker/","text":"worker_state_verification worker_state_verification ( ctx ) Perform cli commands verification (state checking). Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required Source code in src/trecover/app/cli/worker.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 @cli . callback ( invoke_without_command = True ) def worker_state_verification ( ctx : Context ) -> None : \"\"\" Perform cli commands verification (state checking). Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. \"\"\" from trecover.config import log if var . WORKER_PID . exists (): if ctx . invoked_subcommand in ( 'start' , None ): log . project_console . print ( ':rocket: The worker service is already started' , style = 'bright_blue' ) ctx . exit ( 0 ) elif ctx . invoked_subcommand is None : worker_start ( name = 'TRecoverWorker' , pool = var . PoolType . solo , loglevel = var . LogLevel . info , concurrency = var . CELERY_WORKERS , broker_url = var . CELERY_BROKER , backend_url = var . CELERY_BACKEND , attach = False , no_daemon = False ) elif ctx . invoked_subcommand != 'start' : log . project_console . print ( 'The worker service is not started' , style = 'yellow' ) ctx . exit ( 1 ) worker_start worker_start ( name = Option ( \"TRecoverWorker\" , \"--name\" , \"-n\" , help = \"Set custom worker hostname.\" , ), pool = Option ( var . PoolType . solo , \"--pool\" , \"-p\" , help = \"Worker processes/threads pool type.\" , ), loglevel = Option ( var . LogLevel . info , \"--loglevel\" , \"-l\" , help = \"Logging level.\" , ), concurrency = Option ( var . CELERY_WORKERS , \"-c\" , help = \"The number of worker processes.\" , ), broker_url = Option ( var . CELERY_BROKER , \"--broker\" , help = \"Broker url.\" ), backend_url = Option ( var . CELERY_BACKEND , \"--backend\" , help = \"Backend url.\" ), attach = Option ( False , \"--attach\" , \"-a\" , is_flag = True , help = \"Attach output and error streams\" , ), no_daemon = Option ( False , \"--no-daemon\" , is_flag = True , help = \"Do not run as a daemon process\" , ), ) Start API service. Parameters: Name Type Description Default name str , default Custom worker hostname. Option('TRecoverWorker', '--name', '-n', help='Set custom worker hostname.') pool str Worker processes/threads pool type. Option(var.PoolType.solo, '--pool', '-p', help='Worker processes/threads pool type.') loglevel var . LogLevel Level of logging. 'debug' concurrency int , default The number of worker processes. Option(var.CELERY_WORKERS, '-c', help='The number of worker processes.') broker_url str , default Broker url. Option(var.CELERY_BROKER, '--broker', help='Broker url.') backend_url str , default Backend url. Option(var.CELERY_BACKEND, '--backend', help='Backend url.') attach bool , default Attach output and error streams. Option(False, '--attach', '-a', is_flag=True, help='Attach output and error streams') no_daemon bool , default Do not run as a daemon process. Option(False, '--no-daemon', is_flag=True, help='Do not run as a daemon process') Raises: Type Description typer.BadParameter: If non-solo pool type is selected for windows platform. Source code in src/trecover/app/cli/worker.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 @cli . command ( name = 'start' , help = 'Start service' ) def worker_start ( name : str = Option ( 'TRecoverWorker' , '--name' , '-n' , help = 'Set custom worker hostname.' ), pool : var . PoolType = Option ( var . PoolType . solo , '--pool' , '-p' , help = 'Worker processes/threads pool type.' ), loglevel : var . LogLevel = Option ( var . LogLevel . info , '--loglevel' , '-l' , help = 'Logging level.' ), concurrency : int = Option ( var . CELERY_WORKERS , '-c' , help = 'The number of worker processes.' ), broker_url : str = Option ( var . CELERY_BROKER , '--broker' , help = 'Broker url.' ), backend_url : str = Option ( var . CELERY_BACKEND , '--backend' , help = 'Backend url.' ), attach : bool = Option ( False , '--attach' , '-a' , is_flag = True , help = 'Attach output and error streams' ), no_daemon : bool = Option ( False , '--no-daemon' , is_flag = True , help = 'Do not run as a daemon process' ) ) -> None : \"\"\" Start API service. Parameters ---------- name : str, default='TRecoverWorker' Custom worker hostname. pool : str, {'prefork', 'eventlet', 'gevent', 'processes', 'solo'}, default='solo' Worker processes/threads pool type. loglevel : {'debug', 'info', 'warning', 'error', 'critical'}, default='info' Level of logging. concurrency : int, default=ENV(CELERY_WORKERS) or 1 The number of worker processes. broker_url : str, default=ENV(CELERY_BROKER) or 'pyamqp://guest@localhost' Broker url. backend_url : str, default=ENV(CELERY_BACKEND) or 'redis://localhost' Backend url. attach : bool, default=False Attach output and error streams. no_daemon : bool, default=False Do not run as a daemon process. Raises ------ typer.BadParameter: If non-solo pool type is selected for windows platform. \"\"\" import platform from subprocess import run from trecover.config import log from trecover.utils.cli import start_service if platform . system () == 'Windows' and pool != var . PoolType . solo : raise BadParameter ( \"Windows platform only supports 'solo' pool\" ) argv = [ 'celery' , '--broker' , broker_url , '--result-backend' , backend_url , '--app' , 'trecover.app.api.backend.celeryapp' , 'worker' , '--hostname' , name , '--concurrency' , str ( concurrency ), '--pool' , pool , '--loglevel' , loglevel ] if no_daemon : run ( argv ) else : start_service ( argv , name = 'worker' , logfile = log . WORKER_LOG , pidfile = var . WORKER_PID ) if attach : worker_attach ( live = False ) worker_stop worker_stop () Stop worker service. Source code in src/trecover/app/cli/worker.py 107 108 109 110 111 112 113 114 @cli . command ( name = 'stop' , help = 'Stop service' ) def worker_stop () -> None : \"\"\" Stop worker service. \"\"\" from trecover.config import log from trecover.utils.cli import stop_service stop_service ( name = 'worker' , pidfile = var . WORKER_PID , logfile = log . WORKER_LOG ) worker_status worker_status () Display worker service status. Source code in src/trecover/app/cli/worker.py 117 118 119 120 121 122 123 @cli . command ( name = 'status' , help = 'Display service status' ) def worker_status () -> None : \"\"\" Display worker service status. \"\"\" from trecover.utils.cli import check_service check_service ( name = 'worker' , pidfile = var . WORKER_PID ) worker_attach worker_attach ( live = Option ( False , \"--live\" , \"-l\" , is_flag = True , help = \"Stream only fresh log records\" , ) ) Attach local output stream to a running worker service. Parameters: Name Type Description Default live bool , Default Stream only fresh log records. Option(False, '--live', '-l', is_flag=True, help='Stream only fresh log records') Source code in src/trecover/app/cli/worker.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 @cli . command ( name = 'attach' , help = 'Attach local output stream to a service' ) def worker_attach ( live : bool = Option ( False , '--live' , '-l' , is_flag = True , help = 'Stream only fresh log records' ) ) -> None : \"\"\" Attach local output stream to a running worker service. Parameters ---------- live : bool, Default=False Stream only fresh log records. \"\"\" from trecover.config import log from trecover.utils.cli import stream with log . project_console . screen (): for record in stream (( 'worker' , log . WORKER_LOG ), live = live ): log . project_console . print ( record ) log . project_console . clear ()","title":"Worker"},{"location":"src/trecover/app/cli/worker/#src.trecover.app.cli.worker.worker_state_verification","text":"worker_state_verification ( ctx ) Perform cli commands verification (state checking). Parameters: Name Type Description Default ctx Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. required Source code in src/trecover/app/cli/worker.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 @cli . callback ( invoke_without_command = True ) def worker_state_verification ( ctx : Context ) -> None : \"\"\" Perform cli commands verification (state checking). Parameters ---------- ctx : Context Typer (Click like) special internal object that holds state relevant for the script execution at every single level. \"\"\" from trecover.config import log if var . WORKER_PID . exists (): if ctx . invoked_subcommand in ( 'start' , None ): log . project_console . print ( ':rocket: The worker service is already started' , style = 'bright_blue' ) ctx . exit ( 0 ) elif ctx . invoked_subcommand is None : worker_start ( name = 'TRecoverWorker' , pool = var . PoolType . solo , loglevel = var . LogLevel . info , concurrency = var . CELERY_WORKERS , broker_url = var . CELERY_BROKER , backend_url = var . CELERY_BACKEND , attach = False , no_daemon = False ) elif ctx . invoked_subcommand != 'start' : log . project_console . print ( 'The worker service is not started' , style = 'yellow' ) ctx . exit ( 1 )","title":"worker_state_verification()"},{"location":"src/trecover/app/cli/worker/#src.trecover.app.cli.worker.worker_start","text":"worker_start ( name = Option ( \"TRecoverWorker\" , \"--name\" , \"-n\" , help = \"Set custom worker hostname.\" , ), pool = Option ( var . PoolType . solo , \"--pool\" , \"-p\" , help = \"Worker processes/threads pool type.\" , ), loglevel = Option ( var . LogLevel . info , \"--loglevel\" , \"-l\" , help = \"Logging level.\" , ), concurrency = Option ( var . CELERY_WORKERS , \"-c\" , help = \"The number of worker processes.\" , ), broker_url = Option ( var . CELERY_BROKER , \"--broker\" , help = \"Broker url.\" ), backend_url = Option ( var . CELERY_BACKEND , \"--backend\" , help = \"Backend url.\" ), attach = Option ( False , \"--attach\" , \"-a\" , is_flag = True , help = \"Attach output and error streams\" , ), no_daemon = Option ( False , \"--no-daemon\" , is_flag = True , help = \"Do not run as a daemon process\" , ), ) Start API service. Parameters: Name Type Description Default name str , default Custom worker hostname. Option('TRecoverWorker', '--name', '-n', help='Set custom worker hostname.') pool str Worker processes/threads pool type. Option(var.PoolType.solo, '--pool', '-p', help='Worker processes/threads pool type.') loglevel var . LogLevel Level of logging. 'debug' concurrency int , default The number of worker processes. Option(var.CELERY_WORKERS, '-c', help='The number of worker processes.') broker_url str , default Broker url. Option(var.CELERY_BROKER, '--broker', help='Broker url.') backend_url str , default Backend url. Option(var.CELERY_BACKEND, '--backend', help='Backend url.') attach bool , default Attach output and error streams. Option(False, '--attach', '-a', is_flag=True, help='Attach output and error streams') no_daemon bool , default Do not run as a daemon process. Option(False, '--no-daemon', is_flag=True, help='Do not run as a daemon process') Raises: Type Description typer.BadParameter: If non-solo pool type is selected for windows platform. Source code in src/trecover/app/cli/worker.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 @cli . command ( name = 'start' , help = 'Start service' ) def worker_start ( name : str = Option ( 'TRecoverWorker' , '--name' , '-n' , help = 'Set custom worker hostname.' ), pool : var . PoolType = Option ( var . PoolType . solo , '--pool' , '-p' , help = 'Worker processes/threads pool type.' ), loglevel : var . LogLevel = Option ( var . LogLevel . info , '--loglevel' , '-l' , help = 'Logging level.' ), concurrency : int = Option ( var . CELERY_WORKERS , '-c' , help = 'The number of worker processes.' ), broker_url : str = Option ( var . CELERY_BROKER , '--broker' , help = 'Broker url.' ), backend_url : str = Option ( var . CELERY_BACKEND , '--backend' , help = 'Backend url.' ), attach : bool = Option ( False , '--attach' , '-a' , is_flag = True , help = 'Attach output and error streams' ), no_daemon : bool = Option ( False , '--no-daemon' , is_flag = True , help = 'Do not run as a daemon process' ) ) -> None : \"\"\" Start API service. Parameters ---------- name : str, default='TRecoverWorker' Custom worker hostname. pool : str, {'prefork', 'eventlet', 'gevent', 'processes', 'solo'}, default='solo' Worker processes/threads pool type. loglevel : {'debug', 'info', 'warning', 'error', 'critical'}, default='info' Level of logging. concurrency : int, default=ENV(CELERY_WORKERS) or 1 The number of worker processes. broker_url : str, default=ENV(CELERY_BROKER) or 'pyamqp://guest@localhost' Broker url. backend_url : str, default=ENV(CELERY_BACKEND) or 'redis://localhost' Backend url. attach : bool, default=False Attach output and error streams. no_daemon : bool, default=False Do not run as a daemon process. Raises ------ typer.BadParameter: If non-solo pool type is selected for windows platform. \"\"\" import platform from subprocess import run from trecover.config import log from trecover.utils.cli import start_service if platform . system () == 'Windows' and pool != var . PoolType . solo : raise BadParameter ( \"Windows platform only supports 'solo' pool\" ) argv = [ 'celery' , '--broker' , broker_url , '--result-backend' , backend_url , '--app' , 'trecover.app.api.backend.celeryapp' , 'worker' , '--hostname' , name , '--concurrency' , str ( concurrency ), '--pool' , pool , '--loglevel' , loglevel ] if no_daemon : run ( argv ) else : start_service ( argv , name = 'worker' , logfile = log . WORKER_LOG , pidfile = var . WORKER_PID ) if attach : worker_attach ( live = False )","title":"worker_start()"},{"location":"src/trecover/app/cli/worker/#src.trecover.app.cli.worker.worker_stop","text":"worker_stop () Stop worker service. Source code in src/trecover/app/cli/worker.py 107 108 109 110 111 112 113 114 @cli . command ( name = 'stop' , help = 'Stop service' ) def worker_stop () -> None : \"\"\" Stop worker service. \"\"\" from trecover.config import log from trecover.utils.cli import stop_service stop_service ( name = 'worker' , pidfile = var . WORKER_PID , logfile = log . WORKER_LOG )","title":"worker_stop()"},{"location":"src/trecover/app/cli/worker/#src.trecover.app.cli.worker.worker_status","text":"worker_status () Display worker service status. Source code in src/trecover/app/cli/worker.py 117 118 119 120 121 122 123 @cli . command ( name = 'status' , help = 'Display service status' ) def worker_status () -> None : \"\"\" Display worker service status. \"\"\" from trecover.utils.cli import check_service check_service ( name = 'worker' , pidfile = var . WORKER_PID )","title":"worker_status()"},{"location":"src/trecover/app/cli/worker/#src.trecover.app.cli.worker.worker_attach","text":"worker_attach ( live = Option ( False , \"--live\" , \"-l\" , is_flag = True , help = \"Stream only fresh log records\" , ) ) Attach local output stream to a running worker service. Parameters: Name Type Description Default live bool , Default Stream only fresh log records. Option(False, '--live', '-l', is_flag=True, help='Stream only fresh log records') Source code in src/trecover/app/cli/worker.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 @cli . command ( name = 'attach' , help = 'Attach local output stream to a service' ) def worker_attach ( live : bool = Option ( False , '--live' , '-l' , is_flag = True , help = 'Stream only fresh log records' ) ) -> None : \"\"\" Attach local output stream to a running worker service. Parameters ---------- live : bool, Default=False Stream only fresh log records. \"\"\" from trecover.config import log from trecover.utils.cli import stream with log . project_console . screen (): for record in stream (( 'worker' , log . WORKER_LOG ), live = live ): log . project_console . print ( record ) log . project_console . clear ()","title":"worker_attach()"},{"location":"src/trecover/utils/beam_search/","text":"get_steps_params get_steps_params ( src ) Get probability masks and beam widths required for each step of beam search algorithm. Parameters: Name Type Description Default src Tensor [ SEQUENCE_LEN , TOKEN_SIZE ] Keyless reading columns that are passed to the TRecover encoder. required Returns: Name Type Description steps_mask Tensor [ SEQUENCE_LEN , TOKEN_SIZE ] Probability masks that consists of zeros in the places that correspond to the letters allowed for selection in the column (src[i]) and values equal to minus infinity in all others. steps_width List [ int ] The beam width for each step of the algorithm. Source code in src/trecover/utils/beam_search.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def get_steps_params ( src : Tensor ) -> Tuple [ Tensor , List [ int ]]: \"\"\" Get probability masks and beam widths required for each step of beam search algorithm. Parameters ---------- src : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Keyless reading columns that are passed to the TRecover encoder. Returns ------- steps_mask : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Probability masks that consists of zeros in the places that correspond to the letters allowed for selection in the column (src[i]) and values equal to minus infinity in all others. steps_width : List[int] The beam width for each step of the algorithm. \"\"\" return ( torch . full_like ( src , fill_value = float ( '-inf' )) . masked_fill ( src == 1 , value = 0.0 ), src . sum ( dim =- 1 ) . int () . tolist () ) beam_step beam_step ( candidates , step_mask , step_width , encoded_src , model , beam_width , device , ) Implementation of the beam search algorithm step. Parameters: Name Type Description Default candidates List [ Tuple [ Tensor [1, STEP_NUMBER , TOKEN_SIZE ], float ]] List of candidates from the previous step. required step_mask Tensor [ TOKEN_SIZE ] Column's mask that consists of zeros in the places that correspond to the letters allowed for selection in the column and values equal to minus infinity in all others. Required so that only the letters in the column are selected as a candidates. required step_width int Number of candidates that are contained in the step column. required encoded_src Tensor [ SEQUENCE_LEN , 1, D_MODEL ] Columns for keyless reading that were encoded by TRecover encoder. required model TRecover Trained model for keyless reading. required beam_width int Number of candidates that can be selected at the current step. required device torch . device Device on which to allocate the candidate chains. required Returns: Name Type Description step_candidates List[Tuple[Tensor[1, STEP_NUMBER List of candidates of size \"beam_width\" for the current step sorted in descending order of their probabilities. Notes For each chain candidate from the previous step: * Probability distribution is calculated using trained TRecover model to select the next symbol from the current column,taking into account the \"step_mask\". * The most probable symbols are selected from the calculated probability distribution, the number of which is set by the \"step_width\" and \"beam_width\" parameters. * For each selected symbol, a new candidate chain with updated probability is constructed and placed in the \"step_candidates\" list. All candidates are sorted in descending order of probabilities and the most probable ones are selected from them, the number of which is set by the \"beam_width\" parameter. Source code in src/trecover/utils/beam_search.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def beam_step ( candidates : List [ Tuple [ Tensor , float ]], step_mask : Tensor , step_width : int , encoded_src : Tensor , model : TRecover , beam_width : int , device : torch . device ) -> List [ Tuple [ Tensor , float ]]: \"\"\" Implementation of the beam search algorithm step. Parameters ---------- candidates : List[Tuple[Tensor[1, STEP_NUMBER, TOKEN_SIZE], float]] List of candidates from the previous step. step_mask : Tensor[TOKEN_SIZE] Column's mask that consists of zeros in the places that correspond to the letters allowed for selection in the column and values equal to minus infinity in all others. Required so that only the letters in the column are selected as a candidates. step_width : int Number of candidates that are contained in the step column. encoded_src : Tensor[SEQUENCE_LEN, 1, D_MODEL] Columns for keyless reading that were encoded by TRecover encoder. model : TRecover Trained model for keyless reading. beam_width : int Number of candidates that can be selected at the current step. device : torch.device Device on which to allocate the candidate chains. Returns ------- step_candidates : List[Tuple[Tensor[1, STEP_NUMBER + 1, TOKEN_SIZE], float]] List of candidates of size \"beam_width\" for the current step sorted in descending order of their probabilities. Notes ----- For each chain candidate from the previous step: * Probability distribution is calculated using trained TRecover model to select the next symbol from the current column,taking into account the \"step_mask\". * The most probable symbols are selected from the calculated probability distribution, the number of which is set by the \"step_width\" and \"beam_width\" parameters. * For each selected symbol, a new candidate chain with updated probability is constructed and placed in the \"step_candidates\" list. All candidates are sorted in descending order of probabilities and the most probable ones are selected from them, the number of which is set by the \"beam_width\" parameter. \"\"\" step_candidates = list () for chain , score in candidates : prediction = model . predict ( chain , encoded_src , tgt_attn_mask = None , tgt_pad_mask = None , src_pad_mask = None ) probabilities = F . log_softmax ( prediction [ 0 , - 1 ], dim =- 1 ) + step_mask values , indices = probabilities . topk ( k = min ( beam_width , step_width )) for prob , pos in zip ( values , indices ): new_token = torch . zeros ( 1 , 1 , model . token_size , device = device ) new_token [ 0 , 0 , pos ] = 1 step_candidates . append (( torch . cat ([ chain , new_token ], dim = 1 ), score + float ( prob ))) return sorted ( step_candidates , key = lambda candidate : - candidate [ 1 ])[: beam_width ] celery_task_loop celery_task_loop ( task ) Get a beam search algorithm loop function, which is implemented for the Celery task. Parameters: Name Type Description Default task celery . Task Celery task base class. required Returns: Name Type Description inner_loop Callable [[ Tensor , Tensor , TRecover , int , torch . device ], List [ Tuple [ Tensor , float ]]] Beam search algorithm loop function for the Celery task. Source code in src/trecover/utils/beam_search.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def celery_task_loop ( task : celery . Task ) -> Callable [[ Tensor , Tensor , TRecover , int , torch . device ], List [ Tuple [ Tensor , float ]]]: \"\"\" Get a beam search algorithm loop function, which is implemented for the Celery task. Parameters ---------- task : celery.Task Celery task base class. Returns ------- inner_loop : Callable[[Tensor, Tensor, TRecover, int, torch.device], List[Tuple[Tensor, float]]] Beam search algorithm loop function for the Celery task. \"\"\" def inner_loop ( src : Tensor , encoded_src : Tensor , model : TRecover , width : int , device : torch . device ) -> List [ Tuple [ Tensor , float ]]: \"\"\" Beam search algorithm loop implementation for the Celery task. Parameters ---------- src : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Keyless reading columns that are passed to the TRecover encoder. encoded_src : Tensor[SEQUENCE_LEN, 1, D_MODEL] Keyless reading columns that were encoded by TRecover encoder. model : TRecover Trained model for keyless reading. width : int Number of candidates that can be selected at each step. device : torch.device Device on which to allocate the candidate chains. Returns ------- candidates : List[Tuple[Tensor[1, SEQUENCE_LEN + 1, TOKEN_SIZE], float]] List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Notes ----- Probability masks and beam widths values required for each step of the beam search algorithm are calculated by the \"get_steps_params\" function using keyless reading columns (\"src\") that are passed to the TRecover encoder. An empty chain (zero token of shape [1, 1, TOKEN_SIZE]) with zero probability is used as the first candidate for the algorithm. The progress of the Celery task is updated at each step of the algorithm. \"\"\" step_masks , step_widths = get_steps_params ( src ) candidates = [( torch . zeros ( 1 , 1 , model . token_size , device = device ), 0 )] for i in range ( encoded_src . shape [ 0 ]): candidates = beam_step ( candidates , step_masks [ i ], step_widths [ i ], encoded_src , model , width , device ) task . update_state ( meta = { 'progress' : i + 1 }, state = 'PREDICT' ) return candidates return inner_loop cli_interactive_loop cli_interactive_loop ( label = 'Processing' ) Get a beam search algorithm loop function, which is implemented for the cli interface. Parameters: Name Type Description Default label str Label for the cli progress bar. 'Processing' Returns: Name Type Description inner_loop Callable [[ Tensor , Tensor , TRecover , int , torch . device ], List [ Tuple [ Tensor , float ]]] Beam search algorithm loop function for the cli interface. Source code in src/trecover/utils/beam_search.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def cli_interactive_loop ( label : str = 'Processing' ) -> Callable [[ Tensor , Tensor , TRecover , int , torch . device ], List [ Tuple [ Tensor , float ]]]: \"\"\" Get a beam search algorithm loop function, which is implemented for the cli interface. Parameters ---------- label : str Label for the cli progress bar. Returns ------- inner_loop : Callable[[Tensor, Tensor, TRecover, int, torch.device], List[Tuple[Tensor, float]]] Beam search algorithm loop function for the cli interface. \"\"\" def inner_loop ( src : Tensor , encoded_src : Tensor , model : TRecover , width : int , device : torch . device ) -> List [ Tuple [ Tensor , float ]]: \"\"\" Beam search algorithm loop implementation for the cli interface. Parameters ---------- src : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Keyless reading columns that are passed to the TRecover encoder. encoded_src : Tensor[SEQUENCE_LEN, 1, D_MODEL] Keyless reading columns that were encoded by TRecover encoder. model : TRecover Trained model for keyless reading. width : int Number of candidates that can be selected at each step. device : torch.device Device on which to allocate the candidate chains. Returns ------- candidates : List[Tuple[Tensor[1, SEQUENCE_LEN + 1, TOKEN_SIZE], float]] List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Notes ----- Probability masks and beam widths values required for each step of the beam search algorithm are calculated by the \"get_steps_params\" function using keyless reading columns (\"src\") that are passed to the TRecover encoder. An empty chain (zero token of shape [1, 1, TOKEN_SIZE]) with zero probability is used as the first candidate for the algorithm. At each step of the algorithm, the progress bar of the task is updated and displayed in the console. \"\"\" step_masks , step_widths = get_steps_params ( src ) candidates = [( torch . zeros ( 1 , 1 , model . token_size , device = device ), 0 )] with Progress ( TextColumn ( ' {task.description} ' , style = 'bright_blue' ), BarColumn ( complete_style = 'bright_blue' ), TextColumn ( ' {task.percentage:>3.0f} %' , style = 'bright_blue' ), TextColumn ( 'Remaining' , style = 'bright_blue' ), TimeRemainingColumn (), TextColumn ( 'Elapsed' , style = 'bright_blue' ), TimeElapsedColumn (), transient = True , console = log . project_console ) as progress : beam_progress = progress . add_task ( label , total = encoded_src . shape [ 0 ]) for i in range ( encoded_src . shape [ 0 ]): candidates = beam_step ( candidates , step_masks [ i ], step_widths [ i ], encoded_src , model , width , device ) progress . update ( beam_progress , advance = 1 ) return candidates return inner_loop dashboard_loop dashboard_loop ( src , encoded_src , model , width , device ) Beam search algorithm loop implementation for the dashboard interface. Parameters: Name Type Description Default src Tensor [ SEQUENCE_LEN , TOKEN_SIZE ] Keyless reading columns that are passed to the TRecover encoder. required encoded_src Tensor [ SEQUENCE_LEN , 1, D_MODEL ] Keyless reading columns that were encoded by TRecover encoder. required model TRecover Trained model for keyless reading. required width int Number of candidates that can be selected at each step. required device torch . device Device on which to allocate the candidate chains. required Returns: Name Type Description candidates List[Tuple[Tensor[1, SEQUENCE_LEN List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Notes Probability masks and beam widths values required for each step of the beam search algorithm are calculated by the \"get_steps_params\" function using keyless reading columns (\"src\") that are passed to the TRecover encoder. An empty chain (zero token of shape [1, 1, TOKEN_SIZE]) with zero probability is used as the first candidate for the algorithm. At each step of the algorithm, the progress bar is updated and displayed on the dashboard. Source code in src/trecover/utils/beam_search.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 def dashboard_loop ( src : Tensor , encoded_src : Tensor , model : TRecover , width : int , device : torch . device ) -> List [ Tuple [ Tensor , float ]]: \"\"\" Beam search algorithm loop implementation for the dashboard interface. Parameters ---------- src : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Keyless reading columns that are passed to the TRecover encoder. encoded_src : Tensor[SEQUENCE_LEN, 1, D_MODEL] Keyless reading columns that were encoded by TRecover encoder. model : TRecover Trained model for keyless reading. width : int Number of candidates that can be selected at each step. device : torch.device Device on which to allocate the candidate chains. Returns ------- candidates : List[Tuple[Tensor[1, SEQUENCE_LEN + 1, TOKEN_SIZE], float]] List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Notes ----- Probability masks and beam widths values required for each step of the beam search algorithm are calculated by the \"get_steps_params\" function using keyless reading columns (\"src\") that are passed to the TRecover encoder. An empty chain (zero token of shape [1, 1, TOKEN_SIZE]) with zero probability is used as the first candidate for the algorithm. At each step of the algorithm, the progress bar is updated and displayed on the dashboard. \"\"\" import streamlit as st step_masks , step_widths = get_steps_params ( src ) candidates = [( torch . zeros ( 1 , 1 , model . token_size , device = device ), 0 )] progress = st . progress ( 0 ) for i in range ( encoded_src . shape [ 0 ]): candidates = beam_step ( candidates , step_masks [ i ], step_widths [ i ], encoded_src , model , width , device ) progress . progress ( i / encoded_src . shape [ 0 ]) return candidates standard_loop standard_loop ( src , encoded_src , model , width , device ) Base implementation of the beam search algorithm loop. Parameters: Name Type Description Default src Tensor [ SEQUENCE_LEN , TOKEN_SIZE ] Keyless reading columns that are passed to the TRecover encoder. required encoded_src Tensor [ SEQUENCE_LEN , 1, D_MODEL ] Keyless reading columns that were encoded by TRecover encoder. required model TRecover Trained model for keyless reading. required width int Number of candidates that can be selected at each step. required device torch . device Device on which to allocate the candidate chains. required Returns: Name Type Description candidates List[Tuple[Tensor[1, SEQUENCE_LEN List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Notes Probability masks and beam widths values required for each step of the beam search algorithm are calculated by the \"get_steps_params\" function using keyless reading columns (\"src\") that are passed to the TRecover encoder. An empty chain (zero token of shape [1, 1, TOKEN_SIZE]) with zero probability is used as the first candidate for the algorithm. Source code in src/trecover/utils/beam_search.py 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def standard_loop ( src : Tensor , encoded_src : Tensor , model : TRecover , width : int , device : torch . device ) -> List [ Tuple [ Tensor , float ]]: \"\"\" Base implementation of the beam search algorithm loop. Parameters ---------- src : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Keyless reading columns that are passed to the TRecover encoder. encoded_src : Tensor[SEQUENCE_LEN, 1, D_MODEL] Keyless reading columns that were encoded by TRecover encoder. model : TRecover Trained model for keyless reading. width : int Number of candidates that can be selected at each step. device : torch.device Device on which to allocate the candidate chains. Returns ------- candidates : List[Tuple[Tensor[1, SEQUENCE_LEN + 1, TOKEN_SIZE], float]] List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Notes ----- Probability masks and beam widths values required for each step of the beam search algorithm are calculated by the \"get_steps_params\" function using keyless reading columns (\"src\") that are passed to the TRecover encoder. An empty chain (zero token of shape [1, 1, TOKEN_SIZE]) with zero probability is used as the first candidate for the algorithm. \"\"\" step_masks , step_widths = get_steps_params ( src ) candidates = [( torch . zeros ( 1 , 1 , model . token_size , device = device ), 0 )] for i in range ( encoded_src . shape [ 0 ]): candidates = beam_step ( candidates , step_masks [ i ], step_widths [ i ], encoded_src , model , width , device ) return candidates beam_search beam_search ( src , model , width , device , beam_loop = standard_loop ) Beam search algorithm implementation. Parameters: Name Type Description Default src Tensor [ SEQUENCE_LEN , TOKEN_SIZE ] Keyless reading columns that are passed to the TRecover encoder. required model TRecover Trained model for keyless reading. required width int Number of candidates that can be selected at each step. required device torch . device Device on which to allocate the candidate chains. required beam_loop Callable[[Tensor, Tensor, TRecover, int, torch.device], List[Tuple[Tensor, float]]], default=standard_loop Beam search algorithm loop function. standard_loop Returns: Name Type Description candidates List [ Tuple [ Tensor [ SEQUENCE_LEN ], float ]] List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Notes Initially, the keyless reading columns (\"src\") are encoded using TRecover encoder, then the encoded columns (\"encoded_src\") are used at each step of the algorithm. Source code in src/trecover/utils/beam_search.py 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 def beam_search ( src : Tensor , model : TRecover , width : int , device : torch . device , beam_loop : Callable [[ Tensor , Tensor , TRecover , int , torch . device ], List [ Tuple [ Tensor , float ]]] = standard_loop ) -> List [ Tuple [ Tensor , float ]]: \"\"\" Beam search algorithm implementation. Parameters ---------- src : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Keyless reading columns that are passed to the TRecover encoder. model : TRecover Trained model for keyless reading. width : int Number of candidates that can be selected at each step. device : torch.device Device on which to allocate the candidate chains. beam_loop : Callable[[Tensor, Tensor, TRecover, int, torch.device], List[Tuple[Tensor, float]]], default=standard_loop Beam search algorithm loop function. Returns ------- candidates : List[Tuple[Tensor[SEQUENCE_LEN], float]] List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Notes ----- Initially, the keyless reading columns (\"src\") are encoded using TRecover encoder, then the encoded columns (\"encoded_src\") are used at each step of the algorithm. \"\"\" encoded_src = model . encode ( src . unsqueeze ( dim = 0 ), src_pad_mask = None ) candidates = beam_loop ( src , encoded_src , model , width , device ) return [ ( torch . argmax ( chain . squeeze (), dim =- 1 )[ 1 :], score ) # first token is empty_token for chain , score in candidates ] async_beam_step async async_beam_step ( candidates , step_mask , step_width , encoded_src , model , beam_width , device , ) Asynchronous implementation of the beam search algorithm step. Parameters: Name Type Description Default candidates List [ Tuple [ Tensor [1, STEP_NUMBER , TOKEN_SIZE ], float ]] List of candidates from the previous step. required step_mask Tensor [ TOKEN_SIZE ] Column's mask that consists of zeros in the places that correspond to the letters allowed for selection in the column and values equal to minus infinity in all others. Required so that only the letters in the column are selected as a candidates. required step_width int Number of candidates that are contained in the step column. required encoded_src Tensor [ SEQUENCE_LEN , 1, D_MODEL ] Columns for keyless reading that were encoded by TRecover encoder. required model TRecover Trained model for keyless reading. required beam_width int Number of candidates that can be selected at the current step. required device torch . device Device on which to allocate the candidate chains. required Returns: Name Type Description step_candidates List[Tuple[Tensor[1, STEP_NUMBER List of candidates of size \"beam_width\" for the current step sorted in descending order of their probabilities. Notes For each chain candidate from the previous step: * Probability distribution is calculated using trained TRecover model to select the next symbol from the current column,taking into account the \"step_mask\". * The most probable symbols are selected from the calculated probability distribution, the number of which is set by the \"step_width\" and \"beam_width\" parameters. * For each selected symbol, a new candidate chain with updated probability is constructed and placed in the \"step_candidates\" list. All candidates are sorted in descending order of probabilities and the most probable ones are selected from them, the number of which is set by the \"beam_width\" parameter. Source code in src/trecover/utils/beam_search.py 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 async def async_beam_step ( candidates : List [ Tuple [ Tensor , float ]], step_mask : Tensor , step_width : int , encoded_src : Tensor , model : TRecover , beam_width : int , device : torch . device ) -> List [ Tuple [ Tensor , float ]]: \"\"\" Asynchronous implementation of the beam search algorithm step. Parameters ---------- candidates : List[Tuple[Tensor[1, STEP_NUMBER, TOKEN_SIZE], float]] List of candidates from the previous step. step_mask : Tensor[TOKEN_SIZE] Column's mask that consists of zeros in the places that correspond to the letters allowed for selection in the column and values equal to minus infinity in all others. Required so that only the letters in the column are selected as a candidates. step_width : int Number of candidates that are contained in the step column. encoded_src : Tensor[SEQUENCE_LEN, 1, D_MODEL] Columns for keyless reading that were encoded by TRecover encoder. model : TRecover Trained model for keyless reading. beam_width : int Number of candidates that can be selected at the current step. device : torch.device Device on which to allocate the candidate chains. Returns ------- step_candidates : List[Tuple[Tensor[1, STEP_NUMBER + 1, TOKEN_SIZE], float]] List of candidates of size \"beam_width\" for the current step sorted in descending order of their probabilities. Notes ----- For each chain candidate from the previous step: * Probability distribution is calculated using trained TRecover model to select the next symbol from the current column,taking into account the \"step_mask\". * The most probable symbols are selected from the calculated probability distribution, the number of which is set by the \"step_width\" and \"beam_width\" parameters. * For each selected symbol, a new candidate chain with updated probability is constructed and placed in the \"step_candidates\" list. All candidates are sorted in descending order of probabilities and the most probable ones are selected from them, the number of which is set by the \"beam_width\" parameter. \"\"\" async def candidate_step ( chain : Tensor , score : float ) -> None : prediction = model . predict ( chain , encoded_src , tgt_attn_mask = None , tgt_pad_mask = None , src_pad_mask = None ) probabilities = F . log_softmax ( prediction [ 0 , - 1 ], dim =- 1 ) + step_mask values , indices = probabilities . topk ( k = min ( beam_width , step_width )) for prob , pos in zip ( values , indices ): new_token = torch . zeros ( 1 , 1 , model . token_size , device = device ) new_token [ 0 , 0 , pos ] = 1 step_candidates . append (( torch . cat ([ chain , new_token ], dim = 1 ), score + float ( prob ))) step_candidates = list () for candidate_chain , candidate_score in candidates : await candidate_step ( candidate_chain , candidate_score ) return sorted ( step_candidates , key = lambda candidate : - candidate [ 1 ])[: beam_width ] api_interactive_loop api_interactive_loop ( queue , delimiter = '' ) Get an asynchronous beam search algorithm loop function, which is implemented for the API interface. Parameters: Name Type Description Default queue asyncio . Queue Asynchronous queue for storing intermediate results. required delimiter str Delimiter for columns visualization. '' Returns: Name Type Description async_inner_loop Callable [[ Tensor , Tensor , TRecover , int , torch . device ], Awaitable ] Asynchronous beam search algorithm loop function for the API interface. Source code in src/trecover/utils/beam_search.py 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 def api_interactive_loop ( queue : asyncio . Queue , delimiter : str = '' ) -> Callable [[ Tensor , Tensor , TRecover , int , torch . device ], Awaitable ]: \"\"\" Get an asynchronous beam search algorithm loop function, which is implemented for the API interface. Parameters ---------- queue : asyncio.Queue Asynchronous queue for storing intermediate results. delimiter: str, default='' Delimiter for columns visualization. Returns ------- async_inner_loop : Callable[[Tensor, Tensor, TRecover, int, torch.device], Awaitable] Asynchronous beam search algorithm loop function for the API interface. \"\"\" async def async_inner_loop ( src : Tensor , encoded_src : Tensor , model : TRecover , width : int , device : torch . device ) -> None : \"\"\" Asynchronous beam search algorithm loop implementation for the API interface. Parameters ---------- src : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Keyless reading columns that are passed to the TRecover encoder. encoded_src : Tensor[SEQUENCE_LEN, 1, D_MODEL] Keyless reading columns that were encoded by TRecover encoder. model : TRecover Trained model for keyless reading. width : int Number of candidates that can be selected at each step. device : torch.device Device on which to allocate the candidate chains. Notes ----- Probability masks and beam widths values required for each step of the beam search algorithm are calculated by the \"get_steps_params\" function using keyless reading columns (\"src\") that are passed to the TRecover encoder. An empty chain (zero token of shape [1, 1, TOKEN_SIZE]) with zero probability is used as the first candidate for the algorithm. At each step of the algorithm, the intermediate results are placed in an asynchronous queue. At the end of the algorithm, a None value is placed in the asynchronous queue, which is an indicator of its completion. \"\"\" step_masks , step_widths = get_steps_params ( src ) candidates = [( torch . zeros ( 1 , 1 , model . token_size , device = device ), 0 )] for i in range ( encoded_src . shape [ 0 ]): candidates = await async_beam_step ( candidates , step_masks [ i ], step_widths [ i ], encoded_src , model , width , device ) intermediate_result = [ ( tensor_to_target ( torch . argmax ( chain . squeeze (), dim =- 1 )[ 1 :]), score ) for chain , score in candidates ] await queue . put ([( visualize_target ( chain , delimiter ), score ) for chain , score in intermediate_result ]) await queue . put ( None ) return async_inner_loop standard_async_loop async standard_async_loop ( src , encoded_src , model , width , device ) Base asynchronous implementation of the beam search algorithm loop. Parameters: Name Type Description Default src Tensor [ SEQUENCE_LEN , TOKEN_SIZE ] Keyless reading columns that are passed to the TRecover encoder. required encoded_src Tensor [ SEQUENCE_LEN , 1, D_MODEL ] Keyless reading columns that were encoded by TRecover encoder. required model TRecover Trained model for keyless reading. required width int Number of candidates that can be selected at each step. required device torch . device Device on which to allocate the candidate chains. required Returns: Name Type Description candidates List[Tuple[Tensor[1, SEQUENCE_LEN List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Notes Probability masks and beam widths values required for each step of the beam search algorithm are calculated by the \"get_steps_params\" function using keyless reading columns (\"src\") that are passed to the TRecover encoder. An empty chain (zero token of shape [1, 1, TOKEN_SIZE]) with zero probability is used as the first candidate for the algorithm. Source code in src/trecover/utils/beam_search.py 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 async def standard_async_loop ( src : Tensor , encoded_src : Tensor , model : TRecover , width : int , device : torch . device ) -> List [ Tuple [ Tensor , float ]]: \"\"\" Base asynchronous implementation of the beam search algorithm loop. Parameters ---------- src : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Keyless reading columns that are passed to the TRecover encoder. encoded_src : Tensor[SEQUENCE_LEN, 1, D_MODEL] Keyless reading columns that were encoded by TRecover encoder. model : TRecover Trained model for keyless reading. width : int Number of candidates that can be selected at each step. device : torch.device Device on which to allocate the candidate chains. Returns ------- candidates : List[Tuple[Tensor[1, SEQUENCE_LEN + 1, TOKEN_SIZE], float]] List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Notes ----- Probability masks and beam widths values required for each step of the beam search algorithm are calculated by the \"get_steps_params\" function using keyless reading columns (\"src\") that are passed to the TRecover encoder. An empty chain (zero token of shape [1, 1, TOKEN_SIZE]) with zero probability is used as the first candidate for the algorithm. \"\"\" step_masks , step_widths = get_steps_params ( src ) candidates = [( torch . zeros ( 1 , 1 , model . token_size , device = device ), 0 )] for i in range ( encoded_src . shape [ 0 ]): candidates = await async_beam_step ( candidates , step_masks [ i ], step_widths [ i ], encoded_src , model , width , device ) return candidates async_beam_search async async_beam_search ( src , model , width , device , beam_loop = standard_async_loop ) Asynchronous beam search algorithm implementation. Parameters: Name Type Description Default src Tensor [ SEQUENCE_LEN , TOKEN_SIZE ] Keyless reading columns that are passed to the TRecover encoder. required model TRecover Trained model for keyless reading. required width int Number of candidates that can be selected at each step. required device torch . device Device on which to allocate the candidate chains. required beam_loop Callable[[Tensor, Tensor, TRecover, int, torch.device], List[Tuple[Tensor, float]]], default=standard_loop Beam search algorithm loop function. standard_async_loop Returns: Name Type Description candidates Optional [ List [ Tuple [ Tensor [ SEQUENCE_LEN ], float ]]] List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Returns None if \"api_interactive_loop\" is used as a beam search loop function. Notes Initially, the keyless reading columns (\"src\") are encoded using TRecover encoder, then the encoded columns (\"encoded_src\") are used at each step of the asynchronous algorithm. Source code in src/trecover/utils/beam_search.py 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 async def async_beam_search ( src : Tensor , model : TRecover , width : int , device : torch . device , beam_loop : Callable [[ Tensor , Tensor , TRecover , int , torch . device ], Awaitable [ Optional [ List [ Tuple [ Tensor , float ]]]]] = standard_async_loop ) -> Optional [ List [ Tuple [ Tensor , float ]]]: \"\"\" Asynchronous beam search algorithm implementation. Parameters ---------- src : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Keyless reading columns that are passed to the TRecover encoder. model : TRecover Trained model for keyless reading. width : int Number of candidates that can be selected at each step. device : torch.device Device on which to allocate the candidate chains. beam_loop : Callable[[Tensor, Tensor, TRecover, int, torch.device], List[Tuple[Tensor, float]]], default=standard_loop Beam search algorithm loop function. Returns ------- candidates : Optional[List[Tuple[Tensor[SEQUENCE_LEN], float]]] List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Returns None if \"api_interactive_loop\" is used as a beam search loop function. Notes ----- Initially, the keyless reading columns (\"src\") are encoded using TRecover encoder, then the encoded columns (\"encoded_src\") are used at each step of the asynchronous algorithm. \"\"\" encoded_src = model . encode ( src . unsqueeze ( dim = 0 ), src_pad_mask = None ) candidates = await beam_loop ( src , encoded_src , model , width , device ) return [( torch . argmax ( chain . squeeze (), dim =- 1 )[ 1 :], score ) for chain , score in candidates ] if candidates else None","title":"Beam Search"},{"location":"src/trecover/utils/beam_search/#src.trecover.utils.beam_search.get_steps_params","text":"get_steps_params ( src ) Get probability masks and beam widths required for each step of beam search algorithm. Parameters: Name Type Description Default src Tensor [ SEQUENCE_LEN , TOKEN_SIZE ] Keyless reading columns that are passed to the TRecover encoder. required Returns: Name Type Description steps_mask Tensor [ SEQUENCE_LEN , TOKEN_SIZE ] Probability masks that consists of zeros in the places that correspond to the letters allowed for selection in the column (src[i]) and values equal to minus infinity in all others. steps_width List [ int ] The beam width for each step of the algorithm. Source code in src/trecover/utils/beam_search.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def get_steps_params ( src : Tensor ) -> Tuple [ Tensor , List [ int ]]: \"\"\" Get probability masks and beam widths required for each step of beam search algorithm. Parameters ---------- src : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Keyless reading columns that are passed to the TRecover encoder. Returns ------- steps_mask : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Probability masks that consists of zeros in the places that correspond to the letters allowed for selection in the column (src[i]) and values equal to minus infinity in all others. steps_width : List[int] The beam width for each step of the algorithm. \"\"\" return ( torch . full_like ( src , fill_value = float ( '-inf' )) . masked_fill ( src == 1 , value = 0.0 ), src . sum ( dim =- 1 ) . int () . tolist () )","title":"get_steps_params()"},{"location":"src/trecover/utils/beam_search/#src.trecover.utils.beam_search.beam_step","text":"beam_step ( candidates , step_mask , step_width , encoded_src , model , beam_width , device , ) Implementation of the beam search algorithm step. Parameters: Name Type Description Default candidates List [ Tuple [ Tensor [1, STEP_NUMBER , TOKEN_SIZE ], float ]] List of candidates from the previous step. required step_mask Tensor [ TOKEN_SIZE ] Column's mask that consists of zeros in the places that correspond to the letters allowed for selection in the column and values equal to minus infinity in all others. Required so that only the letters in the column are selected as a candidates. required step_width int Number of candidates that are contained in the step column. required encoded_src Tensor [ SEQUENCE_LEN , 1, D_MODEL ] Columns for keyless reading that were encoded by TRecover encoder. required model TRecover Trained model for keyless reading. required beam_width int Number of candidates that can be selected at the current step. required device torch . device Device on which to allocate the candidate chains. required Returns: Name Type Description step_candidates List[Tuple[Tensor[1, STEP_NUMBER List of candidates of size \"beam_width\" for the current step sorted in descending order of their probabilities.","title":"beam_step()"},{"location":"src/trecover/utils/beam_search/#src.trecover.utils.beam_search.beam_step--notes","text":"For each chain candidate from the previous step: * Probability distribution is calculated using trained TRecover model to select the next symbol from the current column,taking into account the \"step_mask\". * The most probable symbols are selected from the calculated probability distribution, the number of which is set by the \"step_width\" and \"beam_width\" parameters. * For each selected symbol, a new candidate chain with updated probability is constructed and placed in the \"step_candidates\" list. All candidates are sorted in descending order of probabilities and the most probable ones are selected from them, the number of which is set by the \"beam_width\" parameter. Source code in src/trecover/utils/beam_search.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def beam_step ( candidates : List [ Tuple [ Tensor , float ]], step_mask : Tensor , step_width : int , encoded_src : Tensor , model : TRecover , beam_width : int , device : torch . device ) -> List [ Tuple [ Tensor , float ]]: \"\"\" Implementation of the beam search algorithm step. Parameters ---------- candidates : List[Tuple[Tensor[1, STEP_NUMBER, TOKEN_SIZE], float]] List of candidates from the previous step. step_mask : Tensor[TOKEN_SIZE] Column's mask that consists of zeros in the places that correspond to the letters allowed for selection in the column and values equal to minus infinity in all others. Required so that only the letters in the column are selected as a candidates. step_width : int Number of candidates that are contained in the step column. encoded_src : Tensor[SEQUENCE_LEN, 1, D_MODEL] Columns for keyless reading that were encoded by TRecover encoder. model : TRecover Trained model for keyless reading. beam_width : int Number of candidates that can be selected at the current step. device : torch.device Device on which to allocate the candidate chains. Returns ------- step_candidates : List[Tuple[Tensor[1, STEP_NUMBER + 1, TOKEN_SIZE], float]] List of candidates of size \"beam_width\" for the current step sorted in descending order of their probabilities. Notes ----- For each chain candidate from the previous step: * Probability distribution is calculated using trained TRecover model to select the next symbol from the current column,taking into account the \"step_mask\". * The most probable symbols are selected from the calculated probability distribution, the number of which is set by the \"step_width\" and \"beam_width\" parameters. * For each selected symbol, a new candidate chain with updated probability is constructed and placed in the \"step_candidates\" list. All candidates are sorted in descending order of probabilities and the most probable ones are selected from them, the number of which is set by the \"beam_width\" parameter. \"\"\" step_candidates = list () for chain , score in candidates : prediction = model . predict ( chain , encoded_src , tgt_attn_mask = None , tgt_pad_mask = None , src_pad_mask = None ) probabilities = F . log_softmax ( prediction [ 0 , - 1 ], dim =- 1 ) + step_mask values , indices = probabilities . topk ( k = min ( beam_width , step_width )) for prob , pos in zip ( values , indices ): new_token = torch . zeros ( 1 , 1 , model . token_size , device = device ) new_token [ 0 , 0 , pos ] = 1 step_candidates . append (( torch . cat ([ chain , new_token ], dim = 1 ), score + float ( prob ))) return sorted ( step_candidates , key = lambda candidate : - candidate [ 1 ])[: beam_width ]","title":"Notes"},{"location":"src/trecover/utils/beam_search/#src.trecover.utils.beam_search.celery_task_loop","text":"celery_task_loop ( task ) Get a beam search algorithm loop function, which is implemented for the Celery task. Parameters: Name Type Description Default task celery . Task Celery task base class. required Returns: Name Type Description inner_loop Callable [[ Tensor , Tensor , TRecover , int , torch . device ], List [ Tuple [ Tensor , float ]]] Beam search algorithm loop function for the Celery task. Source code in src/trecover/utils/beam_search.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def celery_task_loop ( task : celery . Task ) -> Callable [[ Tensor , Tensor , TRecover , int , torch . device ], List [ Tuple [ Tensor , float ]]]: \"\"\" Get a beam search algorithm loop function, which is implemented for the Celery task. Parameters ---------- task : celery.Task Celery task base class. Returns ------- inner_loop : Callable[[Tensor, Tensor, TRecover, int, torch.device], List[Tuple[Tensor, float]]] Beam search algorithm loop function for the Celery task. \"\"\" def inner_loop ( src : Tensor , encoded_src : Tensor , model : TRecover , width : int , device : torch . device ) -> List [ Tuple [ Tensor , float ]]: \"\"\" Beam search algorithm loop implementation for the Celery task. Parameters ---------- src : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Keyless reading columns that are passed to the TRecover encoder. encoded_src : Tensor[SEQUENCE_LEN, 1, D_MODEL] Keyless reading columns that were encoded by TRecover encoder. model : TRecover Trained model for keyless reading. width : int Number of candidates that can be selected at each step. device : torch.device Device on which to allocate the candidate chains. Returns ------- candidates : List[Tuple[Tensor[1, SEQUENCE_LEN + 1, TOKEN_SIZE], float]] List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Notes ----- Probability masks and beam widths values required for each step of the beam search algorithm are calculated by the \"get_steps_params\" function using keyless reading columns (\"src\") that are passed to the TRecover encoder. An empty chain (zero token of shape [1, 1, TOKEN_SIZE]) with zero probability is used as the first candidate for the algorithm. The progress of the Celery task is updated at each step of the algorithm. \"\"\" step_masks , step_widths = get_steps_params ( src ) candidates = [( torch . zeros ( 1 , 1 , model . token_size , device = device ), 0 )] for i in range ( encoded_src . shape [ 0 ]): candidates = beam_step ( candidates , step_masks [ i ], step_widths [ i ], encoded_src , model , width , device ) task . update_state ( meta = { 'progress' : i + 1 }, state = 'PREDICT' ) return candidates return inner_loop","title":"celery_task_loop()"},{"location":"src/trecover/utils/beam_search/#src.trecover.utils.beam_search.cli_interactive_loop","text":"cli_interactive_loop ( label = 'Processing' ) Get a beam search algorithm loop function, which is implemented for the cli interface. Parameters: Name Type Description Default label str Label for the cli progress bar. 'Processing' Returns: Name Type Description inner_loop Callable [[ Tensor , Tensor , TRecover , int , torch . device ], List [ Tuple [ Tensor , float ]]] Beam search algorithm loop function for the cli interface. Source code in src/trecover/utils/beam_search.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def cli_interactive_loop ( label : str = 'Processing' ) -> Callable [[ Tensor , Tensor , TRecover , int , torch . device ], List [ Tuple [ Tensor , float ]]]: \"\"\" Get a beam search algorithm loop function, which is implemented for the cli interface. Parameters ---------- label : str Label for the cli progress bar. Returns ------- inner_loop : Callable[[Tensor, Tensor, TRecover, int, torch.device], List[Tuple[Tensor, float]]] Beam search algorithm loop function for the cli interface. \"\"\" def inner_loop ( src : Tensor , encoded_src : Tensor , model : TRecover , width : int , device : torch . device ) -> List [ Tuple [ Tensor , float ]]: \"\"\" Beam search algorithm loop implementation for the cli interface. Parameters ---------- src : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Keyless reading columns that are passed to the TRecover encoder. encoded_src : Tensor[SEQUENCE_LEN, 1, D_MODEL] Keyless reading columns that were encoded by TRecover encoder. model : TRecover Trained model for keyless reading. width : int Number of candidates that can be selected at each step. device : torch.device Device on which to allocate the candidate chains. Returns ------- candidates : List[Tuple[Tensor[1, SEQUENCE_LEN + 1, TOKEN_SIZE], float]] List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Notes ----- Probability masks and beam widths values required for each step of the beam search algorithm are calculated by the \"get_steps_params\" function using keyless reading columns (\"src\") that are passed to the TRecover encoder. An empty chain (zero token of shape [1, 1, TOKEN_SIZE]) with zero probability is used as the first candidate for the algorithm. At each step of the algorithm, the progress bar of the task is updated and displayed in the console. \"\"\" step_masks , step_widths = get_steps_params ( src ) candidates = [( torch . zeros ( 1 , 1 , model . token_size , device = device ), 0 )] with Progress ( TextColumn ( ' {task.description} ' , style = 'bright_blue' ), BarColumn ( complete_style = 'bright_blue' ), TextColumn ( ' {task.percentage:>3.0f} %' , style = 'bright_blue' ), TextColumn ( 'Remaining' , style = 'bright_blue' ), TimeRemainingColumn (), TextColumn ( 'Elapsed' , style = 'bright_blue' ), TimeElapsedColumn (), transient = True , console = log . project_console ) as progress : beam_progress = progress . add_task ( label , total = encoded_src . shape [ 0 ]) for i in range ( encoded_src . shape [ 0 ]): candidates = beam_step ( candidates , step_masks [ i ], step_widths [ i ], encoded_src , model , width , device ) progress . update ( beam_progress , advance = 1 ) return candidates return inner_loop","title":"cli_interactive_loop()"},{"location":"src/trecover/utils/beam_search/#src.trecover.utils.beam_search.dashboard_loop","text":"dashboard_loop ( src , encoded_src , model , width , device ) Beam search algorithm loop implementation for the dashboard interface. Parameters: Name Type Description Default src Tensor [ SEQUENCE_LEN , TOKEN_SIZE ] Keyless reading columns that are passed to the TRecover encoder. required encoded_src Tensor [ SEQUENCE_LEN , 1, D_MODEL ] Keyless reading columns that were encoded by TRecover encoder. required model TRecover Trained model for keyless reading. required width int Number of candidates that can be selected at each step. required device torch . device Device on which to allocate the candidate chains. required Returns: Name Type Description candidates List[Tuple[Tensor[1, SEQUENCE_LEN List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter.","title":"dashboard_loop()"},{"location":"src/trecover/utils/beam_search/#src.trecover.utils.beam_search.dashboard_loop--notes","text":"Probability masks and beam widths values required for each step of the beam search algorithm are calculated by the \"get_steps_params\" function using keyless reading columns (\"src\") that are passed to the TRecover encoder. An empty chain (zero token of shape [1, 1, TOKEN_SIZE]) with zero probability is used as the first candidate for the algorithm. At each step of the algorithm, the progress bar is updated and displayed on the dashboard. Source code in src/trecover/utils/beam_search.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 def dashboard_loop ( src : Tensor , encoded_src : Tensor , model : TRecover , width : int , device : torch . device ) -> List [ Tuple [ Tensor , float ]]: \"\"\" Beam search algorithm loop implementation for the dashboard interface. Parameters ---------- src : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Keyless reading columns that are passed to the TRecover encoder. encoded_src : Tensor[SEQUENCE_LEN, 1, D_MODEL] Keyless reading columns that were encoded by TRecover encoder. model : TRecover Trained model for keyless reading. width : int Number of candidates that can be selected at each step. device : torch.device Device on which to allocate the candidate chains. Returns ------- candidates : List[Tuple[Tensor[1, SEQUENCE_LEN + 1, TOKEN_SIZE], float]] List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Notes ----- Probability masks and beam widths values required for each step of the beam search algorithm are calculated by the \"get_steps_params\" function using keyless reading columns (\"src\") that are passed to the TRecover encoder. An empty chain (zero token of shape [1, 1, TOKEN_SIZE]) with zero probability is used as the first candidate for the algorithm. At each step of the algorithm, the progress bar is updated and displayed on the dashboard. \"\"\" import streamlit as st step_masks , step_widths = get_steps_params ( src ) candidates = [( torch . zeros ( 1 , 1 , model . token_size , device = device ), 0 )] progress = st . progress ( 0 ) for i in range ( encoded_src . shape [ 0 ]): candidates = beam_step ( candidates , step_masks [ i ], step_widths [ i ], encoded_src , model , width , device ) progress . progress ( i / encoded_src . shape [ 0 ]) return candidates","title":"Notes"},{"location":"src/trecover/utils/beam_search/#src.trecover.utils.beam_search.standard_loop","text":"standard_loop ( src , encoded_src , model , width , device ) Base implementation of the beam search algorithm loop. Parameters: Name Type Description Default src Tensor [ SEQUENCE_LEN , TOKEN_SIZE ] Keyless reading columns that are passed to the TRecover encoder. required encoded_src Tensor [ SEQUENCE_LEN , 1, D_MODEL ] Keyless reading columns that were encoded by TRecover encoder. required model TRecover Trained model for keyless reading. required width int Number of candidates that can be selected at each step. required device torch . device Device on which to allocate the candidate chains. required Returns: Name Type Description candidates List[Tuple[Tensor[1, SEQUENCE_LEN List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter.","title":"standard_loop()"},{"location":"src/trecover/utils/beam_search/#src.trecover.utils.beam_search.standard_loop--notes","text":"Probability masks and beam widths values required for each step of the beam search algorithm are calculated by the \"get_steps_params\" function using keyless reading columns (\"src\") that are passed to the TRecover encoder. An empty chain (zero token of shape [1, 1, TOKEN_SIZE]) with zero probability is used as the first candidate for the algorithm. Source code in src/trecover/utils/beam_search.py 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def standard_loop ( src : Tensor , encoded_src : Tensor , model : TRecover , width : int , device : torch . device ) -> List [ Tuple [ Tensor , float ]]: \"\"\" Base implementation of the beam search algorithm loop. Parameters ---------- src : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Keyless reading columns that are passed to the TRecover encoder. encoded_src : Tensor[SEQUENCE_LEN, 1, D_MODEL] Keyless reading columns that were encoded by TRecover encoder. model : TRecover Trained model for keyless reading. width : int Number of candidates that can be selected at each step. device : torch.device Device on which to allocate the candidate chains. Returns ------- candidates : List[Tuple[Tensor[1, SEQUENCE_LEN + 1, TOKEN_SIZE], float]] List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Notes ----- Probability masks and beam widths values required for each step of the beam search algorithm are calculated by the \"get_steps_params\" function using keyless reading columns (\"src\") that are passed to the TRecover encoder. An empty chain (zero token of shape [1, 1, TOKEN_SIZE]) with zero probability is used as the first candidate for the algorithm. \"\"\" step_masks , step_widths = get_steps_params ( src ) candidates = [( torch . zeros ( 1 , 1 , model . token_size , device = device ), 0 )] for i in range ( encoded_src . shape [ 0 ]): candidates = beam_step ( candidates , step_masks [ i ], step_widths [ i ], encoded_src , model , width , device ) return candidates","title":"Notes"},{"location":"src/trecover/utils/beam_search/#src.trecover.utils.beam_search.beam_search","text":"beam_search ( src , model , width , device , beam_loop = standard_loop ) Beam search algorithm implementation. Parameters: Name Type Description Default src Tensor [ SEQUENCE_LEN , TOKEN_SIZE ] Keyless reading columns that are passed to the TRecover encoder. required model TRecover Trained model for keyless reading. required width int Number of candidates that can be selected at each step. required device torch . device Device on which to allocate the candidate chains. required beam_loop Callable[[Tensor, Tensor, TRecover, int, torch.device], List[Tuple[Tensor, float]]], default=standard_loop Beam search algorithm loop function. standard_loop Returns: Name Type Description candidates List [ Tuple [ Tensor [ SEQUENCE_LEN ], float ]] List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter.","title":"beam_search()"},{"location":"src/trecover/utils/beam_search/#src.trecover.utils.beam_search.beam_search--notes","text":"Initially, the keyless reading columns (\"src\") are encoded using TRecover encoder, then the encoded columns (\"encoded_src\") are used at each step of the algorithm. Source code in src/trecover/utils/beam_search.py 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 def beam_search ( src : Tensor , model : TRecover , width : int , device : torch . device , beam_loop : Callable [[ Tensor , Tensor , TRecover , int , torch . device ], List [ Tuple [ Tensor , float ]]] = standard_loop ) -> List [ Tuple [ Tensor , float ]]: \"\"\" Beam search algorithm implementation. Parameters ---------- src : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Keyless reading columns that are passed to the TRecover encoder. model : TRecover Trained model for keyless reading. width : int Number of candidates that can be selected at each step. device : torch.device Device on which to allocate the candidate chains. beam_loop : Callable[[Tensor, Tensor, TRecover, int, torch.device], List[Tuple[Tensor, float]]], default=standard_loop Beam search algorithm loop function. Returns ------- candidates : List[Tuple[Tensor[SEQUENCE_LEN], float]] List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Notes ----- Initially, the keyless reading columns (\"src\") are encoded using TRecover encoder, then the encoded columns (\"encoded_src\") are used at each step of the algorithm. \"\"\" encoded_src = model . encode ( src . unsqueeze ( dim = 0 ), src_pad_mask = None ) candidates = beam_loop ( src , encoded_src , model , width , device ) return [ ( torch . argmax ( chain . squeeze (), dim =- 1 )[ 1 :], score ) # first token is empty_token for chain , score in candidates ]","title":"Notes"},{"location":"src/trecover/utils/beam_search/#src.trecover.utils.beam_search.async_beam_step","text":"async_beam_step ( candidates , step_mask , step_width , encoded_src , model , beam_width , device , ) Asynchronous implementation of the beam search algorithm step. Parameters: Name Type Description Default candidates List [ Tuple [ Tensor [1, STEP_NUMBER , TOKEN_SIZE ], float ]] List of candidates from the previous step. required step_mask Tensor [ TOKEN_SIZE ] Column's mask that consists of zeros in the places that correspond to the letters allowed for selection in the column and values equal to minus infinity in all others. Required so that only the letters in the column are selected as a candidates. required step_width int Number of candidates that are contained in the step column. required encoded_src Tensor [ SEQUENCE_LEN , 1, D_MODEL ] Columns for keyless reading that were encoded by TRecover encoder. required model TRecover Trained model for keyless reading. required beam_width int Number of candidates that can be selected at the current step. required device torch . device Device on which to allocate the candidate chains. required Returns: Name Type Description step_candidates List[Tuple[Tensor[1, STEP_NUMBER List of candidates of size \"beam_width\" for the current step sorted in descending order of their probabilities.","title":"async_beam_step()"},{"location":"src/trecover/utils/beam_search/#src.trecover.utils.beam_search.async_beam_step--notes","text":"For each chain candidate from the previous step: * Probability distribution is calculated using trained TRecover model to select the next symbol from the current column,taking into account the \"step_mask\". * The most probable symbols are selected from the calculated probability distribution, the number of which is set by the \"step_width\" and \"beam_width\" parameters. * For each selected symbol, a new candidate chain with updated probability is constructed and placed in the \"step_candidates\" list. All candidates are sorted in descending order of probabilities and the most probable ones are selected from them, the number of which is set by the \"beam_width\" parameter. Source code in src/trecover/utils/beam_search.py 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 async def async_beam_step ( candidates : List [ Tuple [ Tensor , float ]], step_mask : Tensor , step_width : int , encoded_src : Tensor , model : TRecover , beam_width : int , device : torch . device ) -> List [ Tuple [ Tensor , float ]]: \"\"\" Asynchronous implementation of the beam search algorithm step. Parameters ---------- candidates : List[Tuple[Tensor[1, STEP_NUMBER, TOKEN_SIZE], float]] List of candidates from the previous step. step_mask : Tensor[TOKEN_SIZE] Column's mask that consists of zeros in the places that correspond to the letters allowed for selection in the column and values equal to minus infinity in all others. Required so that only the letters in the column are selected as a candidates. step_width : int Number of candidates that are contained in the step column. encoded_src : Tensor[SEQUENCE_LEN, 1, D_MODEL] Columns for keyless reading that were encoded by TRecover encoder. model : TRecover Trained model for keyless reading. beam_width : int Number of candidates that can be selected at the current step. device : torch.device Device on which to allocate the candidate chains. Returns ------- step_candidates : List[Tuple[Tensor[1, STEP_NUMBER + 1, TOKEN_SIZE], float]] List of candidates of size \"beam_width\" for the current step sorted in descending order of their probabilities. Notes ----- For each chain candidate from the previous step: * Probability distribution is calculated using trained TRecover model to select the next symbol from the current column,taking into account the \"step_mask\". * The most probable symbols are selected from the calculated probability distribution, the number of which is set by the \"step_width\" and \"beam_width\" parameters. * For each selected symbol, a new candidate chain with updated probability is constructed and placed in the \"step_candidates\" list. All candidates are sorted in descending order of probabilities and the most probable ones are selected from them, the number of which is set by the \"beam_width\" parameter. \"\"\" async def candidate_step ( chain : Tensor , score : float ) -> None : prediction = model . predict ( chain , encoded_src , tgt_attn_mask = None , tgt_pad_mask = None , src_pad_mask = None ) probabilities = F . log_softmax ( prediction [ 0 , - 1 ], dim =- 1 ) + step_mask values , indices = probabilities . topk ( k = min ( beam_width , step_width )) for prob , pos in zip ( values , indices ): new_token = torch . zeros ( 1 , 1 , model . token_size , device = device ) new_token [ 0 , 0 , pos ] = 1 step_candidates . append (( torch . cat ([ chain , new_token ], dim = 1 ), score + float ( prob ))) step_candidates = list () for candidate_chain , candidate_score in candidates : await candidate_step ( candidate_chain , candidate_score ) return sorted ( step_candidates , key = lambda candidate : - candidate [ 1 ])[: beam_width ]","title":"Notes"},{"location":"src/trecover/utils/beam_search/#src.trecover.utils.beam_search.api_interactive_loop","text":"api_interactive_loop ( queue , delimiter = '' ) Get an asynchronous beam search algorithm loop function, which is implemented for the API interface. Parameters: Name Type Description Default queue asyncio . Queue Asynchronous queue for storing intermediate results. required delimiter str Delimiter for columns visualization. '' Returns: Name Type Description async_inner_loop Callable [[ Tensor , Tensor , TRecover , int , torch . device ], Awaitable ] Asynchronous beam search algorithm loop function for the API interface. Source code in src/trecover/utils/beam_search.py 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 def api_interactive_loop ( queue : asyncio . Queue , delimiter : str = '' ) -> Callable [[ Tensor , Tensor , TRecover , int , torch . device ], Awaitable ]: \"\"\" Get an asynchronous beam search algorithm loop function, which is implemented for the API interface. Parameters ---------- queue : asyncio.Queue Asynchronous queue for storing intermediate results. delimiter: str, default='' Delimiter for columns visualization. Returns ------- async_inner_loop : Callable[[Tensor, Tensor, TRecover, int, torch.device], Awaitable] Asynchronous beam search algorithm loop function for the API interface. \"\"\" async def async_inner_loop ( src : Tensor , encoded_src : Tensor , model : TRecover , width : int , device : torch . device ) -> None : \"\"\" Asynchronous beam search algorithm loop implementation for the API interface. Parameters ---------- src : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Keyless reading columns that are passed to the TRecover encoder. encoded_src : Tensor[SEQUENCE_LEN, 1, D_MODEL] Keyless reading columns that were encoded by TRecover encoder. model : TRecover Trained model for keyless reading. width : int Number of candidates that can be selected at each step. device : torch.device Device on which to allocate the candidate chains. Notes ----- Probability masks and beam widths values required for each step of the beam search algorithm are calculated by the \"get_steps_params\" function using keyless reading columns (\"src\") that are passed to the TRecover encoder. An empty chain (zero token of shape [1, 1, TOKEN_SIZE]) with zero probability is used as the first candidate for the algorithm. At each step of the algorithm, the intermediate results are placed in an asynchronous queue. At the end of the algorithm, a None value is placed in the asynchronous queue, which is an indicator of its completion. \"\"\" step_masks , step_widths = get_steps_params ( src ) candidates = [( torch . zeros ( 1 , 1 , model . token_size , device = device ), 0 )] for i in range ( encoded_src . shape [ 0 ]): candidates = await async_beam_step ( candidates , step_masks [ i ], step_widths [ i ], encoded_src , model , width , device ) intermediate_result = [ ( tensor_to_target ( torch . argmax ( chain . squeeze (), dim =- 1 )[ 1 :]), score ) for chain , score in candidates ] await queue . put ([( visualize_target ( chain , delimiter ), score ) for chain , score in intermediate_result ]) await queue . put ( None ) return async_inner_loop","title":"api_interactive_loop()"},{"location":"src/trecover/utils/beam_search/#src.trecover.utils.beam_search.standard_async_loop","text":"standard_async_loop ( src , encoded_src , model , width , device ) Base asynchronous implementation of the beam search algorithm loop. Parameters: Name Type Description Default src Tensor [ SEQUENCE_LEN , TOKEN_SIZE ] Keyless reading columns that are passed to the TRecover encoder. required encoded_src Tensor [ SEQUENCE_LEN , 1, D_MODEL ] Keyless reading columns that were encoded by TRecover encoder. required model TRecover Trained model for keyless reading. required width int Number of candidates that can be selected at each step. required device torch . device Device on which to allocate the candidate chains. required Returns: Name Type Description candidates List[Tuple[Tensor[1, SEQUENCE_LEN List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter.","title":"standard_async_loop()"},{"location":"src/trecover/utils/beam_search/#src.trecover.utils.beam_search.standard_async_loop--notes","text":"Probability masks and beam widths values required for each step of the beam search algorithm are calculated by the \"get_steps_params\" function using keyless reading columns (\"src\") that are passed to the TRecover encoder. An empty chain (zero token of shape [1, 1, TOKEN_SIZE]) with zero probability is used as the first candidate for the algorithm. Source code in src/trecover/utils/beam_search.py 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 async def standard_async_loop ( src : Tensor , encoded_src : Tensor , model : TRecover , width : int , device : torch . device ) -> List [ Tuple [ Tensor , float ]]: \"\"\" Base asynchronous implementation of the beam search algorithm loop. Parameters ---------- src : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Keyless reading columns that are passed to the TRecover encoder. encoded_src : Tensor[SEQUENCE_LEN, 1, D_MODEL] Keyless reading columns that were encoded by TRecover encoder. model : TRecover Trained model for keyless reading. width : int Number of candidates that can be selected at each step. device : torch.device Device on which to allocate the candidate chains. Returns ------- candidates : List[Tuple[Tensor[1, SEQUENCE_LEN + 1, TOKEN_SIZE], float]] List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Notes ----- Probability masks and beam widths values required for each step of the beam search algorithm are calculated by the \"get_steps_params\" function using keyless reading columns (\"src\") that are passed to the TRecover encoder. An empty chain (zero token of shape [1, 1, TOKEN_SIZE]) with zero probability is used as the first candidate for the algorithm. \"\"\" step_masks , step_widths = get_steps_params ( src ) candidates = [( torch . zeros ( 1 , 1 , model . token_size , device = device ), 0 )] for i in range ( encoded_src . shape [ 0 ]): candidates = await async_beam_step ( candidates , step_masks [ i ], step_widths [ i ], encoded_src , model , width , device ) return candidates","title":"Notes"},{"location":"src/trecover/utils/beam_search/#src.trecover.utils.beam_search.async_beam_search","text":"async_beam_search ( src , model , width , device , beam_loop = standard_async_loop ) Asynchronous beam search algorithm implementation. Parameters: Name Type Description Default src Tensor [ SEQUENCE_LEN , TOKEN_SIZE ] Keyless reading columns that are passed to the TRecover encoder. required model TRecover Trained model for keyless reading. required width int Number of candidates that can be selected at each step. required device torch . device Device on which to allocate the candidate chains. required beam_loop Callable[[Tensor, Tensor, TRecover, int, torch.device], List[Tuple[Tensor, float]]], default=standard_loop Beam search algorithm loop function. standard_async_loop Returns: Name Type Description candidates Optional [ List [ Tuple [ Tensor [ SEQUENCE_LEN ], float ]]] List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Returns None if \"api_interactive_loop\" is used as a beam search loop function.","title":"async_beam_search()"},{"location":"src/trecover/utils/beam_search/#src.trecover.utils.beam_search.async_beam_search--notes","text":"Initially, the keyless reading columns (\"src\") are encoded using TRecover encoder, then the encoded columns (\"encoded_src\") are used at each step of the asynchronous algorithm. Source code in src/trecover/utils/beam_search.py 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 async def async_beam_search ( src : Tensor , model : TRecover , width : int , device : torch . device , beam_loop : Callable [[ Tensor , Tensor , TRecover , int , torch . device ], Awaitable [ Optional [ List [ Tuple [ Tensor , float ]]]]] = standard_async_loop ) -> Optional [ List [ Tuple [ Tensor , float ]]]: \"\"\" Asynchronous beam search algorithm implementation. Parameters ---------- src : Tensor[SEQUENCE_LEN, TOKEN_SIZE] Keyless reading columns that are passed to the TRecover encoder. model : TRecover Trained model for keyless reading. width : int Number of candidates that can be selected at each step. device : torch.device Device on which to allocate the candidate chains. beam_loop : Callable[[Tensor, Tensor, TRecover, int, torch.device], List[Tuple[Tensor, float]]], default=standard_loop Beam search algorithm loop function. Returns ------- candidates : Optional[List[Tuple[Tensor[SEQUENCE_LEN], float]]] List of chains sorted in descending order of probabilities. The number of candidates is set by the \"width\" parameter. Returns None if \"api_interactive_loop\" is used as a beam search loop function. Notes ----- Initially, the keyless reading columns (\"src\") are encoded using TRecover encoder, then the encoded columns (\"encoded_src\") are used at each step of the asynchronous algorithm. \"\"\" encoded_src = model . encode ( src . unsqueeze ( dim = 0 ), src_pad_mask = None ) candidates = await beam_loop ( src , encoded_src , model , width , device ) return [( torch . argmax ( chain . squeeze (), dim =- 1 )[ 1 :], score ) for chain , score in candidates ] if candidates else None","title":"Notes"},{"location":"src/trecover/utils/cli/","text":"download download ( direct_link , filepath ) Download file. Parameters: Name Type Description Default direct_link str Sharing link to the file on GutHub. required filepath Path Path to the downloaded file. required Source code in src/trecover/utils/cli.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def download ( direct_link : str , filepath : Path ) -> None : \"\"\" Download file. Parameters ---------- direct_link : str Sharing link to the file on GutHub. filepath : Path Path to the downloaded file. \"\"\" filepath . parent . mkdir ( parents = True , exist_ok = True ) with filepath . open ( mode = 'wb' ) as fw : response = requests . get ( direct_link , stream = True ) total_length = response . headers . get ( 'content-length' ) if total_length is None : # no content length header fw . write ( response . content ) else : data_stream = response . iter_content ( chunk_size = 4096 ) with Progress ( TextColumn ( ' {task.description} ' , style = 'bright_blue' ), BarColumn ( complete_style = 'bright_blue' ), DownloadColumn (), TransferSpeedColumn (), transient = True , console = log . project_console ) as progress : download_progress = progress . add_task ( 'Downloading' , total = int ( total_length )) for data in data_stream : fw . write ( data ) progress . update ( download_progress , advance = 4096 ) get_real_direct_link get_real_direct_link ( sharing_link ) Get a direct download link. Parameters: Name Type Description Default sharing_link str Sharing link to the file on Yandex disk. required Returns: Name Type Description str str Direct link if it converts, otherwise None. Source code in src/trecover/utils/cli.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def get_real_direct_link ( sharing_link : str ) -> str : \"\"\" Get a direct download link. Parameters ---------- sharing_link : str Sharing link to the file on Yandex disk. Returns ------- str: Direct link if it converts, otherwise None. \"\"\" pk_request = requests . get ( f 'https://cloud-api.yandex.net/v1/disk/public/resources/download?public_key= { sharing_link } ' ) return pk_request . json () . get ( 'href' ) extract_filename extract_filename ( direct_link ) Get filename of downloaded data Source code in src/trecover/utils/cli.py 79 80 81 82 83 84 85 86 def extract_filename ( direct_link : str ) -> Optional [ str ]: \"\"\" Get filename of downloaded data \"\"\" for chunk in direct_link . strip () . split ( '&' ): if chunk . startswith ( 'filename=' ): return chunk . split ( '=' )[ 1 ] return None download_from_disk download_from_disk ( sharing_link , save_dir ) Download file from Yandex disk. Parameters: Name Type Description Default sharing_link str Sharing link to the file on Yandex disk. required save_dir Path Path where to store downloaded file. required Returns: Name Type Description filepath Optional [ Path ] Path to the file if download was successful, otherwise None. Source code in src/trecover/utils/cli.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def download_from_disk ( sharing_link : str , save_dir : Path ) -> Optional [ Path ]: \"\"\" Download file from Yandex disk. Parameters ---------- sharing_link : str Sharing link to the file on Yandex disk. save_dir : Path Path where to store downloaded file. Returns ------- filepath : Optional[Path] Path to the file if download was successful, otherwise None. \"\"\" if not ( direct_link := get_real_direct_link ( sharing_link )): log . project_logger . error ( f '[red]Failed to download data from [/][bright_blue] { sharing_link } ' ) return None filename = extract_filename ( direct_link ) or 'downloaded_data' # Try to recover the filename from the link filepath = save_dir / filename download ( direct_link = direct_link , filepath = filepath ) log . project_console . print ( f 'Downloaded \" { filename } \" to { filepath . absolute () } ' , style = 'green' ) return filepath download_from_github download_from_github ( direct_link , save_dir ) Download file from GutHub assets. Parameters: Name Type Description Default direct_link str Sharing link to the file on GutHub. required save_dir Path Path where to store downloaded file. required Returns: Name Type Description filepath Path Path to the downloaded file. Source code in src/trecover/utils/cli.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def download_from_github ( direct_link : str , save_dir : Path ) -> Path : \"\"\" Download file from GutHub assets. Parameters ---------- direct_link : str Sharing link to the file on GutHub. save_dir : Path Path where to store downloaded file. Returns ------- filepath : Path Path to the downloaded file. \"\"\" filename = direct_link . split ( '/' )[ - 1 ] filepath = save_dir / filename download ( direct_link = direct_link , filepath = filepath ) log . project_console . print ( f 'Downloaded \" { filename } \" to { filepath . absolute () } ' , style = 'green' ) return filepath download_archive download_archive ( link , save_dir , yandex_disk = False ) Download archive file and extract it to save_dir. Parameters: Name Type Description Default link str Sharing link to the archive file on Yandex disk or GitHub assets. required save_dir Path Path where to store extracted data required yandex_disk bool , default If the link is to Yandex disk. False Source code in src/trecover/utils/cli.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def download_archive ( link : str , save_dir : Path , yandex_disk : bool = False ) -> None : \"\"\" Download archive file and extract it to save_dir. Parameters ---------- link : str Sharing link to the archive file on Yandex disk or GitHub assets. save_dir : Path Path where to store extracted data yandex_disk : bool, default=False If the link is to Yandex disk. \"\"\" filepath = download_from_disk ( link , save_dir ) if yandex_disk else download_from_github ( link , save_dir ) if filepath : with ZipFile ( filepath ) as zf : zf . extractall ( path = save_dir ) os . remove ( filepath ) log . project_console . print ( f 'Archive extracted to { save_dir . absolute () } ' , style = 'green' ) get_files_columns get_files_columns ( inference_path , separator , noisy , min_noise , max_noise , n_to_show , ) Get columns for keyless reading from plain data contained in the files with defined noise range. Parameters: Name Type Description Default inference_path Path Paths to folder with files that contain data to read or create noised columns for keyless reading. required separator str Separator to split the data into columns. required noisy bool Indicates that the data in the files is already noisy and contains columns for keyless reading. required min_noise int Minimum noise range value. required max_noise int Maximum noise range value. required n_to_show int Maximum number of columns. Zero means no restrictions. required Returns: Type Description files , files_columns List of paths and batch of columns for keyless reading. Source code in src/trecover/utils/cli.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def get_files_columns ( inference_path : Path , separator : str , noisy : bool , min_noise : int , max_noise : int , n_to_show : int , ) -> Tuple [ List [ Path ], List [ List [ str ]]]: \"\"\" Get columns for keyless reading from plain data contained in the files with defined noise range. Parameters ---------- inference_path : Path Paths to folder with files that contain data to read or create noised columns for keyless reading. separator : str Separator to split the data into columns. noisy : bool Indicates that the data in the files is already noisy and contains columns for keyless reading. min_noise : int Minimum noise range value. max_noise : int Maximum noise range value. n_to_show : int Maximum number of columns. Zero means no restrictions. Returns ------- (files, files_columns) : Tuple[List[Path], List[List[str]]] List of paths and batch of columns for keyless reading. \"\"\" from trecover.utils.inference import read_files_columns , create_files_noisy_columns if inference_path . is_file (): files = [ inference_path , ] else : files = [ file for file in inference_path . iterdir ()] if noisy : files_columns = read_files_columns ( files , separator , n_to_show ) else : files_columns = create_files_noisy_columns ( files , min_noise , max_noise , n_to_show ) return files , files_columns parse_config parse_config ( file ) Parse configuration file for 'trecover up' command. Source code in src/trecover/utils/cli.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def parse_config ( file : Path ) -> Namespace : \"\"\" Parse configuration file for 'trecover up' command. \"\"\" conf = var . DEFAULT_CONFIG parsed_conf = toml . load ( str ( file . absolute ())) for service , params in parsed_conf . items (): for variable , value in params . items (): conf [ service ][ variable ] = value for service , params in conf . items (): conf [ service ] = Namespace ( ** params ) return Namespace ( ** conf ) start_service start_service ( argv , name , logfile , pidfile ) Start service as a new process with given pid and log files. Parameters: Name Type Description Default argv List [ str ] New process command. required name str Service name. required logfile Path Service logfile path. required pidfile Path Service pidfile path. required Source code in src/trecover/utils/cli.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def start_service ( argv : List [ str ], name : str , logfile : Path , pidfile : Path ) -> None : \"\"\" Start service as a new process with given pid and log files. Parameters ---------- argv : List[str] New process command. name : str Service name. logfile : Path Service logfile path. pidfile : Path Service pidfile path. \"\"\" if platform . system () == 'Windows' : from subprocess import CREATE_NO_WINDOW process = Popen ( argv , creationflags = CREATE_NO_WINDOW , stdout = logfile . open ( mode = 'w+' ), stderr = STDOUT , universal_newlines = True , start_new_session = True ) else : process = Popen ( argv , stdout = logfile . open ( mode = 'w+' ), stderr = STDOUT , universal_newlines = True , start_new_session = True ) with pidfile . open ( 'w' ) as f : f . write ( str ( process . pid )) log . project_console . print ( f 'The { name } service is started' , style = 'bright_blue' ) stop_service stop_service ( name , pidfile , logfile ) Send an interrupt signal to the process with given pid. Parameters: Name Type Description Default name str Service name. required pidfile Path Service pidfile path. required logfile Path Service logfile path. required Source code in src/trecover/utils/cli.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 def stop_service ( name : str , pidfile : Path , logfile : Path ) -> None : \"\"\" Send an interrupt signal to the process with given pid. Parameters ---------- name : str Service name. pidfile : Path Service pidfile path. logfile : Path Service logfile path. \"\"\" try : with pidfile . open () as f : pid = int ( f . read ()) service = psutil . Process ( pid = pid ) for child_proc in service . children ( recursive = True ): child_proc . kill () if platform . system () != 'Windows' : service . kill () service . wait () if logfile . exists (): os . remove ( logfile ) except ValueError : log . project_console . print ( f 'The { name } service could not be stopped correctly' ' because its PID file is corrupted' , style = 'red' ) except ( OSError , psutil . NoSuchProcess ): log . project_console . print ( f 'The { name } service could not be stopped correctly' ' because it probably failed earlier' , style = 'red' ) else : log . project_console . print ( f 'The { name } service is stopped' , style = 'bright_blue' ) finally : if pidfile . exists (): os . remove ( pidfile ) check_service check_service ( name , pidfile ) Display status of the process with given pid. Parameters: Name Type Description Default name str Service name. required pidfile Path Service pidfile path. required Source code in src/trecover/utils/cli.py 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def check_service ( name : str , pidfile : Path ) -> None : \"\"\" Display status of the process with given pid. Parameters ---------- name : str Service name. pidfile : Path Service pidfile path. \"\"\" try : with pidfile . open () as f : pid = int ( f . read ()) if psutil . pid_exists ( pid ): log . project_console . print ( f ':rocket: The { name } status: running' , style = 'bright_blue' ) else : log . project_console . print ( f 'The { name } status: dead' , style = 'red' ) except FileNotFoundError : log . project_console . print ( f 'The { name } service is not started' , style = 'yellow' ) except ValueError : log . project_console . print ( f 'The { name } service could not be checked correctly' ' because its PID file is corrupted' , style = 'red' ) stream stream ( * services , live = False , period = 0.1 ) Get a generator that yields the services' stdout streams. Parameters: Name Type Description Default *services Union [ Tuple [ str , Path ], Tuple [ Tuple [ str , Path ]]] Sequence of services' names and logfile's paths. () live bool , default Yield only new services' logs. False period float , default Generator's delay. 0.1 Returns: Type Description Optional [ Generator [ str , None, None]] Generator that yields the services' stdout streams or None if services are stopped. Source code in src/trecover/utils/cli.py 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 def stream ( * services : Union [ Tuple [ str , Path ], Tuple [ Tuple [ str , Path ]]], live : bool = False , period : float = 0.1 ) -> Optional [ Generator [ str , None , None ]]: \"\"\" Get a generator that yields the services' stdout streams. Parameters ---------- *services : Union[Tuple[str, Path], Tuple[Tuple[str, Path]]] Sequence of services' names and logfile's paths. live : bool, default=False Yield only new services' logs. period : float, default=0.1 Generator's delay. Returns ------- Optional[Generator[str, None, None]]: Generator that yields the services' stdout streams or None if services are stopped. \"\"\" names = list () streams = list () alignment = 0 try : for ( name , log_file ) in services : if not log_file . exists (): continue names . append ( name ) service_stream = log_file . open () if live : service_stream . seek ( 0 , 2 ) streams . append ( service_stream ) if len ( name ) > alignment : alignment = len ( name ) if not ( n_services := len ( names )): return None while True : for i in range ( n_services ): if record := streams [ i ] . read () . strip (): color = var . COLORS [ i % len ( var . COLORS )] for record_line in record . split ( ' \\n ' ): yield f '[ { color } ] { names [ i ] : < { alignment }} |[/ { color } ] { record_line } ' time . sleep ( period ) finally : for service_stream in streams : service_stream . close ()","title":"CLI"},{"location":"src/trecover/utils/cli/#src.trecover.utils.cli.download","text":"download ( direct_link , filepath ) Download file. Parameters: Name Type Description Default direct_link str Sharing link to the file on GutHub. required filepath Path Path to the downloaded file. required Source code in src/trecover/utils/cli.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def download ( direct_link : str , filepath : Path ) -> None : \"\"\" Download file. Parameters ---------- direct_link : str Sharing link to the file on GutHub. filepath : Path Path to the downloaded file. \"\"\" filepath . parent . mkdir ( parents = True , exist_ok = True ) with filepath . open ( mode = 'wb' ) as fw : response = requests . get ( direct_link , stream = True ) total_length = response . headers . get ( 'content-length' ) if total_length is None : # no content length header fw . write ( response . content ) else : data_stream = response . iter_content ( chunk_size = 4096 ) with Progress ( TextColumn ( ' {task.description} ' , style = 'bright_blue' ), BarColumn ( complete_style = 'bright_blue' ), DownloadColumn (), TransferSpeedColumn (), transient = True , console = log . project_console ) as progress : download_progress = progress . add_task ( 'Downloading' , total = int ( total_length )) for data in data_stream : fw . write ( data ) progress . update ( download_progress , advance = 4096 )","title":"download()"},{"location":"src/trecover/utils/cli/#src.trecover.utils.cli.get_real_direct_link","text":"get_real_direct_link ( sharing_link ) Get a direct download link. Parameters: Name Type Description Default sharing_link str Sharing link to the file on Yandex disk. required Returns: Name Type Description str str Direct link if it converts, otherwise None. Source code in src/trecover/utils/cli.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def get_real_direct_link ( sharing_link : str ) -> str : \"\"\" Get a direct download link. Parameters ---------- sharing_link : str Sharing link to the file on Yandex disk. Returns ------- str: Direct link if it converts, otherwise None. \"\"\" pk_request = requests . get ( f 'https://cloud-api.yandex.net/v1/disk/public/resources/download?public_key= { sharing_link } ' ) return pk_request . json () . get ( 'href' )","title":"get_real_direct_link()"},{"location":"src/trecover/utils/cli/#src.trecover.utils.cli.extract_filename","text":"extract_filename ( direct_link ) Get filename of downloaded data Source code in src/trecover/utils/cli.py 79 80 81 82 83 84 85 86 def extract_filename ( direct_link : str ) -> Optional [ str ]: \"\"\" Get filename of downloaded data \"\"\" for chunk in direct_link . strip () . split ( '&' ): if chunk . startswith ( 'filename=' ): return chunk . split ( '=' )[ 1 ] return None","title":"extract_filename()"},{"location":"src/trecover/utils/cli/#src.trecover.utils.cli.download_from_disk","text":"download_from_disk ( sharing_link , save_dir ) Download file from Yandex disk. Parameters: Name Type Description Default sharing_link str Sharing link to the file on Yandex disk. required save_dir Path Path where to store downloaded file. required Returns: Name Type Description filepath Optional [ Path ] Path to the file if download was successful, otherwise None. Source code in src/trecover/utils/cli.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def download_from_disk ( sharing_link : str , save_dir : Path ) -> Optional [ Path ]: \"\"\" Download file from Yandex disk. Parameters ---------- sharing_link : str Sharing link to the file on Yandex disk. save_dir : Path Path where to store downloaded file. Returns ------- filepath : Optional[Path] Path to the file if download was successful, otherwise None. \"\"\" if not ( direct_link := get_real_direct_link ( sharing_link )): log . project_logger . error ( f '[red]Failed to download data from [/][bright_blue] { sharing_link } ' ) return None filename = extract_filename ( direct_link ) or 'downloaded_data' # Try to recover the filename from the link filepath = save_dir / filename download ( direct_link = direct_link , filepath = filepath ) log . project_console . print ( f 'Downloaded \" { filename } \" to { filepath . absolute () } ' , style = 'green' ) return filepath","title":"download_from_disk()"},{"location":"src/trecover/utils/cli/#src.trecover.utils.cli.download_from_github","text":"download_from_github ( direct_link , save_dir ) Download file from GutHub assets. Parameters: Name Type Description Default direct_link str Sharing link to the file on GutHub. required save_dir Path Path where to store downloaded file. required Returns: Name Type Description filepath Path Path to the downloaded file. Source code in src/trecover/utils/cli.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def download_from_github ( direct_link : str , save_dir : Path ) -> Path : \"\"\" Download file from GutHub assets. Parameters ---------- direct_link : str Sharing link to the file on GutHub. save_dir : Path Path where to store downloaded file. Returns ------- filepath : Path Path to the downloaded file. \"\"\" filename = direct_link . split ( '/' )[ - 1 ] filepath = save_dir / filename download ( direct_link = direct_link , filepath = filepath ) log . project_console . print ( f 'Downloaded \" { filename } \" to { filepath . absolute () } ' , style = 'green' ) return filepath","title":"download_from_github()"},{"location":"src/trecover/utils/cli/#src.trecover.utils.cli.download_archive","text":"download_archive ( link , save_dir , yandex_disk = False ) Download archive file and extract it to save_dir. Parameters: Name Type Description Default link str Sharing link to the archive file on Yandex disk or GitHub assets. required save_dir Path Path where to store extracted data required yandex_disk bool , default If the link is to Yandex disk. False Source code in src/trecover/utils/cli.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def download_archive ( link : str , save_dir : Path , yandex_disk : bool = False ) -> None : \"\"\" Download archive file and extract it to save_dir. Parameters ---------- link : str Sharing link to the archive file on Yandex disk or GitHub assets. save_dir : Path Path where to store extracted data yandex_disk : bool, default=False If the link is to Yandex disk. \"\"\" filepath = download_from_disk ( link , save_dir ) if yandex_disk else download_from_github ( link , save_dir ) if filepath : with ZipFile ( filepath ) as zf : zf . extractall ( path = save_dir ) os . remove ( filepath ) log . project_console . print ( f 'Archive extracted to { save_dir . absolute () } ' , style = 'green' )","title":"download_archive()"},{"location":"src/trecover/utils/cli/#src.trecover.utils.cli.get_files_columns","text":"get_files_columns ( inference_path , separator , noisy , min_noise , max_noise , n_to_show , ) Get columns for keyless reading from plain data contained in the files with defined noise range. Parameters: Name Type Description Default inference_path Path Paths to folder with files that contain data to read or create noised columns for keyless reading. required separator str Separator to split the data into columns. required noisy bool Indicates that the data in the files is already noisy and contains columns for keyless reading. required min_noise int Minimum noise range value. required max_noise int Maximum noise range value. required n_to_show int Maximum number of columns. Zero means no restrictions. required Returns: Type Description files , files_columns List of paths and batch of columns for keyless reading. Source code in src/trecover/utils/cli.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def get_files_columns ( inference_path : Path , separator : str , noisy : bool , min_noise : int , max_noise : int , n_to_show : int , ) -> Tuple [ List [ Path ], List [ List [ str ]]]: \"\"\" Get columns for keyless reading from plain data contained in the files with defined noise range. Parameters ---------- inference_path : Path Paths to folder with files that contain data to read or create noised columns for keyless reading. separator : str Separator to split the data into columns. noisy : bool Indicates that the data in the files is already noisy and contains columns for keyless reading. min_noise : int Minimum noise range value. max_noise : int Maximum noise range value. n_to_show : int Maximum number of columns. Zero means no restrictions. Returns ------- (files, files_columns) : Tuple[List[Path], List[List[str]]] List of paths and batch of columns for keyless reading. \"\"\" from trecover.utils.inference import read_files_columns , create_files_noisy_columns if inference_path . is_file (): files = [ inference_path , ] else : files = [ file for file in inference_path . iterdir ()] if noisy : files_columns = read_files_columns ( files , separator , n_to_show ) else : files_columns = create_files_noisy_columns ( files , min_noise , max_noise , n_to_show ) return files , files_columns","title":"get_files_columns()"},{"location":"src/trecover/utils/cli/#src.trecover.utils.cli.parse_config","text":"parse_config ( file ) Parse configuration file for 'trecover up' command. Source code in src/trecover/utils/cli.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def parse_config ( file : Path ) -> Namespace : \"\"\" Parse configuration file for 'trecover up' command. \"\"\" conf = var . DEFAULT_CONFIG parsed_conf = toml . load ( str ( file . absolute ())) for service , params in parsed_conf . items (): for variable , value in params . items (): conf [ service ][ variable ] = value for service , params in conf . items (): conf [ service ] = Namespace ( ** params ) return Namespace ( ** conf )","title":"parse_config()"},{"location":"src/trecover/utils/cli/#src.trecover.utils.cli.start_service","text":"start_service ( argv , name , logfile , pidfile ) Start service as a new process with given pid and log files. Parameters: Name Type Description Default argv List [ str ] New process command. required name str Service name. required logfile Path Service logfile path. required pidfile Path Service pidfile path. required Source code in src/trecover/utils/cli.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def start_service ( argv : List [ str ], name : str , logfile : Path , pidfile : Path ) -> None : \"\"\" Start service as a new process with given pid and log files. Parameters ---------- argv : List[str] New process command. name : str Service name. logfile : Path Service logfile path. pidfile : Path Service pidfile path. \"\"\" if platform . system () == 'Windows' : from subprocess import CREATE_NO_WINDOW process = Popen ( argv , creationflags = CREATE_NO_WINDOW , stdout = logfile . open ( mode = 'w+' ), stderr = STDOUT , universal_newlines = True , start_new_session = True ) else : process = Popen ( argv , stdout = logfile . open ( mode = 'w+' ), stderr = STDOUT , universal_newlines = True , start_new_session = True ) with pidfile . open ( 'w' ) as f : f . write ( str ( process . pid )) log . project_console . print ( f 'The { name } service is started' , style = 'bright_blue' )","title":"start_service()"},{"location":"src/trecover/utils/cli/#src.trecover.utils.cli.stop_service","text":"stop_service ( name , pidfile , logfile ) Send an interrupt signal to the process with given pid. Parameters: Name Type Description Default name str Service name. required pidfile Path Service pidfile path. required logfile Path Service logfile path. required Source code in src/trecover/utils/cli.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 def stop_service ( name : str , pidfile : Path , logfile : Path ) -> None : \"\"\" Send an interrupt signal to the process with given pid. Parameters ---------- name : str Service name. pidfile : Path Service pidfile path. logfile : Path Service logfile path. \"\"\" try : with pidfile . open () as f : pid = int ( f . read ()) service = psutil . Process ( pid = pid ) for child_proc in service . children ( recursive = True ): child_proc . kill () if platform . system () != 'Windows' : service . kill () service . wait () if logfile . exists (): os . remove ( logfile ) except ValueError : log . project_console . print ( f 'The { name } service could not be stopped correctly' ' because its PID file is corrupted' , style = 'red' ) except ( OSError , psutil . NoSuchProcess ): log . project_console . print ( f 'The { name } service could not be stopped correctly' ' because it probably failed earlier' , style = 'red' ) else : log . project_console . print ( f 'The { name } service is stopped' , style = 'bright_blue' ) finally : if pidfile . exists (): os . remove ( pidfile )","title":"stop_service()"},{"location":"src/trecover/utils/cli/#src.trecover.utils.cli.check_service","text":"check_service ( name , pidfile ) Display status of the process with given pid. Parameters: Name Type Description Default name str Service name. required pidfile Path Service pidfile path. required Source code in src/trecover/utils/cli.py 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def check_service ( name : str , pidfile : Path ) -> None : \"\"\" Display status of the process with given pid. Parameters ---------- name : str Service name. pidfile : Path Service pidfile path. \"\"\" try : with pidfile . open () as f : pid = int ( f . read ()) if psutil . pid_exists ( pid ): log . project_console . print ( f ':rocket: The { name } status: running' , style = 'bright_blue' ) else : log . project_console . print ( f 'The { name } status: dead' , style = 'red' ) except FileNotFoundError : log . project_console . print ( f 'The { name } service is not started' , style = 'yellow' ) except ValueError : log . project_console . print ( f 'The { name } service could not be checked correctly' ' because its PID file is corrupted' , style = 'red' )","title":"check_service()"},{"location":"src/trecover/utils/cli/#src.trecover.utils.cli.stream","text":"stream ( * services , live = False , period = 0.1 ) Get a generator that yields the services' stdout streams. Parameters: Name Type Description Default *services Union [ Tuple [ str , Path ], Tuple [ Tuple [ str , Path ]]] Sequence of services' names and logfile's paths. () live bool , default Yield only new services' logs. False period float , default Generator's delay. 0.1 Returns: Type Description Optional [ Generator [ str , None, None]] Generator that yields the services' stdout streams or None if services are stopped. Source code in src/trecover/utils/cli.py 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 def stream ( * services : Union [ Tuple [ str , Path ], Tuple [ Tuple [ str , Path ]]], live : bool = False , period : float = 0.1 ) -> Optional [ Generator [ str , None , None ]]: \"\"\" Get a generator that yields the services' stdout streams. Parameters ---------- *services : Union[Tuple[str, Path], Tuple[Tuple[str, Path]]] Sequence of services' names and logfile's paths. live : bool, default=False Yield only new services' logs. period : float, default=0.1 Generator's delay. Returns ------- Optional[Generator[str, None, None]]: Generator that yields the services' stdout streams or None if services are stopped. \"\"\" names = list () streams = list () alignment = 0 try : for ( name , log_file ) in services : if not log_file . exists (): continue names . append ( name ) service_stream = log_file . open () if live : service_stream . seek ( 0 , 2 ) streams . append ( service_stream ) if len ( name ) > alignment : alignment = len ( name ) if not ( n_services := len ( names )): return None while True : for i in range ( n_services ): if record := streams [ i ] . read () . strip (): color = var . COLORS [ i % len ( var . COLORS )] for record_line in record . split ( ' \\n ' ): yield f '[ { color } ] { names [ i ] : < { alignment }} |[/ { color } ] { record_line } ' time . sleep ( period ) finally : for service_stream in streams : service_stream . close ()","title":"stream()"},{"location":"src/trecover/utils/docker/","text":"get_client get_client () Get the docker client Source code in src/trecover/utils/docker.py 13 14 15 16 def get_client () -> DockerClient : \"\"\" Get the docker client \"\"\" return docker . from_env () is_docker_running is_docker_running () Check if docker is running Source code in src/trecover/utils/docker.py 19 20 21 22 23 24 25 26 def is_docker_running () -> bool : \"\"\" Check if docker is running \"\"\" try : get_client () . ping () except ( docker . errors . APIError , docker . errors . DockerException ): return False return True get_images_list get_images_list () Get list of docker images Source code in src/trecover/utils/docker.py 29 30 31 32 33 34 35 36 def get_images_list () -> List [ str ]: \"\"\" Get list of docker images \"\"\" image_names = list () for image in get_client () . images . list (): image_names . extend ( image . tags ) return image_names get_containers_list get_containers_list () Get list of docker containers Source code in src/trecover/utils/docker.py 39 40 41 42 def get_containers_list () -> List [ str ]: \"\"\" Get list of docker containers \"\"\" return [ container . name for container in get_client () . containers . list ( all = True )] get_container get_container ( id_or_name ) Get the docker container with given name or id. Parameters: Name Type Description Default id_or_name str Docker container id or name. required Returns: Type Description Optional [ Container ] Container instance if it exists, otherwise None. Source code in src/trecover/utils/docker.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def get_container ( id_or_name : str ) -> Optional [ Container ]: \"\"\" Get the docker container with given name or id. Parameters ---------- id_or_name : str Docker container id or name. Returns ------- Optional[Container]: Container instance if it exists, otherwise None. \"\"\" try : return get_client () . containers . get ( id_or_name ) except docker . errors . NotFound : return None get_volume get_volume ( id_or_name ) Get the docker volume with given name or id. Parameters: Name Type Description Default id_or_name str Volume id or name. required Returns: Type Description Optional [ Volume ] Volume instance if it exists, otherwise None. Source code in src/trecover/utils/docker.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def get_volume ( id_or_name : str ) -> Optional [ Volume ]: \"\"\" Get the docker volume with given name or id. Parameters ---------- id_or_name : str Volume id or name. Returns ------- Optional[Volume]: Volume instance if it exists, otherwise None. \"\"\" try : return get_client () . volumes . get ( id_or_name ) except docker . errors . NotFound : return None get_image get_image ( name ) Get the docker image with given name. Parameters: Name Type Description Default name str Docker image name. required Returns: Type Description Optional [ Image ] Image instance if it exists, otherwise None. Source code in src/trecover/utils/docker.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def get_image ( name : str ) -> Optional [ Image ]: \"\"\" Get the docker image with given name. Parameters ---------- name : str Docker image name. Returns ------- Optional[Image]: Image instance if it exists, otherwise None. \"\"\" try : return get_client () . images . get ( name ) except docker . errors . NotFound : return None pull_image pull_image ( name ) Pull (download) the docker image with given name. Parameters: Name Type Description Default name str Docker image name. required Returns: Name Type Description Image Image Image instance of the pulled docker image. Source code in src/trecover/utils/docker.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def pull_image ( name : str ) -> Image : \"\"\" Pull (download) the docker image with given name. Parameters ---------- name : str Docker image name. Returns ------- Image: Image instance of the pulled docker image. \"\"\" with Live ( console = log . project_console ) as screen : screen . update ( '[bright_blue]Waiting' ) for state in get_client () . api . pull ( name , stream = True , decode = True ): try : screen . update ( f '[bright_blue] { state [ \"status\" ] } [/] { state [ \"progress\" ] } ' ) except KeyError : screen . update ( f '[bright_blue] { state [ \"status\" ] } ' ) return get_image ( name )","title":"Docker"},{"location":"src/trecover/utils/docker/#src.trecover.utils.docker.get_client","text":"get_client () Get the docker client Source code in src/trecover/utils/docker.py 13 14 15 16 def get_client () -> DockerClient : \"\"\" Get the docker client \"\"\" return docker . from_env ()","title":"get_client()"},{"location":"src/trecover/utils/docker/#src.trecover.utils.docker.is_docker_running","text":"is_docker_running () Check if docker is running Source code in src/trecover/utils/docker.py 19 20 21 22 23 24 25 26 def is_docker_running () -> bool : \"\"\" Check if docker is running \"\"\" try : get_client () . ping () except ( docker . errors . APIError , docker . errors . DockerException ): return False return True","title":"is_docker_running()"},{"location":"src/trecover/utils/docker/#src.trecover.utils.docker.get_images_list","text":"get_images_list () Get list of docker images Source code in src/trecover/utils/docker.py 29 30 31 32 33 34 35 36 def get_images_list () -> List [ str ]: \"\"\" Get list of docker images \"\"\" image_names = list () for image in get_client () . images . list (): image_names . extend ( image . tags ) return image_names","title":"get_images_list()"},{"location":"src/trecover/utils/docker/#src.trecover.utils.docker.get_containers_list","text":"get_containers_list () Get list of docker containers Source code in src/trecover/utils/docker.py 39 40 41 42 def get_containers_list () -> List [ str ]: \"\"\" Get list of docker containers \"\"\" return [ container . name for container in get_client () . containers . list ( all = True )]","title":"get_containers_list()"},{"location":"src/trecover/utils/docker/#src.trecover.utils.docker.get_container","text":"get_container ( id_or_name ) Get the docker container with given name or id. Parameters: Name Type Description Default id_or_name str Docker container id or name. required Returns: Type Description Optional [ Container ] Container instance if it exists, otherwise None. Source code in src/trecover/utils/docker.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def get_container ( id_or_name : str ) -> Optional [ Container ]: \"\"\" Get the docker container with given name or id. Parameters ---------- id_or_name : str Docker container id or name. Returns ------- Optional[Container]: Container instance if it exists, otherwise None. \"\"\" try : return get_client () . containers . get ( id_or_name ) except docker . errors . NotFound : return None","title":"get_container()"},{"location":"src/trecover/utils/docker/#src.trecover.utils.docker.get_volume","text":"get_volume ( id_or_name ) Get the docker volume with given name or id. Parameters: Name Type Description Default id_or_name str Volume id or name. required Returns: Type Description Optional [ Volume ] Volume instance if it exists, otherwise None. Source code in src/trecover/utils/docker.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def get_volume ( id_or_name : str ) -> Optional [ Volume ]: \"\"\" Get the docker volume with given name or id. Parameters ---------- id_or_name : str Volume id or name. Returns ------- Optional[Volume]: Volume instance if it exists, otherwise None. \"\"\" try : return get_client () . volumes . get ( id_or_name ) except docker . errors . NotFound : return None","title":"get_volume()"},{"location":"src/trecover/utils/docker/#src.trecover.utils.docker.get_image","text":"get_image ( name ) Get the docker image with given name. Parameters: Name Type Description Default name str Docker image name. required Returns: Type Description Optional [ Image ] Image instance if it exists, otherwise None. Source code in src/trecover/utils/docker.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def get_image ( name : str ) -> Optional [ Image ]: \"\"\" Get the docker image with given name. Parameters ---------- name : str Docker image name. Returns ------- Optional[Image]: Image instance if it exists, otherwise None. \"\"\" try : return get_client () . images . get ( name ) except docker . errors . NotFound : return None","title":"get_image()"},{"location":"src/trecover/utils/docker/#src.trecover.utils.docker.pull_image","text":"pull_image ( name ) Pull (download) the docker image with given name. Parameters: Name Type Description Default name str Docker image name. required Returns: Name Type Description Image Image Image instance of the pulled docker image. Source code in src/trecover/utils/docker.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def pull_image ( name : str ) -> Image : \"\"\" Pull (download) the docker image with given name. Parameters ---------- name : str Docker image name. Returns ------- Image: Image instance of the pulled docker image. \"\"\" with Live ( console = log . project_console ) as screen : screen . update ( '[bright_blue]Waiting' ) for state in get_client () . api . pull ( name , stream = True , decode = True ): try : screen . update ( f '[bright_blue] { state [ \"status\" ] } [/] { state [ \"progress\" ] } ' ) except KeyError : screen . update ( f '[bright_blue] { state [ \"status\" ] } ' ) return get_image ( name )","title":"pull_image()"},{"location":"src/trecover/utils/inference/","text":"create_noisy_columns create_noisy_columns ( data , min_noise , max_noise ) Generate columns for keyless reading from plain data with defined noise range. Parameters: Name Type Description Default data str Plain text. required min_noise int Minimum noise range value. required max_noise int Maximum noise range value. required Returns: Name Type Description columns List [ str ] Columns for keyless reading. Source code in src/trecover/utils/inference.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def create_noisy_columns ( data : str , min_noise : int , max_noise : int ) -> List [ str ]: \"\"\" Generate columns for keyless reading from plain data with defined noise range. Parameters ---------- data : str Plain text. min_noise : int Minimum noise range value. max_noise : int Maximum noise range value. Returns ------- columns : List[str] Columns for keyless reading. \"\"\" np . random . seed ( None ) columns = list () data = re . sub ( r '[^A-Za-z]' , '' , data ) . lower () for symbol in data : noise_size = np . random . randint ( low = min_noise , high = max_noise , size = 1 )[ 0 ] noise_indexes = np . random . choice ( list ( var . ALPHABET . difference ( symbol )), size = noise_size , replace = False ) columns . append ( f \" { symbol }{ '' . join ( noise_indexes ) } \" ) return columns create_files_noisy_columns create_files_noisy_columns ( files , min_noise , max_noise , n_to_show = 0 ) Generate columns for keyless reading from plain data contained in the files with defined noise range. Parameters: Name Type Description Default files List [ Union [ str , Path ]] Paths to files that contain plain data to create noised columns for keyless reading. required min_noise int Minimum noise range value. required max_noise int Maximum noise range value. required n_to_show int , default Maximum number of columns. Zero means no restrictions. 0 Returns: Name Type Description files_columns List [ List [ str ]] Batch of columns for keyless reading. Source code in src/trecover/utils/inference.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def create_files_noisy_columns ( files : List [ Union [ str , Path ]], min_noise : int , max_noise : int , n_to_show : int = 0 ) -> List [ List [ str ]]: \"\"\" Generate columns for keyless reading from plain data contained in the files with defined noise range. Parameters ---------- files : List[Union[str, Path]] Paths to files that contain plain data to create noised columns for keyless reading. min_noise : int Minimum noise range value. max_noise : int Maximum noise range value. n_to_show : int, default=0 Maximum number of columns. Zero means no restrictions. Returns ------- files_columns : List[List[str]] Batch of columns for keyless reading. \"\"\" files_columns = list () for file in files : with open ( file ) as f : data = f . read () if n_to_show > 0 : data = data [: n_to_show ] columns = create_noisy_columns ( data , min_noise , max_noise ) files_columns . append ( columns ) return files_columns data_to_columns data_to_columns ( data , separator = ' ' ) Clean and split noised data. Parameters: Name Type Description Default data str Noised columns for keyless reading. required separator str , default Separator to split the data into columns. ' ' Returns: Type Description List [ str ] Columns for keyless reading. Source code in src/trecover/utils/inference.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def data_to_columns ( data : str , separator : str = ' ' ) -> List [ str ]: \"\"\" Clean and split noised data. Parameters ---------- data : str Noised columns for keyless reading. separator : str, default=' ' Separator to split the data into columns. Returns ------- List[str]: Columns for keyless reading. \"\"\" data = re . sub ( separator , ' ' , data ) cleaned_data = re . sub ( r '[^A-Za-z ]' , '' , data ) . lower () return cleaned_data . split ( ' ' ) read_files_columns read_files_columns ( files , separator , n_to_show = 0 ) Read, clean and split noised data contained in the files. Parameters: Name Type Description Default files List [ Union [ str , Path ]] Paths to files that contain noised data for keyless reading. required separator str Separator to split the data into columns. required n_to_show int , default Maximum number of columns. Zero means no restrictions. 0 Returns: Name Type Description files_columns List [ List [ str ]] Batch of columns for keyless reading. Source code in src/trecover/utils/inference.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def read_files_columns ( files : List [ Union [ str , Path ]], separator : str , n_to_show : int = 0 ) -> List [ List [ str ]]: \"\"\" Read, clean and split noised data contained in the files. Parameters ---------- files : List[Union[str, Path]] Paths to files that contain noised data for keyless reading. separator : str Separator to split the data into columns. n_to_show : int, default=0 Maximum number of columns. Zero means no restrictions. Returns ------- files_columns : List[List[str]] Batch of columns for keyless reading. \"\"\" files_columns = list () for file in files : with open ( file ) as f : data = f . read () columns = data_to_columns ( data , separator ) if n_to_show > 0 : columns = columns [: n_to_show ] files_columns . append ( columns ) return files_columns","title":"Inference"},{"location":"src/trecover/utils/inference/#src.trecover.utils.inference.create_noisy_columns","text":"create_noisy_columns ( data , min_noise , max_noise ) Generate columns for keyless reading from plain data with defined noise range. Parameters: Name Type Description Default data str Plain text. required min_noise int Minimum noise range value. required max_noise int Maximum noise range value. required Returns: Name Type Description columns List [ str ] Columns for keyless reading. Source code in src/trecover/utils/inference.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def create_noisy_columns ( data : str , min_noise : int , max_noise : int ) -> List [ str ]: \"\"\" Generate columns for keyless reading from plain data with defined noise range. Parameters ---------- data : str Plain text. min_noise : int Minimum noise range value. max_noise : int Maximum noise range value. Returns ------- columns : List[str] Columns for keyless reading. \"\"\" np . random . seed ( None ) columns = list () data = re . sub ( r '[^A-Za-z]' , '' , data ) . lower () for symbol in data : noise_size = np . random . randint ( low = min_noise , high = max_noise , size = 1 )[ 0 ] noise_indexes = np . random . choice ( list ( var . ALPHABET . difference ( symbol )), size = noise_size , replace = False ) columns . append ( f \" { symbol }{ '' . join ( noise_indexes ) } \" ) return columns","title":"create_noisy_columns()"},{"location":"src/trecover/utils/inference/#src.trecover.utils.inference.create_files_noisy_columns","text":"create_files_noisy_columns ( files , min_noise , max_noise , n_to_show = 0 ) Generate columns for keyless reading from plain data contained in the files with defined noise range. Parameters: Name Type Description Default files List [ Union [ str , Path ]] Paths to files that contain plain data to create noised columns for keyless reading. required min_noise int Minimum noise range value. required max_noise int Maximum noise range value. required n_to_show int , default Maximum number of columns. Zero means no restrictions. 0 Returns: Name Type Description files_columns List [ List [ str ]] Batch of columns for keyless reading. Source code in src/trecover/utils/inference.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def create_files_noisy_columns ( files : List [ Union [ str , Path ]], min_noise : int , max_noise : int , n_to_show : int = 0 ) -> List [ List [ str ]]: \"\"\" Generate columns for keyless reading from plain data contained in the files with defined noise range. Parameters ---------- files : List[Union[str, Path]] Paths to files that contain plain data to create noised columns for keyless reading. min_noise : int Minimum noise range value. max_noise : int Maximum noise range value. n_to_show : int, default=0 Maximum number of columns. Zero means no restrictions. Returns ------- files_columns : List[List[str]] Batch of columns for keyless reading. \"\"\" files_columns = list () for file in files : with open ( file ) as f : data = f . read () if n_to_show > 0 : data = data [: n_to_show ] columns = create_noisy_columns ( data , min_noise , max_noise ) files_columns . append ( columns ) return files_columns","title":"create_files_noisy_columns()"},{"location":"src/trecover/utils/inference/#src.trecover.utils.inference.data_to_columns","text":"data_to_columns ( data , separator = ' ' ) Clean and split noised data. Parameters: Name Type Description Default data str Noised columns for keyless reading. required separator str , default Separator to split the data into columns. ' ' Returns: Type Description List [ str ] Columns for keyless reading. Source code in src/trecover/utils/inference.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def data_to_columns ( data : str , separator : str = ' ' ) -> List [ str ]: \"\"\" Clean and split noised data. Parameters ---------- data : str Noised columns for keyless reading. separator : str, default=' ' Separator to split the data into columns. Returns ------- List[str]: Columns for keyless reading. \"\"\" data = re . sub ( separator , ' ' , data ) cleaned_data = re . sub ( r '[^A-Za-z ]' , '' , data ) . lower () return cleaned_data . split ( ' ' )","title":"data_to_columns()"},{"location":"src/trecover/utils/inference/#src.trecover.utils.inference.read_files_columns","text":"read_files_columns ( files , separator , n_to_show = 0 ) Read, clean and split noised data contained in the files. Parameters: Name Type Description Default files List [ Union [ str , Path ]] Paths to files that contain noised data for keyless reading. required separator str Separator to split the data into columns. required n_to_show int , default Maximum number of columns. Zero means no restrictions. 0 Returns: Name Type Description files_columns List [ List [ str ]] Batch of columns for keyless reading. Source code in src/trecover/utils/inference.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def read_files_columns ( files : List [ Union [ str , Path ]], separator : str , n_to_show : int = 0 ) -> List [ List [ str ]]: \"\"\" Read, clean and split noised data contained in the files. Parameters ---------- files : List[Union[str, Path]] Paths to files that contain noised data for keyless reading. separator : str Separator to split the data into columns. n_to_show : int, default=0 Maximum number of columns. Zero means no restrictions. Returns ------- files_columns : List[List[str]] Batch of columns for keyless reading. \"\"\" files_columns = list () for file in files : with open ( file ) as f : data = f . read () columns = data_to_columns ( data , separator ) if n_to_show > 0 : columns = columns [: n_to_show ] files_columns . append ( columns ) return files_columns","title":"read_files_columns()"},{"location":"src/trecover/utils/model/","text":"get_recent_weights_path get_recent_weights_path ( exp_dir , exp_mark , weights_name = None ) Get a model recent weights path. Parameters: Name Type Description Default exp_dir Path Experiment directory path. required exp_mark str Experiment folder mark. required weights_name str , default Weights filename. None Returns: Type Description Optional [ Path ] Recent weights path if it exists otherwise None object. Source code in src/trecover/utils/model.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def get_recent_weights_path ( exp_dir : Path , exp_mark : str , weights_name : Optional [ str ] = None ) -> Optional [ Path ]: \"\"\" Get a model recent weights path. Parameters ---------- exp_dir : Path Experiment directory path. exp_mark : str Experiment folder mark. weights_name : str, default=None Weights filename. Returns ------- Optional[Path]: Recent weights path if it exists otherwise None object. \"\"\" if weights_name : return weights_path if ( weights_path := exp_dir / exp_mark / weights_name ) . exists () else None if ( weights_path := exp_dir / exp_mark / 'weights' ) . exists (): recent_weights = None most_recent_timestamp = 0 for weights in weights_path . iterdir (): if timestamp := weights . stat () . st_ctime > most_recent_timestamp : recent_weights = weights most_recent_timestamp = timestamp return recent_weights get_model get_model ( token_size , pe_max_len , num_layers , d_model , n_heads , d_ff , dropout , device = torch . device ( \"cpu\" ), weights = None , silently = False , ) Get a model with specified configuration. Parameters: Name Type Description Default token_size int Token (column) size. required pe_max_len int Positional encoding max length. required num_layers int Number of encoder and decoder blocks required d_model int Model dimension - number of expected features in the encoder (decoder) input. required n_heads int Number of encoder and decoder attention heads. required d_ff int Dimension of the feedforward layer. required dropout float Dropout range. required device torch . device , default Device on which to allocate the model. torch.device('cpu') weights Path , default Model weights path for initialization. None silently bool , default Initialize the model silently without any verbose information. False Returns: Name Type Description model TRecover Initialized model. Raises: Type Description SystemExit: If the weight's path is not provided and the cli 'stop' option is selected. Source code in src/trecover/utils/model.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def get_model ( token_size : int , pe_max_len : int , num_layers : int , d_model : int , n_heads : int , d_ff : int , dropout : float , device : torch . device = torch . device ( 'cpu' ), weights : Optional [ Path ] = None , silently : bool = False ) -> TRecover : \"\"\" Get a model with specified configuration. Parameters ---------- token_size : int Token (column) size. pe_max_len : int Positional encoding max length. num_layers : int Number of encoder and decoder blocks d_model : int Model dimension - number of expected features in the encoder (decoder) input. n_heads : int Number of encoder and decoder attention heads. d_ff : int Dimension of the feedforward layer. dropout : float, Dropout range. device : torch.device, default=torch.device('cpu') Device on which to allocate the model. weights : Path, default=None Model weights path for initialization. silently : bool, default=False Initialize the model silently without any verbose information. Returns ------- model : TRecover Initialized model. Raises ------ SystemExit: If the weight's path is not provided and the cli 'stop' option is selected. \"\"\" model = TRecover ( token_size , pe_max_len , num_layers , d_model , n_heads , d_ff , dropout ) . to ( device ) if weights and weights . exists () and weights . is_file (): model . load_parameters ( weights , device = device ) if not silently : log . project_console . print ( f 'The below model parameters have been loaded: \\n { weights } ' , style = 'bright_green' ) return model if weights : log . project_console . print ( f 'Failed to load model parameters: { str ( weights ) } ' , style = 'bold red' ) else : log . project_console . print ( \"Model parameters aren't specified\" , style = 'bright_blue' ) if silently or Confirm . ask ( prompt = '[bright_blue]Continue training from scratch?' , default = True , console = log . project_console ): return model else : raise SystemExit","title":"Model"},{"location":"src/trecover/utils/model/#src.trecover.utils.model.get_recent_weights_path","text":"get_recent_weights_path ( exp_dir , exp_mark , weights_name = None ) Get a model recent weights path. Parameters: Name Type Description Default exp_dir Path Experiment directory path. required exp_mark str Experiment folder mark. required weights_name str , default Weights filename. None Returns: Type Description Optional [ Path ] Recent weights path if it exists otherwise None object. Source code in src/trecover/utils/model.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def get_recent_weights_path ( exp_dir : Path , exp_mark : str , weights_name : Optional [ str ] = None ) -> Optional [ Path ]: \"\"\" Get a model recent weights path. Parameters ---------- exp_dir : Path Experiment directory path. exp_mark : str Experiment folder mark. weights_name : str, default=None Weights filename. Returns ------- Optional[Path]: Recent weights path if it exists otherwise None object. \"\"\" if weights_name : return weights_path if ( weights_path := exp_dir / exp_mark / weights_name ) . exists () else None if ( weights_path := exp_dir / exp_mark / 'weights' ) . exists (): recent_weights = None most_recent_timestamp = 0 for weights in weights_path . iterdir (): if timestamp := weights . stat () . st_ctime > most_recent_timestamp : recent_weights = weights most_recent_timestamp = timestamp return recent_weights","title":"get_recent_weights_path()"},{"location":"src/trecover/utils/model/#src.trecover.utils.model.get_model","text":"get_model ( token_size , pe_max_len , num_layers , d_model , n_heads , d_ff , dropout , device = torch . device ( \"cpu\" ), weights = None , silently = False , ) Get a model with specified configuration. Parameters: Name Type Description Default token_size int Token (column) size. required pe_max_len int Positional encoding max length. required num_layers int Number of encoder and decoder blocks required d_model int Model dimension - number of expected features in the encoder (decoder) input. required n_heads int Number of encoder and decoder attention heads. required d_ff int Dimension of the feedforward layer. required dropout float Dropout range. required device torch . device , default Device on which to allocate the model. torch.device('cpu') weights Path , default Model weights path for initialization. None silently bool , default Initialize the model silently without any verbose information. False Returns: Name Type Description model TRecover Initialized model. Raises: Type Description SystemExit: If the weight's path is not provided and the cli 'stop' option is selected. Source code in src/trecover/utils/model.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def get_model ( token_size : int , pe_max_len : int , num_layers : int , d_model : int , n_heads : int , d_ff : int , dropout : float , device : torch . device = torch . device ( 'cpu' ), weights : Optional [ Path ] = None , silently : bool = False ) -> TRecover : \"\"\" Get a model with specified configuration. Parameters ---------- token_size : int Token (column) size. pe_max_len : int Positional encoding max length. num_layers : int Number of encoder and decoder blocks d_model : int Model dimension - number of expected features in the encoder (decoder) input. n_heads : int Number of encoder and decoder attention heads. d_ff : int Dimension of the feedforward layer. dropout : float, Dropout range. device : torch.device, default=torch.device('cpu') Device on which to allocate the model. weights : Path, default=None Model weights path for initialization. silently : bool, default=False Initialize the model silently without any verbose information. Returns ------- model : TRecover Initialized model. Raises ------ SystemExit: If the weight's path is not provided and the cli 'stop' option is selected. \"\"\" model = TRecover ( token_size , pe_max_len , num_layers , d_model , n_heads , d_ff , dropout ) . to ( device ) if weights and weights . exists () and weights . is_file (): model . load_parameters ( weights , device = device ) if not silently : log . project_console . print ( f 'The below model parameters have been loaded: \\n { weights } ' , style = 'bright_green' ) return model if weights : log . project_console . print ( f 'Failed to load model parameters: { str ( weights ) } ' , style = 'bold red' ) else : log . project_console . print ( \"Model parameters aren't specified\" , style = 'bright_blue' ) if silently or Confirm . ask ( prompt = '[bright_blue]Continue training from scratch?' , default = True , console = log . project_console ): return model else : raise SystemExit","title":"get_model()"},{"location":"src/trecover/utils/train/","text":"ExperimentParams Bases: dict Container for experiment parameters Source code in src/trecover/utils/train.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class ExperimentParams ( dict ): \"\"\" Container for experiment parameters \"\"\" def __init__ ( self , params : Union [ argparse . Namespace , Dict , None ] = None ): super ( ExperimentParams , self ) . __init__ () if isinstance ( params , argparse . Namespace ): self . __dict__ . update ( vars ( params )) if isinstance ( params , dict ): self . __dict__ . update ( params ) def __getitem__ ( self , key : Any ) -> Any : return self . __dict__ [ key ] def __repr__ ( self ): return repr ( self . __dict__ ) def __str__ ( self ): return str ( self . __dict__ ) def jsonify ( self ) -> Dict : \"\"\" Simplify experiment parameters for further json serialization. Returns ------- Experiment parameters as a dictionary with python built-in types values. \"\"\" simplified = dict () simplified . update ( self . __dict__ ) for key , value in self . __dict__ . items (): if isinstance ( value , Path ): simplified [ key ] = str ( value ) return simplified def update ( self , * args : Any , ** kwargs : Any ) -> None : self . __dict__ . update ( * args , ** kwargs ) jsonify jsonify () Simplify experiment parameters for further json serialization. Returns: Type Description Experiment parameters as a dictionary with python built-in types values. Source code in src/trecover/utils/train.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def jsonify ( self ) -> Dict : \"\"\" Simplify experiment parameters for further json serialization. Returns ------- Experiment parameters as a dictionary with python built-in types values. \"\"\" simplified = dict () simplified . update ( self . __dict__ ) for key , value in self . __dict__ . items (): if isinstance ( value , Path ): simplified [ key ] = str ( value ) return simplified set_seeds set_seeds ( seed = 2531 ) Set seeds for experiment reproducibility Source code in src/trecover/utils/train.py 57 58 59 60 61 62 63 def set_seeds ( seed : int = 2531 ) -> None : \"\"\" Set seeds for experiment reproducibility \"\"\" np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) # multi-GPU get_experiment_params get_experiment_params ( parser , args = None ) Parse command line arguments as ExperimentParams object. Parameters: Name Type Description Default parser ArgumentParser Object for parsing command line strings into Python objects. required args Optional [ List [ str ]], default Command line arguments. None Returns: Name Type Description ExperimentParams ExperimentParams Parsed command line arguments as ExperimentParams object. Source code in src/trecover/utils/train.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def get_experiment_params ( parser : ArgumentParser , args : Optional [ List [ str ]] = None ) -> ExperimentParams : \"\"\" Parse command line arguments as ExperimentParams object. Parameters ---------- parser : ArgumentParser Object for parsing command line strings into Python objects. args : Optional[List[str]], default=None Command line arguments. Returns ------- ExperimentParams: Parsed command line arguments as ExperimentParams object. \"\"\" return ExperimentParams ( parser . parse_args ( args = args )) load_params load_params ( model_params ) Get experiment parameters container. Parameters: Name Type Description Default model_params Path Path to serialized experiment parameters. required Returns: Name Type Description ExperimentParams ExperimentParams Experiment parameters container as a ExperimentParams object. Source code in src/trecover/utils/train.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def load_params ( model_params : Path ) -> ExperimentParams : \"\"\" Get experiment parameters container. Parameters ---------- model_params : Path Path to serialized experiment parameters. Returns ------- ExperimentParams: Experiment parameters container as a ExperimentParams object. \"\"\" return ExperimentParams ( json . load ( model_params . open ())) save_params save_params ( data , filepath , sort = False ) Save experiment parameters on disk. Parameters: Name Type Description Default data Dict Experiment parameters. required filepath Path File path for saving. required sort bool , default Perform parameters keys sorting. False Source code in src/trecover/utils/train.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def save_params ( data : Dict , filepath : Path , sort = False ) -> None : \"\"\" Save experiment parameters on disk. Parameters ---------- data : Dict Experiment parameters. filepath : Path File path for saving. sort : bool, default=False Perform parameters keys sorting. \"\"\" with filepath . open ( 'w' ) as f : json . dump ( data , indent = 2 , fp = f , sort_keys = sort ) get_experiment_mark get_experiment_mark () Generate a string mark based on current date Source code in src/trecover/utils/train.py 125 126 127 128 129 def get_experiment_mark () -> str : \"\"\" Generate a string mark based on current date \"\"\" date = datetime . now () return f ' { date . month : 0>2 } - { date . day : 0>2 } - { date . hour : 0>2 } - { date . minute : 0>2 } ' optimizer_to_str optimizer_to_str ( optimizer ) Get optimizer object string representation. Parameters: Name Type Description Default optimizer Optimizer Optimizer object for representation. required Returns: Name Type Description str str Optimizer string representation. Source code in src/trecover/utils/train.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def optimizer_to_str ( optimizer : Optimizer ) -> str : \"\"\" Get optimizer object string representation. Parameters ---------- optimizer : Optimizer Optimizer object for representation. Returns ------- str: Optimizer string representation. \"\"\" optimizer_str_repr = str ( optimizer ) optimizer_name = optimizer_str_repr . split ()[ 0 ] optimizer_params = re . findall ( r '(.+): (.+)' , optimizer_str_repr ) return f \" { optimizer_name } ( { ', ' . join ([ f ' { param . strip () } = { value . strip () } ' for param , value in optimizer_params ]) } )\" transfer transfer ( tensors , to_device ) Transfer the tensors to a specified device. Parameters: Name Type Description Default tensors Tuple Sequence of tensors to transfer. required to_device torch . device The desired device of returned tensors. required Returns: Type Description Tuple [ Optional [ Tensor ], ...] Tuple of tensors allocated on the specified device. Source code in src/trecover/utils/train.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def transfer ( tensors : Tuple [ Optional [ Tensor ], ... ], to_device : torch . device ) -> Tuple [ Optional [ Tensor ], ... ]: \"\"\" Transfer the tensors to a specified device. Parameters ---------- tensors : Tuple Sequence of tensors to transfer. to_device : torch.device The desired device of returned tensors. Returns ------- Tuple[Optional[Tensor], ...]: Tuple of tensors allocated on the specified device. \"\"\" return tuple ([ tensor . to ( to_device ) if isinstance ( tensor , Tensor ) else tensor for tensor in tensors ])","title":"Train"},{"location":"src/trecover/utils/train/#src.trecover.utils.train.ExperimentParams","text":"Bases: dict Container for experiment parameters Source code in src/trecover/utils/train.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class ExperimentParams ( dict ): \"\"\" Container for experiment parameters \"\"\" def __init__ ( self , params : Union [ argparse . Namespace , Dict , None ] = None ): super ( ExperimentParams , self ) . __init__ () if isinstance ( params , argparse . Namespace ): self . __dict__ . update ( vars ( params )) if isinstance ( params , dict ): self . __dict__ . update ( params ) def __getitem__ ( self , key : Any ) -> Any : return self . __dict__ [ key ] def __repr__ ( self ): return repr ( self . __dict__ ) def __str__ ( self ): return str ( self . __dict__ ) def jsonify ( self ) -> Dict : \"\"\" Simplify experiment parameters for further json serialization. Returns ------- Experiment parameters as a dictionary with python built-in types values. \"\"\" simplified = dict () simplified . update ( self . __dict__ ) for key , value in self . __dict__ . items (): if isinstance ( value , Path ): simplified [ key ] = str ( value ) return simplified def update ( self , * args : Any , ** kwargs : Any ) -> None : self . __dict__ . update ( * args , ** kwargs )","title":"ExperimentParams"},{"location":"src/trecover/utils/train/#src.trecover.utils.train.ExperimentParams.jsonify","text":"jsonify () Simplify experiment parameters for further json serialization. Returns: Type Description Experiment parameters as a dictionary with python built-in types values. Source code in src/trecover/utils/train.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def jsonify ( self ) -> Dict : \"\"\" Simplify experiment parameters for further json serialization. Returns ------- Experiment parameters as a dictionary with python built-in types values. \"\"\" simplified = dict () simplified . update ( self . __dict__ ) for key , value in self . __dict__ . items (): if isinstance ( value , Path ): simplified [ key ] = str ( value ) return simplified","title":"jsonify()"},{"location":"src/trecover/utils/train/#src.trecover.utils.train.set_seeds","text":"set_seeds ( seed = 2531 ) Set seeds for experiment reproducibility Source code in src/trecover/utils/train.py 57 58 59 60 61 62 63 def set_seeds ( seed : int = 2531 ) -> None : \"\"\" Set seeds for experiment reproducibility \"\"\" np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) # multi-GPU","title":"set_seeds()"},{"location":"src/trecover/utils/train/#src.trecover.utils.train.get_experiment_params","text":"get_experiment_params ( parser , args = None ) Parse command line arguments as ExperimentParams object. Parameters: Name Type Description Default parser ArgumentParser Object for parsing command line strings into Python objects. required args Optional [ List [ str ]], default Command line arguments. None Returns: Name Type Description ExperimentParams ExperimentParams Parsed command line arguments as ExperimentParams object. Source code in src/trecover/utils/train.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def get_experiment_params ( parser : ArgumentParser , args : Optional [ List [ str ]] = None ) -> ExperimentParams : \"\"\" Parse command line arguments as ExperimentParams object. Parameters ---------- parser : ArgumentParser Object for parsing command line strings into Python objects. args : Optional[List[str]], default=None Command line arguments. Returns ------- ExperimentParams: Parsed command line arguments as ExperimentParams object. \"\"\" return ExperimentParams ( parser . parse_args ( args = args ))","title":"get_experiment_params()"},{"location":"src/trecover/utils/train/#src.trecover.utils.train.load_params","text":"load_params ( model_params ) Get experiment parameters container. Parameters: Name Type Description Default model_params Path Path to serialized experiment parameters. required Returns: Name Type Description ExperimentParams ExperimentParams Experiment parameters container as a ExperimentParams object. Source code in src/trecover/utils/train.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def load_params ( model_params : Path ) -> ExperimentParams : \"\"\" Get experiment parameters container. Parameters ---------- model_params : Path Path to serialized experiment parameters. Returns ------- ExperimentParams: Experiment parameters container as a ExperimentParams object. \"\"\" return ExperimentParams ( json . load ( model_params . open ()))","title":"load_params()"},{"location":"src/trecover/utils/train/#src.trecover.utils.train.save_params","text":"save_params ( data , filepath , sort = False ) Save experiment parameters on disk. Parameters: Name Type Description Default data Dict Experiment parameters. required filepath Path File path for saving. required sort bool , default Perform parameters keys sorting. False Source code in src/trecover/utils/train.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def save_params ( data : Dict , filepath : Path , sort = False ) -> None : \"\"\" Save experiment parameters on disk. Parameters ---------- data : Dict Experiment parameters. filepath : Path File path for saving. sort : bool, default=False Perform parameters keys sorting. \"\"\" with filepath . open ( 'w' ) as f : json . dump ( data , indent = 2 , fp = f , sort_keys = sort )","title":"save_params()"},{"location":"src/trecover/utils/train/#src.trecover.utils.train.get_experiment_mark","text":"get_experiment_mark () Generate a string mark based on current date Source code in src/trecover/utils/train.py 125 126 127 128 129 def get_experiment_mark () -> str : \"\"\" Generate a string mark based on current date \"\"\" date = datetime . now () return f ' { date . month : 0>2 } - { date . day : 0>2 } - { date . hour : 0>2 } - { date . minute : 0>2 } '","title":"get_experiment_mark()"},{"location":"src/trecover/utils/train/#src.trecover.utils.train.optimizer_to_str","text":"optimizer_to_str ( optimizer ) Get optimizer object string representation. Parameters: Name Type Description Default optimizer Optimizer Optimizer object for representation. required Returns: Name Type Description str str Optimizer string representation. Source code in src/trecover/utils/train.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def optimizer_to_str ( optimizer : Optimizer ) -> str : \"\"\" Get optimizer object string representation. Parameters ---------- optimizer : Optimizer Optimizer object for representation. Returns ------- str: Optimizer string representation. \"\"\" optimizer_str_repr = str ( optimizer ) optimizer_name = optimizer_str_repr . split ()[ 0 ] optimizer_params = re . findall ( r '(.+): (.+)' , optimizer_str_repr ) return f \" { optimizer_name } ( { ', ' . join ([ f ' { param . strip () } = { value . strip () } ' for param , value in optimizer_params ]) } )\"","title":"optimizer_to_str()"},{"location":"src/trecover/utils/train/#src.trecover.utils.train.transfer","text":"transfer ( tensors , to_device ) Transfer the tensors to a specified device. Parameters: Name Type Description Default tensors Tuple Sequence of tensors to transfer. required to_device torch . device The desired device of returned tensors. required Returns: Type Description Tuple [ Optional [ Tensor ], ...] Tuple of tensors allocated on the specified device. Source code in src/trecover/utils/train.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def transfer ( tensors : Tuple [ Optional [ Tensor ], ... ], to_device : torch . device ) -> Tuple [ Optional [ Tensor ], ... ]: \"\"\" Transfer the tensors to a specified device. Parameters ---------- tensors : Tuple Sequence of tensors to transfer. to_device : torch.device The desired device of returned tensors. Returns ------- Tuple[Optional[Tensor], ...]: Tuple of tensors allocated on the specified device. \"\"\" return tuple ([ tensor . to ( to_device ) if isinstance ( tensor , Tensor ) else tensor for tensor in tensors ])","title":"transfer()"},{"location":"src/trecover/utils/transform/","text":"columns_to_tensor columns_to_tensor ( columns , device = torch . device ( 'cpu' )) Convert the columns to a torch tensor. Parameters: Name Type Description Default columns List [ str ] Columns for keyless reading. required device torch . device , default The desired device of returned tensor. torch.device('cpu') Returns: Name Type Description tensor Tensor [ SEQUENCE_LEN , len ( var . ALPHABET )] Columns as a torch tensor. Source code in src/trecover/utils/transform.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def columns_to_tensor ( columns : List [ str ], device : torch . device = torch . device ( 'cpu' )) -> Tensor : \"\"\" Convert the columns to a torch tensor. Parameters ---------- columns : List[str] Columns for keyless reading. device : torch.device, default=torch.device('cpu') The desired device of returned tensor. Returns ------- tensor : Tensor[SEQUENCE_LEN, len(var.ALPHABET)] Columns as a torch tensor. \"\"\" tensor = torch . zeros (( len ( columns ), len ( var . ALPHABET )), dtype = torch . float , device = device ) for col in range ( len ( columns )): for symbol in columns [ col ]: tensor [ col , var . ALPHABET2NUM [ symbol ]] = 1 return tensor files_columns_to_tensors files_columns_to_tensors ( files_columns , device = torch . device ( \"cpu\" ) ) Convert the batch of columns to torch tensors. Parameters: Name Type Description Default files_columns List [ List [ str ]] Batch of columns for keyless reading. required device torch . device , default The desired device of returned tensor. torch.device('cpu') Returns: Type Description List [ Tensor ] Columns batch as a list of torch tensors. Source code in src/trecover/utils/transform.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def files_columns_to_tensors ( files_columns : List [ List [ str ]], device : torch . device = torch . device ( 'cpu' ) ) -> List [ Tensor ]: \"\"\" Convert the batch of columns to torch tensors. Parameters ---------- files_columns : List[List[str]] Batch of columns for keyless reading. device : torch.device, default=torch.device('cpu') The desired device of returned tensor. Returns ------- List[Tensor]: Columns batch as a list of torch tensors. \"\"\" return [ columns_to_tensor ( columns , device ) for columns in files_columns ] tensor_to_columns tensor_to_columns ( grid ) Convert the columns' tensor representation to a list of strings with alphabet symbols. Parameters: Name Type Description Default grid Tensor [ SEQUENCE_LEN , len ( var . ALPHABET )] Columns for keyless reading as a tensor. required Returns: Type Description List [ str ] Columns' tensor representation as a list of strings with alphabet symbols. Source code in src/trecover/utils/transform.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def tensor_to_columns ( grid : Tensor ) -> List [ str ]: \"\"\" Convert the columns' tensor representation to a list of strings with alphabet symbols. Parameters ---------- grid : Tensor[SEQUENCE_LEN, len(var.ALPHABET)] Columns for keyless reading as a tensor. Returns ------- List[str]: Columns' tensor representation as a list of strings with alphabet symbols. \"\"\" return [ '' . join ([ var . NUM2ALPHABET [ pos ] for pos in range ( grid . shape [ 1 ]) if grid [ c , pos ]]) for c in range ( grid . shape [ 0 ]) ] tensor_to_target tensor_to_target ( tgt ) Convert the target tensor representation to list of alphabet symbols. Parameters: Name Type Description Default tgt Tensor [ SEQUENCE_LEN ] Target tensor representation of columns' correct symbols. required Returns: Type Description List [ str ] Target tensor representation to list of alphabet symbols. Source code in src/trecover/utils/transform.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def tensor_to_target ( tgt : Tensor ) -> List [ str ]: \"\"\" Convert the target tensor representation to list of alphabet symbols. Parameters ---------- tgt : Tensor[SEQUENCE_LEN] Target tensor representation of columns' correct symbols. Returns ------- List[str]: Target tensor representation to list of alphabet symbols. \"\"\" return [ var . NUM2ALPHABET [ ch_id ] for ch_id in tgt . tolist ()]","title":"Transform"},{"location":"src/trecover/utils/transform/#src.trecover.utils.transform.columns_to_tensor","text":"columns_to_tensor ( columns , device = torch . device ( 'cpu' )) Convert the columns to a torch tensor. Parameters: Name Type Description Default columns List [ str ] Columns for keyless reading. required device torch . device , default The desired device of returned tensor. torch.device('cpu') Returns: Name Type Description tensor Tensor [ SEQUENCE_LEN , len ( var . ALPHABET )] Columns as a torch tensor. Source code in src/trecover/utils/transform.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def columns_to_tensor ( columns : List [ str ], device : torch . device = torch . device ( 'cpu' )) -> Tensor : \"\"\" Convert the columns to a torch tensor. Parameters ---------- columns : List[str] Columns for keyless reading. device : torch.device, default=torch.device('cpu') The desired device of returned tensor. Returns ------- tensor : Tensor[SEQUENCE_LEN, len(var.ALPHABET)] Columns as a torch tensor. \"\"\" tensor = torch . zeros (( len ( columns ), len ( var . ALPHABET )), dtype = torch . float , device = device ) for col in range ( len ( columns )): for symbol in columns [ col ]: tensor [ col , var . ALPHABET2NUM [ symbol ]] = 1 return tensor","title":"columns_to_tensor()"},{"location":"src/trecover/utils/transform/#src.trecover.utils.transform.files_columns_to_tensors","text":"files_columns_to_tensors ( files_columns , device = torch . device ( \"cpu\" ) ) Convert the batch of columns to torch tensors. Parameters: Name Type Description Default files_columns List [ List [ str ]] Batch of columns for keyless reading. required device torch . device , default The desired device of returned tensor. torch.device('cpu') Returns: Type Description List [ Tensor ] Columns batch as a list of torch tensors. Source code in src/trecover/utils/transform.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def files_columns_to_tensors ( files_columns : List [ List [ str ]], device : torch . device = torch . device ( 'cpu' ) ) -> List [ Tensor ]: \"\"\" Convert the batch of columns to torch tensors. Parameters ---------- files_columns : List[List[str]] Batch of columns for keyless reading. device : torch.device, default=torch.device('cpu') The desired device of returned tensor. Returns ------- List[Tensor]: Columns batch as a list of torch tensors. \"\"\" return [ columns_to_tensor ( columns , device ) for columns in files_columns ]","title":"files_columns_to_tensors()"},{"location":"src/trecover/utils/transform/#src.trecover.utils.transform.tensor_to_columns","text":"tensor_to_columns ( grid ) Convert the columns' tensor representation to a list of strings with alphabet symbols. Parameters: Name Type Description Default grid Tensor [ SEQUENCE_LEN , len ( var . ALPHABET )] Columns for keyless reading as a tensor. required Returns: Type Description List [ str ] Columns' tensor representation as a list of strings with alphabet symbols. Source code in src/trecover/utils/transform.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def tensor_to_columns ( grid : Tensor ) -> List [ str ]: \"\"\" Convert the columns' tensor representation to a list of strings with alphabet symbols. Parameters ---------- grid : Tensor[SEQUENCE_LEN, len(var.ALPHABET)] Columns for keyless reading as a tensor. Returns ------- List[str]: Columns' tensor representation as a list of strings with alphabet symbols. \"\"\" return [ '' . join ([ var . NUM2ALPHABET [ pos ] for pos in range ( grid . shape [ 1 ]) if grid [ c , pos ]]) for c in range ( grid . shape [ 0 ]) ]","title":"tensor_to_columns()"},{"location":"src/trecover/utils/transform/#src.trecover.utils.transform.tensor_to_target","text":"tensor_to_target ( tgt ) Convert the target tensor representation to list of alphabet symbols. Parameters: Name Type Description Default tgt Tensor [ SEQUENCE_LEN ] Target tensor representation of columns' correct symbols. required Returns: Type Description List [ str ] Target tensor representation to list of alphabet symbols. Source code in src/trecover/utils/transform.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def tensor_to_target ( tgt : Tensor ) -> List [ str ]: \"\"\" Convert the target tensor representation to list of alphabet symbols. Parameters ---------- tgt : Tensor[SEQUENCE_LEN] Target tensor representation of columns' correct symbols. Returns ------- List[str]: Target tensor representation to list of alphabet symbols. \"\"\" return [ var . NUM2ALPHABET [ ch_id ] for ch_id in tgt . tolist ()]","title":"tensor_to_target()"},{"location":"src/trecover/utils/visualization/","text":"visualize_columns visualize_columns ( columns , delimiter = '' , as_rows = False ) Get the columns string representation. Parameters: Name Type Description Default columns List [ str ] Columns for keyless reading. required delimiter str Delimiter for columns visualization. '' as_rows bool Return visualization as a list of strings. False Returns: Type Description Union [ str , List [ str ]] Visualization as a string or list of strings. Source code in src/trecover/utils/visualization.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def visualize_columns ( columns : List [ str ], delimiter : str = '' , as_rows = False ) -> Union [ str , List [ str ]]: \"\"\" Get the columns string representation. Parameters ---------- columns : List[str] Columns for keyless reading. delimiter : str Delimiter for columns visualization. as_rows : bool Return visualization as a list of strings. Returns ------- Union[str, List[str]]: Visualization as a string or list of strings. \"\"\" if len ( columns ) == 0 : return '' rows = list () columns = [ list ( column ) for column in columns ] max_depth = max ([ len ( column ) for column in columns ]) for d in range ( max_depth ): row = str () for c in range ( len ( columns )): row += f ' { delimiter }{ columns [ c ][ d ] } ' if d < len ( columns [ c ]) else f ' { delimiter } ' rows . append ( f ' { row }{ delimiter } ' ) return rows if as_rows else ' \\n ' . join ( rows ) visualize_target visualize_target ( target , delimiter = '' ) Get the target string representation. Parameters: Name Type Description Default target List [ str ] List of correct symbols for each column. required delimiter str Delimiter for target visualization. '' Returns: Name Type Description str str Visualization as a string. Source code in src/trecover/utils/visualization.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def visualize_target ( target : List [ str ], delimiter : str = '' ) -> str : \"\"\" Get the target string representation. Parameters ---------- target : List[str] List of correct symbols for each column. delimiter : str Delimiter for target visualization. Returns ------- str: Visualization as a string. \"\"\" return f ' { delimiter }{ delimiter . join ( target ) }{ delimiter } '","title":"Visualization"},{"location":"src/trecover/utils/visualization/#src.trecover.utils.visualization.visualize_columns","text":"visualize_columns ( columns , delimiter = '' , as_rows = False ) Get the columns string representation. Parameters: Name Type Description Default columns List [ str ] Columns for keyless reading. required delimiter str Delimiter for columns visualization. '' as_rows bool Return visualization as a list of strings. False Returns: Type Description Union [ str , List [ str ]] Visualization as a string or list of strings. Source code in src/trecover/utils/visualization.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def visualize_columns ( columns : List [ str ], delimiter : str = '' , as_rows = False ) -> Union [ str , List [ str ]]: \"\"\" Get the columns string representation. Parameters ---------- columns : List[str] Columns for keyless reading. delimiter : str Delimiter for columns visualization. as_rows : bool Return visualization as a list of strings. Returns ------- Union[str, List[str]]: Visualization as a string or list of strings. \"\"\" if len ( columns ) == 0 : return '' rows = list () columns = [ list ( column ) for column in columns ] max_depth = max ([ len ( column ) for column in columns ]) for d in range ( max_depth ): row = str () for c in range ( len ( columns )): row += f ' { delimiter }{ columns [ c ][ d ] } ' if d < len ( columns [ c ]) else f ' { delimiter } ' rows . append ( f ' { row }{ delimiter } ' ) return rows if as_rows else ' \\n ' . join ( rows )","title":"visualize_columns()"},{"location":"src/trecover/utils/visualization/#src.trecover.utils.visualization.visualize_target","text":"visualize_target ( target , delimiter = '' ) Get the target string representation. Parameters: Name Type Description Default target List [ str ] List of correct symbols for each column. required delimiter str Delimiter for target visualization. '' Returns: Name Type Description str str Visualization as a string. Source code in src/trecover/utils/visualization.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def visualize_target ( target : List [ str ], delimiter : str = '' ) -> str : \"\"\" Get the target string representation. Parameters ---------- target : List[str] List of correct symbols for each column. delimiter : str Delimiter for target visualization. Returns ------- str: Visualization as a string. \"\"\" return f ' { delimiter }{ delimiter . join ( target ) }{ delimiter } '","title":"visualize_target()"}]}